---
title: "SGLang: Efficient Execution of Structured Language Model Programs"
date: 2026-02-13
---

# SGLang: Efficient Execution of Structured Language Model Programs

<p style="color: var(--vp-c-text-2); font-size: 14px;">
ğŸ“… 2026-02-13 &nbsp;Â·&nbsp; ğŸ·ï¸ æ¨ç†å¼•æ“ &nbsp;Â·&nbsp; ğŸ“– è®ºæ–‡ç²¾è¯»
</p>

> **è®ºæ–‡ä¿¡æ¯**
> - **ä½œè€…**: Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, Ying Sheng
> - **æœºæ„**: UC Berkeley, Stanford University
> - **å‘è¡¨**: ECCV 2024 / arXiv 2023
> - **é“¾æ¥**: [arXiv:2312.07104](https://arxiv.org/abs/2312.07104)

## Brief Intro

<div class="lang-zh">

SGLang æå‡ºäº†ä¸€å¥— **ç»“æ„åŒ–ç”Ÿæˆè¯­è¨€ï¼ˆStructured Generation Languageï¼‰** å‰ç«¯å’Œå¯¹åº”çš„é«˜æ•ˆè¿è¡Œæ—¶ç³»ç»Ÿï¼Œé€šè¿‡ **RadixAttention**ï¼ˆKV Cache è‡ªåŠ¨å¤ç”¨ï¼‰ã€**å‹ç¼©æœ‰é™çŠ¶æ€æœº**ï¼ˆé«˜é€Ÿçº¦æŸè§£ç ï¼‰å’Œ **API æ¨æµ‹æ‰§è¡Œ** ä¸‰å¤§æ ¸å¿ƒä¼˜åŒ–ï¼Œå°†å¤æ‚ LLM ç¨‹åºçš„æ‰§è¡Œé€Ÿåº¦æå‡ **æœ€é«˜ 6.4 å€**ã€‚

</div>

<div class="lang-en">

SGLang introduces a **Structured Generation Language** frontend together with an efficient runtime system. It achieves up to **6.4x** speedup on complex LLM programs through three core optimizations: **RadixAttention** (automatic KV Cache reuse), **Compressed Finite State Machine** (fast constrained decoding), and **API Speculative Execution**.

</div>

---

<div class="lang-zh">

## èƒŒæ™¯ï¼šä¸ºä»€ä¹ˆéœ€è¦ SGLangï¼Ÿ

### 1. LLM åº”ç”¨æ—¥è¶‹å¤æ‚

ç°ä»£ LLM åº”ç”¨æ—©å·²è¶…è¶Šäº†ç®€å•çš„å•è½®é—®ç­”ã€‚å…¸å‹åœºæ™¯åŒ…æ‹¬ï¼š

- **å¤šè½®å¯¹è¯**ï¼šéœ€è¦ç»´æŠ¤å¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡
- **å¤šæ¨¡æ€æ¨ç†**ï¼šåŒæ—¶å¤„ç†æ–‡æœ¬å’Œå›¾åƒè¾“å…¥
- **ç»“æ„åŒ–è¾“å‡º**ï¼šè¦æ±‚æ¨¡å‹è¾“å‡º JSONã€ä»£ç ç­‰ç‰¹å®šæ ¼å¼
- **å¤æ‚å·¥ä½œæµ**ï¼šåŒ…å«åˆ†æ”¯åˆ¤æ–­ã€å¹¶è¡Œæ¨ç†ã€å¤šæ­¥èšåˆç­‰é€»è¾‘

### 2. ç°æœ‰æ–¹æ¡ˆçš„ç—›ç‚¹

| é—®é¢˜ | è¯´æ˜ |
|------|------|
| **ç¼–ç¨‹å¤æ‚** | ç”¨åŸç”Ÿ API å®ç°å¤æ‚æ¨ç†æµç¨‹éœ€è¦å¤§é‡æ ·æ¿ä»£ç  |
| **KV Cache æµªè´¹** | å¤šæ¬¡è°ƒç”¨ä¹‹é—´çš„å…±äº«å‰ç¼€è¢«é‡å¤è®¡ç®— |
| **çº¦æŸè§£ç æ…¢** | æ­£åˆ™è¡¨è¾¾å¼/JSON Schema çº¦æŸæ¯ä¸ª Token éƒ½è¦éå†è¯è¡¨ |
| **å»¶è¿Ÿé«˜** | å¤šæ¬¡ä¸²è¡Œ API è°ƒç”¨çš„ç½‘ç»œå¾€è¿”å¼€é”€ç´¯ç§¯ä¸¥é‡ |

### 3. SGLang çš„ç›®æ ‡

è®¾è®¡ä¸€ä¸ªåŒæ—¶è§£å†³ **ç¼–ç¨‹æ•ˆç‡** å’Œ **æ‰§è¡Œæ•ˆç‡** çš„ç»Ÿä¸€æ¡†æ¶ï¼Œè®©å¼€å‘è€…ç”¨ç®€æ´çš„ Python ä»£ç æè¿°å¤æ‚ LLM ç¨‹åºï¼Œè€Œè¿è¡Œæ—¶è‡ªåŠ¨å®Œæˆå„ç§æ€§èƒ½ä¼˜åŒ–ã€‚

</div>

<div class="lang-en">

## Background: Why SGLang?

### 1. Increasingly Complex LLM Applications

Modern LLM applications go far beyond simple single-turn Q&A. Typical scenarios include:

- **Multi-turn conversations**: maintaining dialogue history and context
- **Multi-modal reasoning**: processing text and image inputs simultaneously
- **Structured output**: requiring JSON, code, or other specific formats
- **Complex workflows**: branching logic, parallel reasoning, multi-step aggregation

### 2. Pain Points of Existing Solutions

| Problem | Description |
|---------|-------------|
| **Complex programming** | Implementing complex inference pipelines with raw APIs requires substantial boilerplate |
| **KV Cache waste** | Shared prefixes across multiple calls are redundantly computed |
| **Slow constrained decoding** | Regex/JSON Schema constraints require scanning the entire vocabulary per token |
| **High latency** | Sequential API round-trips accumulate significant network overhead |

### 3. SGLang's Goal

Design a unified framework that addresses both **programming efficiency** and **execution efficiency**, allowing developers to describe complex LLM programs in concise Python while the runtime automatically handles performance optimizations.

</div>

---

<div class="lang-zh">

## æ ¸å¿ƒæ–¹æ³•

### SGLang å‰ç«¯ï¼šç»“æ„åŒ–ç”Ÿæˆè¯­è¨€

SGLang åµŒå…¥åœ¨ Python ä¸­ï¼Œæä¾›ä¸€ç»„ç›´è§‚çš„åŸè¯­æ¥æ„å»º LLM ç¨‹åºï¼š

| åŸè¯­ | ä½œç”¨ | ç¤ºä¾‹ |
|------|------|------|
| `gen()` | è°ƒç”¨æ¨¡å‹ç”Ÿæˆæ–‡æœ¬ | `gen("answer", stop=".")` |
| `select()` | ä»å€™é€‰ä¸­é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„é€‰é¡¹ | `select("choice", choices=["yes","no"])` |
| `fork()` | åˆ†å‰æ‰§è¡Œæµï¼Œå®ç°å¹¶è¡Œç”Ÿæˆ | `s.fork(3)` |
| `image()` | åµŒå…¥å¤šæ¨¡æ€å›¾åƒè¾“å…¥ | `image(path)` |
| `system()` / `user()` / `assistant()` | æ„å»ºå¯¹è¯æ¶ˆæ¯ | å¯¹åº” Chat æ¨¡æ¿è§’è‰² |
| `regex=` | æ­£åˆ™çº¦æŸè¾“å‡ºæ ¼å¼ | `gen("out", regex=r'\d+')` |

</div>

<div class="lang-en">

## Core Methods

### SGLang Frontend: Structured Generation Language

SGLang is embedded in Python, offering a set of intuitive primitives for building LLM programs:

| Primitive | Purpose | Example |
|-----------|---------|---------|
| `gen()` | Generate text from the model | `gen("answer", stop=".")` |
| `select()` | Select the highest-probability option | `select("choice", choices=["yes","no"])` |
| `fork()` | Fork execution for parallel generation | `s.fork(3)` |
| `image()` | Embed multi-modal image input | `image(path)` |
| `system()` / `user()` / `assistant()` | Build chat messages | Maps to chat template roles |
| `regex=` | Constrain output format via regex | `gen("out", regex=r'\d+')` |

</div>

<div class="lang-zh">

### ä»£ç ç¤ºä¾‹è¯¦è§£

ä¸‹é¢æ˜¯è®ºæ–‡ä¸­çš„æ ¸å¿ƒç¤ºä¾‹â€”â€”ä¸€ä¸ª **å¤šç»´åº¦è®ºæ–‡è¯„å®¡ç¨‹åº**ï¼Œå±•ç¤ºäº† SGLang å‡ ä¹æ‰€æœ‰å…³é”®ç‰¹æ€§ï¼š

</div>

<div class="lang-en">

### Code Example Walkthrough

Below is the key example from the paper â€” a **multi-dimensional essay judge** that showcases nearly all SGLang features:

</div>

```python
dimensions = ["Clarity", "Originality", "Evidence"]

@function
def multi_dimensional_judge(s, path, essay):
    # â‘  æ„å»ºç³»ç»Ÿæç¤ºå’Œå¤šæ¨¡æ€è¾“å…¥ / Build system prompt & multi-modal input
    s += system("Evaluate an essay about an image.")
    s += user(image(path) + "Essay:" + essay)
    s += assistant("Sure!")

    # â‘¡ å…ˆåˆ¤æ–­ç›¸å…³æ€§ï¼Œç”¨ select åšäºŒåˆ†ç±» / Check relevance via select
    s += user("Is the essay related to the image?")
    s += assistant(select("related", choices=["yes", "no"]))
    if s["related"] == "no":
        return

    # â‘¢ fork å¹¶è¡Œè¯„ä¼°å¤šä¸ªç»´åº¦ / Fork for parallel multi-dimension evaluation
    forks = s.fork(len(dimensions))
    for f, dim in zip(forks, dimensions):
        f += user("Evaluate based on the following dimension:" +
                  dim + ". End your judgment with the word 'END'")
        f += assistant("Judgment:" + gen("judgment", stop="END"))

    # â‘£ æ±‡æ€»å„ç»´åº¦åˆ¤æ–­ / Merge judgments from all forks
    judgment = "\n".join(f["judgment"] for f in forks)

    # â‘¤ ç”Ÿæˆæ€»ç»“å’Œç­‰çº§ / Generate summary and grade
    s += user("Provide the judgment, summary, and a letter grade")
    s += assistant(judgment + "In summary," + gen("summary", stop=".")
                   + "The grade of it is" + gen("grade"))

    # â‘¥ ç”¨æ­£åˆ™çº¦æŸè¾“å‡º JSON æ ¼å¼ / Constrained JSON output via regex
    schema = r'\{"summary": "[\w\d\s]+\.", "grade": "[ABCD][+-]?"\}'
    s += user("Return in the JSON format.")
    s += assistant(gen("output", regex=schema))

state = multi_dimensional_judge.run(...)
print(state["output"])
```

<div class="lang-zh">

#### é€æ®µè§£æ

**â‘  å¤šæ¨¡æ€å¯¹è¯æ„å»º**

```python
s += system("Evaluate an essay about an image.")
s += user(image(path) + "Essay:" + essay)
s += assistant("Sure!")
```

`system()`ã€`user()`ã€`assistant()` ç”¨äºæ„å»ºç¬¦åˆ Chat æ¨¡æ¿çš„å¯¹è¯ã€‚`image(path)` åµŒå…¥å›¾ç‰‡è¾“å…¥ï¼Œå®ç°å¤šæ¨¡æ€äº¤äº’ã€‚è¿è¡Œæ—¶ä¼šè‡ªåŠ¨å¤„ç† Chat æ¨¡æ¿æ ¼å¼åŒ–å’Œå¤šæ¨¡æ€ç¼–ç ï¼Œå¹¶é€šè¿‡ **KV Cache Reuse** å¤ç”¨å·²ç»è®¡ç®—è¿‡çš„å‰ç¼€ã€‚

**â‘¡ æ¡ä»¶åˆ†æ”¯ï¼š`select` åŸè¯­**

```python
s += assistant(select("related", choices=["yes", "no"]))
if s["related"] == "no":
    return
```

`select()` ä¸æ˜¯è®©æ¨¡å‹è‡ªç”±ç”Ÿæˆå†åšå­—ç¬¦ä¸²åŒ¹é…ï¼Œè€Œæ˜¯ç›´æ¥æ¯”è¾ƒ `"yes"` å’Œ `"no"` ä¸¤ä¸ªé€‰é¡¹åœ¨å½“å‰ä¸Šä¸‹æ–‡ä¸‹çš„ **å¯¹æ•°æ¦‚ç‡ï¼ˆlog-probabilityï¼‰**ï¼Œé€‰æ‹©æ¦‚ç‡æ›´é«˜çš„é‚£ä¸ªã€‚è¿™æ¯”ç”Ÿæˆåè§£ææ›´å¿«ã€æ›´å¯é ã€‚ä¹‹åå¯ä»¥ç”¨ Python åŸç”Ÿçš„ `if` è¯­å¥åšæ§åˆ¶æµâ€”â€”è¿™æ˜¯ SGLang åµŒå…¥ Python çš„å…³é”®ä¼˜åŠ¿ã€‚

**â‘¢ å¹¶è¡Œç”Ÿæˆï¼š`fork` åŸè¯­**

```python
forks = s.fork(len(dimensions))
for f, dim in zip(forks, dimensions):
    f += user("Evaluate based on the following dimension:" + dim + "...")
    f += assistant("Judgment:" + gen("judgment", stop="END"))
```

`fork()` å°†å½“å‰æ‰§è¡ŒçŠ¶æ€"åˆ†å‰"ä¸ºå¤šä¸ªç‹¬ç«‹å‰¯æœ¬ï¼Œæ¯ä¸ªå‰¯æœ¬åœ¨ä¸åŒç»´åº¦ä¸Šå¹¶è¡Œç”Ÿæˆè¯„ä»·ã€‚å…³é”®ä¼˜åŒ–ï¼šæ‰€æœ‰åˆ†å‰å…±äº«ç›¸åŒçš„ **KV Cache å‰ç¼€**ï¼ˆå³åˆ° fork ç‚¹ä¹‹å‰çš„æ‰€æœ‰è®¡ç®—ï¼‰ï¼Œé¿å…äº†é‡å¤çš„ Prefill è®¡ç®—ã€‚å¤šä¸ª `gen()` è°ƒç”¨ä¼šè¢« **æ‰¹å¤„ç†ï¼ˆbatchedï¼‰** åœ¨åŒä¸€ä¸ª forward pass ä¸­å¹¶è¡Œæ‰§è¡Œã€‚

**â‘£ ç»“æœæ±‡æ€»ä¸ Python æ§åˆ¶æµ**

```python
judgment = "\n".join(f["judgment"] for f in forks)
```

é€šè¿‡ `f["judgment"]` ç›´æ¥æå–å„åˆ†å‰çš„ç”Ÿæˆç»“æœï¼Œç„¶åç”¨åŸç”Ÿ Python å­—ç¬¦ä¸²æ“ä½œè¿›è¡Œæ‹¼æ¥ã€‚è¿è¡Œæ—¶é€šè¿‡ **API æ¨æµ‹æ‰§è¡Œï¼ˆSpeculative Executionï¼‰** ä¼˜åŒ–ï¼šåœ¨ç­‰å¾… `select` ç»“æœçš„åŒæ—¶ï¼Œæ¨æµ‹æ€§åœ°å¯åŠ¨åç»­çš„ç”Ÿæˆä»»åŠ¡ï¼Œå‡å°‘ä¸²è¡Œç­‰å¾…ã€‚

**â‘¤ å¤šæ¬¡ç”Ÿæˆæ‹¼æ¥**

```python
s += assistant(judgment + "In summary," + gen("summary", stop=".")
               + "The grade of it is" + gen("grade"))
```

åœ¨åŒä¸€ä¸ª `assistant()` ä¸­æ··åˆå¸¸é‡å­—ç¬¦ä¸²å’Œå¤šæ¬¡ `gen()` è°ƒç”¨ã€‚æ¯æ¬¡ `gen()` ä½¿ç”¨ `stop` å‚æ•°æŒ‡å®šåœæ­¢æ¡ä»¶ï¼Œè¿è¡Œæ—¶è‡ªåŠ¨ç®¡ç† KV Cache çš„è¿ç»­æ€§ã€‚

**â‘¥ ç»“æ„åŒ–è¾“å‡ºï¼šæ­£åˆ™çº¦æŸ**

```python
schema = r'\{"summary": "[\w\d\s]+\.", "grade": "[ABCD][+-]?"\}'
s += assistant(gen("output", regex=schema))
```

é€šè¿‡ `regex` å‚æ•°å¼ºåˆ¶æ¨¡å‹è¾“å‡ºç¬¦åˆ JSON Schema çš„å­—ç¬¦ä¸²ã€‚è¿è¡Œæ—¶ä½¿ç”¨ **å‹ç¼©æœ‰é™çŠ¶æ€æœºï¼ˆCompressed Finite State Machineï¼‰** é«˜æ•ˆå®ç°çº¦æŸè§£ç ï¼Œç›¸æ¯”é€ Token éå†è¯è¡¨çš„æœ´ç´ æ–¹æ³•å¿«æ•°å€ã€‚

</div>

<div class="lang-en">

#### Step-by-Step Breakdown

**â‘  Multi-modal Chat Construction**

```python
s += system("Evaluate an essay about an image.")
s += user(image(path) + "Essay:" + essay)
s += assistant("Sure!")
```

`system()`, `user()`, `assistant()` build a conversation following the chat template. `image(path)` embeds an image input for multi-modal interaction. The runtime automatically handles chat template formatting and multi-modal encoding, and leverages **KV Cache Reuse** to avoid recomputing shared prefixes.

**â‘¡ Conditional Branching: the `select` Primitive**

```python
s += assistant(select("related", choices=["yes", "no"]))
if s["related"] == "no":
    return
```

`select()` does not let the model freely generate and then parse the string. Instead, it directly compares the **log-probabilities** of `"yes"` and `"no"` under the current context and picks the higher one. This is both faster and more reliable than post-hoc parsing. Afterwards, native Python `if` statements drive control flow â€” a key advantage of SGLang being embedded in Python.

**â‘¢ Parallel Generation: the `fork` Primitive**

```python
forks = s.fork(len(dimensions))
for f, dim in zip(forks, dimensions):
    f += user("Evaluate based on the following dimension:" + dim + "...")
    f += assistant("Judgment:" + gen("judgment", stop="END"))
```

`fork()` clones the current execution state into multiple independent copies, each evaluating a different dimension in parallel. Key optimization: all forks share the same **KV Cache prefix** (everything computed before the fork point), avoiding redundant prefill. Multiple `gen()` calls are **batched** into a single forward pass for parallel execution.

**â‘£ Merging Results with Python Control Flow**

```python
judgment = "\n".join(f["judgment"] for f in forks)
```

`f["judgment"]` directly extracts each fork's generation result, then native Python string operations merge them. The runtime further optimizes via **API Speculative Execution**: while waiting for the `select` result, it speculatively launches subsequent generation tasks to reduce serial waiting.

**â‘¤ Multiple Generation Calls**

```python
s += assistant(judgment + "In summary," + gen("summary", stop=".")
               + "The grade of it is" + gen("grade"))
```

Constant strings and multiple `gen()` calls are mixed within a single `assistant()` turn. Each `gen()` uses a `stop` parameter for its termination condition, and the runtime automatically manages KV Cache continuity.

**â‘¥ Structured Output: Regex Constraint**

```python
schema = r'\{"summary": "[\w\d\s]+\.", "grade": "[ABCD][+-]?"\}'
s += assistant(gen("output", regex=schema))
```

The `regex` parameter forces the model output to match a JSON Schema string. The runtime uses a **Compressed Finite State Machine (FSM)** for efficient constrained decoding, achieving several times speedup over the naive approach of scanning the full vocabulary per token.

</div>

---

<div class="lang-zh">

## è¿è¡Œæ—¶ä¼˜åŒ–

### 1. RadixAttentionï¼šè‡ªåŠ¨ KV Cache å¤ç”¨

**æ ¸å¿ƒæ€æƒ³**ï¼šç”¨ **Radix Treeï¼ˆåŸºæ•°æ ‘ï¼‰** æ•°æ®ç»“æ„ç®¡ç†æ‰€æœ‰è¯·æ±‚çš„ KV Cacheï¼Œè‡ªåŠ¨å‘ç°å’Œå¤ç”¨å…±äº«å‰ç¼€ã€‚

```
             [System Prompt]
              /           \
     [User: Image+Essay]   [User: Other...]
      /         \
  [Fork 0]   [Fork 1]   [Fork 2]
  Clarity    Originality  Evidence
```

**å…³é”®ç‰¹æ€§**ï¼š

- **è‡ªåŠ¨å‰ç¼€åŒ¹é…**ï¼šæ–°è¯·æ±‚åˆ°æ¥æ—¶ï¼Œè‡ªåŠ¨åœ¨ Radix Tree ä¸­æŸ¥æ‰¾æœ€é•¿åŒ¹é…å‰ç¼€
- **LRU é©±é€**ï¼šå†…å­˜ä¸è¶³æ—¶ï¼ŒæŒ‰æœ€è¿‘æœ€å°‘ä½¿ç”¨ç­–ç•¥é©±é€ç¼“å­˜
- **è·¨è¯·æ±‚å¤ç”¨**ï¼šä¸åŒè¯·æ±‚ä¹‹é—´å¦‚æœæœ‰ç›¸åŒçš„ System Prompt æˆ–ä¸Šä¸‹æ–‡å‰ç¼€ï¼Œå¯ä»¥ç›´æ¥å¤ç”¨
- **Fork ä¼˜åŒ–**ï¼š`fork()` äº§ç”Ÿçš„å¤šä¸ªåˆ†å‰å¤©ç„¶å…±äº«çˆ¶èŠ‚ç‚¹çš„ KV Cache

ä¸‹å›¾å±•ç¤ºäº† Radix Tree åœ¨å¤„ç†ä¸åŒ LLM è¯·æ±‚æ—¶çš„æ¼”åŒ–è¿‡ç¨‹ï¼ˆæ­¥éª¤ 1-9ï¼‰ï¼ŒåŒ…æ‹¬èŠ‚ç‚¹æ’å…¥ã€å‰ç¼€å…±äº«ã€fork åˆ†å‰å’Œ LRU é©±é€ç­‰æ“ä½œï¼š

![RadixAttention Radix Tree æ“ä½œç¤ºä¾‹](/img/inference/SGLangTree.png)

ä¸‹é¢ç”¨ç®€åŒ–çš„ Python ä»£ç æ¼”ç¤º Radix Tree ç®¡ç† KV Cache çš„æ ¸å¿ƒé€»è¾‘ï¼Œå¯¹åº”ä¸Šå›¾çš„å…³é”®æ“ä½œï¼š

```python
import time
from typing import Optional

class RadixTreeNode:
    """Radix Tree èŠ‚ç‚¹ï¼Œæ¯ä¸ªèŠ‚ç‚¹å­˜å‚¨ä¸€æ®µ token åºåˆ—åŠå…¶å¯¹åº”çš„ KV Cache"""
    def __init__(self, tokens: tuple = (), parent=None):
        self.tokens = tokens                  # è¯¥èŠ‚ç‚¹å­˜å‚¨çš„ token ç‰‡æ®µ
        self.children: dict[int, 'RadixTreeNode'] = {}  # å­èŠ‚ç‚¹æ˜ å°„ (é¦– token â†’ å­èŠ‚ç‚¹)
        self.parent: Optional['RadixTreeNode'] = parent
        self.ref_count = 0                    # å¼•ç”¨è®¡æ•°ï¼šæ­£åœ¨ä½¿ç”¨è¯¥èŠ‚ç‚¹çš„è¯·æ±‚æ•°
        self.last_access = time.time()        # LRU æ—¶é—´æˆ³

class RadixTree:
    """
    ç®€åŒ–ç‰ˆ Radix Treeï¼Œæ¨¡æ‹Ÿ SGLang RadixAttention çš„ KV Cache ç®¡ç†ã€‚
    - insert(): æ’å…¥æ–°çš„ token åºåˆ—ï¼Œè‡ªåŠ¨å¤ç”¨å·²æœ‰å‰ç¼€ï¼ˆå¯¹åº”å›¾ä¸­æ­¥éª¤ 2-7ï¼‰
    - match_prefix(): æŸ¥æ‰¾æœ€é•¿åŒ¹é…å‰ç¼€ï¼Œè¿”å›å¯å¤ç”¨çš„ KV Cache é•¿åº¦
    - evict_lru(): LRU é©±é€ï¼Œé‡Šæ”¾ä¸å†ä½¿ç”¨çš„ç¼“å­˜ï¼ˆå¯¹åº”å›¾ä¸­æ­¥éª¤ 5, 8, 9ï¼‰
    """
    def __init__(self, max_nodes: int = 20):
        self.root = RadixTreeNode()
        self.max_nodes = max_nodes
        self.node_count = 1

    def match_prefix(self, tokens: tuple) -> int:
        """æŸ¥æ‰¾æœ€é•¿åŒ¹é…å‰ç¼€ï¼Œè¿”å›å·²ç¼“å­˜çš„ token æ•°é‡ï¼ˆå¯è·³è¿‡çš„ Prefill é•¿åº¦ï¼‰"""
        node = self.root
        matched = 0
        pos = 0
        while pos < len(tokens):
            first_token = tokens[pos]
            if first_token not in node.children:
                break
            child = node.children[first_token]
            # é€ token æ¯”è¾ƒè¯¥èŠ‚ç‚¹å­˜å‚¨çš„ç‰‡æ®µ
            seg_len = len(child.tokens)
            end = min(pos + seg_len, len(tokens))
            match_len = 0
            for i in range(end - pos):
                if child.tokens[i] != tokens[pos + i]:
                    break
                match_len += 1
            matched += match_len
            pos += match_len
            if match_len < seg_len:
                break  # éƒ¨åˆ†åŒ¹é…ï¼Œåœæ­¢
            node = child
            node.last_access = time.time()  # æ›´æ–° LRU æ—¶é—´æˆ³
        return matched

    def insert(self, tokens: tuple) -> 'RadixTreeNode':
        """æ’å…¥ token åºåˆ—ï¼Œè‡ªåŠ¨å¤ç”¨å…±äº«å‰ç¼€ï¼Œå¿…è¦æ—¶åˆ†è£‚èŠ‚ç‚¹"""
        node = self.root
        pos = 0
        while pos < len(tokens):
            first_token = tokens[pos]
            if first_token not in node.children:
                # æ²¡æœ‰åŒ¹é…çš„å­èŠ‚ç‚¹ â†’ åˆ›å»ºæ–°å¶èŠ‚ç‚¹
                new_node = RadixTreeNode(tokens[pos:], parent=node)
                node.children[first_token] = new_node
                self.node_count += 1
                return new_node

            child = node.children[first_token]
            seg = child.tokens
            # æ‰¾åˆ°å…¬å…±å‰ç¼€é•¿åº¦
            common = 0
            for i in range(min(len(seg), len(tokens) - pos)):
                if seg[i] != tokens[pos + i]:
                    break
                common += 1

            if common < len(seg):
                # éƒ¨åˆ†åŒ¹é… â†’ åˆ†è£‚èŠ‚ç‚¹ (å¯¹åº”å›¾ä¸­æ­¥éª¤ 4: å…±äº«å‰ç¼€è¢«æ‹†åˆ†)
                #   åŸ: parent â†’ child("ABCDE")
                #   æ–°: parent â†’ mid("AB") â†’ child("CDE")
                #                          â†’ new("CF...")
                mid = RadixTreeNode(seg[:common], parent=node)
                child.tokens = seg[common:]
                child.parent = mid
                mid.children[child.tokens[0]] = child
                node.children[first_token] = mid
                self.node_count += 1

                if pos + common < len(tokens):
                    new_node = RadixTreeNode(tokens[pos + common:], parent=mid)
                    mid.children[tokens[pos + common]] = new_node
                    self.node_count += 1
                    return new_node
                return mid

            pos += len(seg)
            node = child
            node.last_access = time.time()

        return node

    def evict_lru(self):
        """LRU é©±é€ï¼šç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„å¶èŠ‚ç‚¹ï¼ˆref_count == 0ï¼‰ï¼Œé‡Šæ”¾ KV Cache"""
        while self.node_count > self.max_nodes:
            victim = self._find_lru_leaf()
            if victim is None or victim is self.root:
                break
            parent = victim.parent
            # ä»çˆ¶èŠ‚ç‚¹ä¸­åˆ é™¤
            key = victim.tokens[0]
            if key in parent.children:
                del parent.children[key]
            self.node_count -= 1
            # å¦‚æœçˆ¶èŠ‚ç‚¹åªå‰©ä¸€ä¸ªå­èŠ‚ç‚¹ï¼Œåˆå¹¶ä»¥ä¿æŒç´§å‡‘
            if parent is not self.root and len(parent.children) == 1:
                self._merge_with_child(parent)

    def _find_lru_leaf(self) -> Optional[RadixTreeNode]:
        """æ‰¾åˆ° ref_count == 0 ä¸” last_access æœ€å°çš„å¶èŠ‚ç‚¹"""
        best, best_time = None, float('inf')
        def dfs(node):
            nonlocal best, best_time
            if not node.children and node.ref_count == 0 and node is not self.root:
                if node.last_access < best_time:
                    best, best_time = node, node.last_access
            for child in node.children.values():
                dfs(child)
        dfs(self.root)
        return best

    def _merge_with_child(self, node: RadixTreeNode):
        """å°†åªæœ‰å•ä¸ªå­èŠ‚ç‚¹çš„ä¸­é—´èŠ‚ç‚¹ä¸å…¶å­èŠ‚ç‚¹åˆå¹¶"""
        child = list(node.children.values())[0]
        child.tokens = node.tokens + child.tokens
        child.parent = node.parent
        node.parent.children[node.tokens[0]] = child
        self.node_count -= 1


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# æ¨¡æ‹Ÿå›¾ä¸­æ­¥éª¤ (1)-(9) çš„ç®€åŒ–ç‰ˆæœ¬
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
tree = RadixTree(max_nodes=8)

# ç”¨æ•´æ•°æ¨¡æ‹Ÿ token idï¼Œç”¨å­—æ¯æ ‡æ³¨å«ä¹‰æ–¹ä¾¿ç†è§£
SYS = (10, 11, 12)       # "You are a helpful assistant."
HELLO = (20, 21)          # "User: Hello! Assistant: Hi!"
WHAT = (30, 31, 32)       # "User: What can you do? Assistant: I can ..."
SOLVE = (40, 41)          # "User: Solve this problem ... Assistant: Sure! ..."
STORY = (50, 51)          # "User: Write a story ... Assistant: Sure! ..."

# --- æ­¥éª¤ (2): æ’å…¥ç¬¬ä¸€ä¸ªå¯¹è¯ [sys + hello] â†’ èŠ‚ç‚¹ a ---
seq_a = SYS + HELLO
tree.insert(seq_a)
cached = tree.match_prefix(seq_a)
print(f"Step 2: insert chat_a, cached prefix = {cached} tokens")  # å…¨éƒ¨å‘½ä¸­

# --- æ­¥éª¤ (3): æ’å…¥ç¬¬äºŒä¸ªå¯¹è¯ [sys + solve] â†’ èŠ‚ç‚¹ b ---
seq_b = SYS + SOLVE
cached = tree.match_prefix(seq_b)
print(f"Step 3: insert chat_b, reused prefix = {cached} tokens (= system prompt)")
tree.insert(seq_b)

# --- æ­¥éª¤ (4): ä¸¤ä¸ªå¯¹è¯å…±äº« sys å‰ç¼€ï¼Œåˆ†å‰å‡º hello / what ä¸¤ä¸ªåˆ†æ”¯ ---
seq_c = SYS + WHAT
cached = tree.match_prefix(seq_c)
print(f"Step 4: insert chat_c, reused prefix = {cached} tokens")
tree.insert(seq_c)

# --- æ­¥éª¤ (6): fork äº§ç”Ÿå¤šä¸ªå¹¶è¡Œåˆ†æ”¯ï¼Œå…±äº«çˆ¶èŠ‚ç‚¹å‰ç¼€ ---
Q1 = (60, 61)  # "Question 1: ... Answer 1: ..."
Q2 = (62, 63)  # "Question 2: ... Answer 2: ..."
Q3 = (64, 65)  # "Question 3: ..."
fork_base = SYS + HELLO
for q in [Q1, Q2, Q3]:
    seq = fork_base + q
    cached = tree.match_prefix(seq)
    tree.insert(seq)
    print(f"Step 6 (fork): branch {q}, reused prefix = {cached} tokens")

print(f"\nTotal nodes in tree: {tree.node_count}")
```

è¿è¡Œç»“æœå±•ç¤ºäº† Radix Tree çš„æ ¸å¿ƒä¼˜åŠ¿â€”â€”**è‡ªåŠ¨å‰ç¼€å¤ç”¨**ï¼š

```
Step 2: insert chat_a, cached prefix = 5 tokens
Step 3: insert chat_b, reused prefix = 3 tokens (= system prompt)
Step 4: insert chat_c, reused prefix = 3 tokens
Step 6 (fork): branch (60, 61), reused prefix = 5 tokens
Step 6 (fork): branch (62, 63), reused prefix = 5 tokens
Step 6 (fork): branch (64, 65), reused prefix = 5 tokens
```

æ¯ä¸ª fork åˆ†æ”¯éƒ½è‡ªåŠ¨å¤ç”¨äº† 5 ä¸ª token çš„ KV Cacheï¼ˆsystem prompt + helloï¼‰ï¼Œå¯¹åº”å›¾ä¸­æ­¥éª¤ (6) ä¸­ `e` èŠ‚ç‚¹å…±äº« `a` çš„å‰ç¼€ã€‚å®é™… SGLang ä¸­ï¼Œè¿™æ„å‘³ç€è¿™äº› token çš„ Prefill è®¡ç®—å¯ä»¥å®Œå…¨è·³è¿‡ã€‚

ä¸ vLLM çš„ PagedAttention å¯¹æ¯”ï¼š

| ç‰¹æ€§ | PagedAttention | RadixAttention |
|------|---------------|----------------|
| å…³æ³¨ç‚¹ | å•ä¸ªè¯·æ±‚çš„å†…å­˜ç®¡ç† | è·¨è¯·æ±‚çš„ KV Cache å¤ç”¨ |
| æ•°æ®ç»“æ„ | é¡µè¡¨ (Block Table) | åŸºæ•°æ ‘ (Radix Tree) |
| å…±äº«ç²’åº¦ | ç‰©ç†å—çº§åˆ« | å‰ç¼€çº§åˆ« |
| è‡ªåŠ¨å‘ç° | éœ€è¦æ˜¾å¼æ ‡è®° | è‡ªåŠ¨åŒ¹é…æœ€é•¿å…¬å…±å‰ç¼€ |

### 2. å‹ç¼©æœ‰é™çŠ¶æ€æœºï¼ˆCompressed FSMï¼‰

åœ¨ LLM ç¨‹åºä¸­ï¼Œç”¨æˆ·ç»å¸¸éœ€è¦çº¦æŸæ¨¡å‹è¾“å‡ºéµå¾ªç‰¹å®šæ ¼å¼ï¼ˆå¦‚ JSON Schemaï¼‰ã€‚SGLang é€šè¿‡ `regex` å‚æ•°æ”¯æŒæ­£åˆ™è¡¨è¾¾å¼çº¦æŸã€‚

#### ä¼ ç»Ÿæ–¹æ³•çš„é—®é¢˜

ç°æœ‰ç³»ç»Ÿå°†æ­£åˆ™è¡¨è¾¾å¼è½¬æ¢ä¸º**æœ‰é™çŠ¶æ€æœºï¼ˆFSMï¼‰**ï¼Œç„¶ååœ¨è§£ç è¿‡ç¨‹ä¸­é€ token æ‰§è¡Œï¼š

1. ç»´æŠ¤å½“å‰ FSM çŠ¶æ€
2. ä»åç»­çŠ¶æ€ä¸­æ£€ç´¢å…è®¸çš„ tokenï¼Œéå†æ•´ä¸ªè¯è¡¨
3. å°†æ— æ•ˆ token çš„æ¦‚ç‡è®¾ä¸ºé›¶ï¼ˆmaskï¼‰
4. é‡‡æ ·ä¸€ä¸ª tokenï¼Œè½¬ç§»åˆ°ä¸‹ä¸€ä¸ª FSM çŠ¶æ€

é—®é¢˜åœ¨äºï¼šå½“ä¸€æ®µè¾“å‡ºæ˜¯**ç¡®å®šæ€§çš„**ï¼ˆå¦‚ JSON ä¸­çš„ `{"summary": "` è¿™æ®µå¸¸é‡å­—ç¬¦ä¸²ï¼‰ï¼Œå³ä½¿æ¯ä¸ªä½ç½®åªæœ‰å”¯ä¸€åˆæ³•çš„ä¸‹ä¸€ä¸ª tokenï¼Œä¼ ç»Ÿæ–¹æ³•ä»ç„¶éœ€è¦**é€ token è§£ç **ï¼Œæ¯ä¸ª token éƒ½è¦åšä¸€æ¬¡å®Œæ•´çš„ forward passã€‚è¿™æ˜¾ç„¶æ˜¯å·¨å¤§çš„æµªè´¹ã€‚

#### SGLang çš„å‹ç¼©ä¼˜åŒ–

SGLang åˆ†æ FSM çš„çŠ¶æ€è½¬ç§»å›¾ï¼Œå‘ç°è®¸å¤šçŠ¶æ€åªæœ‰**å”¯ä¸€åˆæ³•çš„è½¬ç§»è·¯å¾„**ï¼ˆç§°ä¸º singular-transition edgesï¼‰ã€‚è¿™äº›è¿ç»­çš„å•ä¸€è½¬ç§»è¾¹å¯ä»¥è¢«**å‹ç¼©ä¸ºä¸€æ¡è¾¹**ï¼Œåœ¨ä¸€æ¬¡ forward pass ä¸­ç›´æ¥è·³è¿‡å¤šä¸ªç¡®å®šæ€§ tokenã€‚

ä¸‹é¢ç”¨ ASCII å›¾è¯´æ˜è¿™ä¸ªè¿‡ç¨‹ï¼ˆå¯¹åº”è®ºæ–‡ Figure 4ï¼‰ï¼š

```
(a) åŸå§‹ FSMï¼ˆä»¥ JSON schema æ­£åˆ™ä¸ºä¾‹ï¼‰:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  regex: \{"summary": "[\w\d\s]+\.", "grade": "[ABCD][+-]?"\} â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    s0 â”€{â”€â†’ s1 â”€"â”€â†’ s2 â”€sâ”€â†’ s3 â”€uâ”€â†’ s4 â”€mâ”€â†’ s5 â”€mâ”€â†’ s6 â”€aâ”€â†’ s7
    â”€râ”€â†’ s8 â”€yâ”€â†’ s9 â”€"â”€â†’ s10 â”€:â”€â†’ s11 â”€ â”€â†’ s12 â”€"â”€â†’ s13
    â”€[è‡ªç”±ç”Ÿæˆ \w\d\s]+â”€â†’ s14 â”€.â”€â†’ s15 â”€"â”€â†’ s16 â”€,â”€â†’ s17 ...

    å…¶ä¸­ s0â†’s13 æ¯ä¸ªçŠ¶æ€éƒ½åªæœ‰å”¯ä¸€åç»§ (singular transition)
    åªæœ‰ s13 æœ‰å¤šä¸ªåç»§ (å¯ä»¥ç”Ÿæˆä»»æ„ \w\d\s å­—ç¬¦)

(b) å‹ç¼©åçš„ FSM:
    s0 â”€â”€{"summary": "â”€â”€â†’ s13 â”€[\w\d\s]+â”€â†’ s14 â”€â”€.", "grade": "â”€â”€â†’ ...
         (ä¸€æ¡å‹ç¼©è¾¹)         (è‡ªç”±ç”Ÿæˆ)         (ä¸€æ¡å‹ç¼©è¾¹)

(c) ä¼ ç»Ÿé€ token è§£ç  (æ¯ä¸ªç®­å¤´ = ä¸€æ¬¡ forward pass):
    { â†’ " â†’ s â†’ u â†’ m â†’ m â†’ a â†’ r â†’ y â†’ " â†’ : â†’ " â†’ [word1] â†’ [word2] â†’ ...
    â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘    â†‘         â†‘
    12 æ¬¡ forward pass ä»…ç”¨äºè§£ç ç¡®å®šæ€§å‰ç¼€ï¼

(d) å‹ç¼©è§£ç  (è·³è¿‡ç¡®å®šæ€§ token):
    {"summary": "  â†’  [word1]  â†’  [word2]  â†’  ...  â†’  .", "grade": "  â†’  A  â†’  "  â†’  }
    (1 æ¬¡ forward)   (æ­£å¸¸è§£ç )   (æ­£å¸¸è§£ç )         (1 æ¬¡ forward)   (è§£ç ) (è§£ç )
```

æ ¸å¿ƒåœ¨äºï¼š**å‹ç¼©è½¬ç§»è¾¹ä¸Šçš„å¤šä¸ª token å¯ä»¥åœ¨ä¸€æ¬¡ forward pass ä¸­å®Œæˆè§£ç **ï¼Œå› ä¸ºè¿™äº› token æ˜¯å®Œå…¨ç¡®å®šçš„ï¼Œä¸éœ€è¦æ¨¡å‹"æ€è€ƒ"é€‰å“ªä¸ªã€‚

#### ä»£ç æ¼”ç¤º

ä¸‹é¢çš„ Python ä»£ç æ¼”ç¤ºäº†å‹ç¼© FSM çš„æ ¸å¿ƒé€»è¾‘â€”â€”å¦‚ä½•ä»æ­£åˆ™è¡¨è¾¾å¼æ„å»º FSMã€è¯†åˆ«å¯å‹ç¼©çš„çŠ¶æ€ã€å¹¶å¯¹æ¯”ä¼ ç»Ÿ vs å‹ç¼©è§£ç çš„æ­¥æ•°å·®å¼‚ï¼š

```python
import re
from collections import defaultdict

class FSMState:
    """æœ‰é™çŠ¶æ€æœºçš„ä¸€ä¸ªçŠ¶æ€"""
    def __init__(self, state_id: int):
        self.id = state_id
        self.transitions: dict[str, int] = {}  # å­—ç¬¦ â†’ ä¸‹ä¸€çŠ¶æ€ id
        self.is_accept = False                  # æ˜¯å¦ä¸ºæ¥å—çŠ¶æ€

class ConstrainedDecoder:
    """
    æ¼”ç¤ºä¼ ç»Ÿ FSM vs å‹ç¼© FSM çš„å—é™è§£ç å™¨ã€‚
    ä¸ºç®€åŒ–æ¼”ç¤ºï¼Œæˆ‘ä»¬ç›´æ¥æ‰‹åŠ¨æ„å»º FSM è€Œéä»æ­£åˆ™è¡¨è¾¾å¼è‡ªåŠ¨ç¼–è¯‘ã€‚
    """
    def __init__(self):
        self.states: list[FSMState] = []
        self.start_state = 0

    def add_state(self, is_accept=False) -> int:
        sid = len(self.states)
        state = FSMState(sid)
        state.is_accept = is_accept
        self.states.append(state)
        return sid

    def add_transition(self, from_state: int, char: str, to_state: int):
        self.states[from_state].transitions[char] = to_state

    def is_singular(self, state_id: int) -> bool:
        """åˆ¤æ–­ä¸€ä¸ªçŠ¶æ€æ˜¯å¦åªæœ‰å”¯ä¸€çš„è½¬ç§»ï¼ˆsingular transitionï¼‰"""
        return len(self.states[state_id].transitions) == 1

    def compress(self) -> list[tuple[int, int, str]]:
        """
        åˆ†æ FSMï¼Œæ‰¾å‡ºæ‰€æœ‰å¯å‹ç¼©çš„è¿ç»­ singular-transition è·¯å¾„ã€‚
        è¿”å›: [(èµ·å§‹çŠ¶æ€, ç»“æŸçŠ¶æ€, å‹ç¼©çš„å­—ç¬¦ä¸²), ...]
        """
        compressed_edges = []
        visited = set()
        for s in range(len(self.states)):
            if s in visited:
                continue
            if not self.is_singular(s):
                continue
            # æ‰¾åˆ°ä¸€æ¡è¿ç»­çš„ singular-transition é“¾
            chain_start = s
            chain_chars = ""
            current = s
            while self.is_singular(current):
                char = list(self.states[current].transitions.keys())[0]
                next_state = self.states[current].transitions[char]
                chain_chars += char
                visited.add(current)
                current = next_state
                if current == chain_start:  # é¿å…å¾ªç¯
                    break
            if len(chain_chars) > 1:  # åªæœ‰é•¿åº¦ > 1 æ‰å€¼å¾—å‹ç¼©
                compressed_edges.append((chain_start, current, chain_chars))
        return compressed_edges


def build_json_fsm() -> ConstrainedDecoder:
    """
    ä¸ºç®€åŒ–çš„ JSON schema æ„å»º FSM:
    {"summary": "[ä»»æ„æ–‡æœ¬].", "grade": "[A-D]"}

    çŠ¶æ€è½¬ç§»:
      s0 -{'- s1 -"- s2 -s- s3 -u- s4 -m- s5 -m- s6 -a- s7
      -r- s8 -y- s9 -"- s10 -:- s11 - - s12 -"- s13
      s13 -[a-z ]- s13 (è‡ªç”±ç”Ÿæˆå¾ªç¯)
      s13 -.- s14 -"- s15 -,- s16 - - s17 -"- s18
      -g- s19 -r- s20 -a- s21 -d- s22 -e- s23 -"- s24
      -:- s25 - - s26 -"- s27
      s27 -[A-D]- s28 -"- s29 -}- s30 (æ¥å—)
    """
    fsm = ConstrainedDecoder()

    # åˆ›å»ºæ‰€æœ‰çŠ¶æ€ (s0 - s30)
    for i in range(31):
        fsm.add_state(is_accept=(i == 30))

    # {"summary": " çš„ç¡®å®šæ€§å‰ç¼€ (s0 â†’ s13)
    prefix = '{"summary": "'
    for i, ch in enumerate(prefix):
        fsm.add_transition(i, ch, i + 1)

    # s13: è‡ªç”±ç”ŸæˆåŒºåŸŸ (å¯ä»¥è¾“å‡ºä»»æ„ [a-z ] å­—ç¬¦)
    for ch in "abcdefghijklmnopqrstuvwxyz ":
        fsm.add_transition(13, ch, 13)
    fsm.add_transition(13, ".", 14)  # å¥å·ç»“æŸè‡ªç”±ç”Ÿæˆ

    # .", "grade": " çš„ç¡®å®šæ€§åç¼€ (s14 â†’ s27)
    suffix = '", "grade": "'
    for i, ch in enumerate(suffix):
        fsm.add_transition(14 + i, ch, 14 + i + 1)

    # s27: ç­‰çº§é€‰æ‹© (A/B/C/D)
    for ch in "ABCD":
        fsm.add_transition(27, ch, 28)

    # "} ç»“æŸ
    fsm.add_transition(28, '"', 29)
    fsm.add_transition(29, '}', 30)

    return fsm


def simulate_decoding(fsm: ConstrainedDecoder, target: str):
    """æ¨¡æ‹Ÿä¼ ç»Ÿé€ token è§£ç """
    state = fsm.start_state
    steps = 0
    for ch in target:
        if ch in fsm.states[state].transitions:
            state = fsm.states[state].transitions[ch]
            steps += 1
        else:
            print(f"  âœ— éæ³•å­—ç¬¦ '{ch}' at state {state}")
            return steps
    return steps


def simulate_compressed_decoding(fsm: ConstrainedDecoder, target: str):
    """æ¨¡æ‹Ÿå‹ç¼© FSM è§£ç ï¼šç¡®å®šæ€§ç‰‡æ®µä¸€æ­¥å®Œæˆ"""
    compressed = fsm.compress()
    # æ„å»ºå‹ç¼©è·³è½¬è¡¨: {èµ·å§‹çŠ¶æ€: (ç»“æŸçŠ¶æ€, è·³è¿‡çš„å­—ç¬¦ä¸²)}
    jump_table = {start: (end, chars) for start, end, chars in compressed}

    state = fsm.start_state
    steps = 0
    pos = 0
    details = []

    while pos < len(target):
        if state in jump_table:
            end_state, chars = jump_table[state]
            # éªŒè¯ç›®æ ‡å­—ç¬¦ä¸²ä¸å‹ç¼©è·¯å¾„åŒ¹é…
            if target[pos:pos+len(chars)] == chars:
                details.append(f"  â†’ å‹ç¼©è·³è¿‡ '{chars}' ({len(chars)} tokens, 1 æ¬¡ forward)")
                pos += len(chars)
                state = end_state
                steps += 1  # åªç®— 1 æ­¥ï¼
                continue
        # æ­£å¸¸é€ token è§£ç 
        ch = target[pos]
        if ch in fsm.states[state].transitions:
            state = fsm.states[state].transitions[ch]
            details.append(f"  â†’ è§£ç  '{ch}' (1 token, 1 æ¬¡ forward)")
            pos += 1
            steps += 1
        else:
            break

    return steps, details


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# è¿è¡Œå¯¹æ¯”æ¼”ç¤º
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
fsm = build_json_fsm()
target = '{"summary": "this is a good paper.", "grade": "A"}'

# åˆ†æå‹ç¼©ç»“æœ
compressed = fsm.compress()
print("=== å‹ç¼© FSM åˆ†æ ===")
for start, end, chars in compressed:
    print(f"  çŠ¶æ€ s{start} â†’ s{end}: å‹ç¼© '{chars}' ({len(chars)} ä¸ªç¡®å®šæ€§ token)")

# ä¼ ç»Ÿè§£ç 
trad_steps = simulate_decoding(fsm, target)
print(f"\n=== ä¼ ç»Ÿé€ token è§£ç  ===")
print(f"  æ€»æ­¥æ•° (forward passes): {trad_steps}")

# å‹ç¼©è§£ç 
comp_steps, details = simulate_compressed_decoding(fsm, target)
print(f"\n=== å‹ç¼© FSM è§£ç  ===")
for d in details:
    print(d)
print(f"  æ€»æ­¥æ•° (forward passes): {comp_steps}")
print(f"\nåŠ é€Ÿæ¯”: {trad_steps / comp_steps:.1f}x (ä¼ ç»Ÿ {trad_steps} æ­¥ â†’ å‹ç¼© {comp_steps} æ­¥)")
```

è¿è¡Œç»“æœï¼š

```
=== å‹ç¼© FSM åˆ†æ ===
  çŠ¶æ€ s0 â†’ s13: å‹ç¼© '{"summary": "' (13 ä¸ªç¡®å®šæ€§ token)
  çŠ¶æ€ s14 â†’ s27: å‹ç¼© '", "grade": "' (13 ä¸ªç¡®å®šæ€§ token)

=== ä¼ ç»Ÿé€ token è§£ç  ===
  æ€»æ­¥æ•° (forward passes): 48

=== å‹ç¼© FSM è§£ç  ===
  â†’ å‹ç¼©è·³è¿‡ '{"summary": "' (13 tokens, 1 æ¬¡ forward)
  â†’ è§£ç  't' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'h' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'i' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  's' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  ' ' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'i' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  's' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  ' ' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'a' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  ' ' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'g' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'o' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'o' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'd' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  ' ' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'p' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'a' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'p' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'e' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  'r' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  '.' (1 token, 1 æ¬¡ forward)
  â†’ å‹ç¼©è·³è¿‡ '", "grade": "' (13 tokens, 1 æ¬¡ forward)
  â†’ è§£ç  'A' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  '"' (1 token, 1 æ¬¡ forward)
  â†’ è§£ç  '}' (1 token, 1 æ¬¡ forward)
  æ€»æ­¥æ•° (forward passes): 24

åŠ é€Ÿæ¯”: 2.0x (ä¼ ç»Ÿ 48 æ­¥ â†’ å‹ç¼© 24 æ­¥)
```

å¯ä»¥çœ‹åˆ°ï¼Œ26 ä¸ªç¡®å®šæ€§ tokenï¼ˆä¸¤æ®µ JSON ç»“æ„å­—ç¬¦ï¼‰è¢«å‹ç¼©ä¸ºä»… 2 æ¬¡ forward passï¼Œæ€»è§£ç æ­¥æ•°ä» 48 é™åˆ° 24ï¼Œå®ç°äº† **2x** åŠ é€Ÿã€‚å®é™…ç”Ÿäº§ä¸­ï¼ŒJSON Schema è¶Šå¤æ‚ã€ç»“æ„æ€§å­—ç¬¦è¶Šå¤šï¼Œå‹ç¼©æ¯”è¶Šé«˜ã€‚

### 3. API æ¨æµ‹æ‰§è¡Œï¼ˆSpeculative Executionï¼‰

å€Ÿé‰´ CPU åˆ†æ”¯é¢„æµ‹çš„æ€æƒ³ï¼Œå½“ç¨‹åºå­˜åœ¨ **æ•°æ®ä¾èµ–**ï¼ˆå¦‚ `select` çš„ç»“æœå†³å®šåç»­åˆ†æ”¯ï¼‰æ—¶ï¼š

1. **æ¨æµ‹æ‰§è¡Œ**ï¼šä¸ç­‰å¾… `select` ç»“æœï¼Œä¹è§‚åœ°å¯åŠ¨æ‰€æœ‰å¯èƒ½çš„åˆ†æ”¯
2. **éªŒè¯ä¸å›æ»š**ï¼šå½“ `select` ç»“æœè¿”å›åï¼Œä¿ç•™æ­£ç¡®åˆ†æ”¯ï¼Œä¸¢å¼ƒé”™è¯¯åˆ†æ”¯
3. **æ”¶ç›Š**ï¼šå°†ä¸²è¡Œçš„"è¯·æ±‚-ç­‰å¾…-è¯·æ±‚"å˜ä¸ºæµæ°´çº¿å¼æ‰§è¡Œï¼Œéšè—ç½‘ç»œå»¶è¿Ÿ

è¿™åœ¨è°ƒç”¨è¿œç¨‹ APIï¼ˆå¦‚ OpenAI APIï¼‰æ—¶æ•ˆæœå°¤å…¶æ˜¾è‘—ï¼Œå› ä¸ºç½‘ç»œå¾€è¿”å»¶è¿Ÿè¿œå¤§äºè®¡ç®—å¼€é”€ã€‚

</div>

<div class="lang-en">

## Runtime Optimizations

### 1. RadixAttention: Automatic KV Cache Reuse

**Core idea**: Use a **Radix Tree** data structure to manage KV Caches across all requests, automatically discovering and reusing shared prefixes.

```
             [System Prompt]
              /           \
     [User: Image+Essay]   [User: Other...]
      /         \
  [Fork 0]   [Fork 1]   [Fork 2]
  Clarity    Originality  Evidence
```

**Key features**:

- **Automatic prefix matching**: When a new request arrives, the Radix Tree finds the longest matching prefix automatically
- **LRU eviction**: When memory is low, cached entries are evicted by least-recently-used policy
- **Cross-request reuse**: Requests sharing the same system prompt or context prefix can directly reuse the cached KV
- **Fork optimization**: Forks created by `fork()` naturally share the parent node's KV Cache

The figure below illustrates how the Radix Tree evolves while serving different LLM requests (steps 1â€“9), including node insertion, prefix sharing, forking, and LRU eviction:

![RadixAttention Radix Tree Operations](/img/inference/SGLangTree.png)

Below is a simplified Python implementation demonstrating the core Radix Tree logic for KV Cache management, corresponding to the key operations shown above:

```python
import time
from typing import Optional

class RadixTreeNode:
    """Radix Tree node: each node stores a segment of tokens and its KV Cache."""
    def __init__(self, tokens: tuple = (), parent=None):
        self.tokens = tokens                  # Token segment stored in this node
        self.children: dict[int, 'RadixTreeNode'] = {}  # Child map (first token â†’ child)
        self.parent: Optional['RadixTreeNode'] = parent
        self.ref_count = 0                    # Number of active requests using this node
        self.last_access = time.time()        # LRU timestamp

class RadixTree:
    """
    Simplified Radix Tree simulating SGLang RadixAttention's KV Cache management.
    - insert(): Insert a token sequence, automatically reusing existing prefixes (steps 2-7)
    - match_prefix(): Find the longest matching prefix, returning reusable KV Cache length
    - evict_lru(): LRU eviction to free unused cache (steps 5, 8, 9)
    """
    def __init__(self, max_nodes: int = 20):
        self.root = RadixTreeNode()
        self.max_nodes = max_nodes
        self.node_count = 1

    def match_prefix(self, tokens: tuple) -> int:
        """Find the longest matching prefix; returns number of cached tokens (skippable prefill)."""
        node = self.root
        matched = 0
        pos = 0
        while pos < len(tokens):
            first_token = tokens[pos]
            if first_token not in node.children:
                break
            child = node.children[first_token]
            seg_len = len(child.tokens)
            end = min(pos + seg_len, len(tokens))
            match_len = 0
            for i in range(end - pos):
                if child.tokens[i] != tokens[pos + i]:
                    break
                match_len += 1
            matched += match_len
            pos += match_len
            if match_len < seg_len:
                break  # Partial match, stop
            node = child
            node.last_access = time.time()
        return matched

    def insert(self, tokens: tuple) -> 'RadixTreeNode':
        """Insert a token sequence, reusing shared prefixes and splitting nodes as needed."""
        node = self.root
        pos = 0
        while pos < len(tokens):
            first_token = tokens[pos]
            if first_token not in node.children:
                # No matching child â†’ create new leaf node
                new_node = RadixTreeNode(tokens[pos:], parent=node)
                node.children[first_token] = new_node
                self.node_count += 1
                return new_node

            child = node.children[first_token]
            seg = child.tokens
            common = 0
            for i in range(min(len(seg), len(tokens) - pos)):
                if seg[i] != tokens[pos + i]:
                    break
                common += 1

            if common < len(seg):
                # Partial match â†’ split node (step 4: shared prefix splits)
                #   Before: parent â†’ child("ABCDE")
                #   After:  parent â†’ mid("AB") â†’ child("CDE")
                #                              â†’ new("CF...")
                mid = RadixTreeNode(seg[:common], parent=node)
                child.tokens = seg[common:]
                child.parent = mid
                mid.children[child.tokens[0]] = child
                node.children[first_token] = mid
                self.node_count += 1

                if pos + common < len(tokens):
                    new_node = RadixTreeNode(tokens[pos + common:], parent=mid)
                    mid.children[tokens[pos + common]] = new_node
                    self.node_count += 1
                    return new_node
                return mid

            pos += len(seg)
            node = child
            node.last_access = time.time()
        return node

    def evict_lru(self):
        """LRU eviction: remove the least-recently-used leaf node (ref_count == 0)."""
        while self.node_count > self.max_nodes:
            victim = self._find_lru_leaf()
            if victim is None or victim is self.root:
                break
            parent = victim.parent
            key = victim.tokens[0]
            if key in parent.children:
                del parent.children[key]
            self.node_count -= 1
            if parent is not self.root and len(parent.children) == 1:
                self._merge_with_child(parent)

    def _find_lru_leaf(self) -> Optional[RadixTreeNode]:
        best, best_time = None, float('inf')
        def dfs(node):
            nonlocal best, best_time
            if not node.children and node.ref_count == 0 and node is not self.root:
                if node.last_access < best_time:
                    best, best_time = node, node.last_access
            for child in node.children.values():
                dfs(child)
        dfs(self.root)
        return best

    def _merge_with_child(self, node: RadixTreeNode):
        child = list(node.children.values())[0]
        child.tokens = node.tokens + child.tokens
        child.parent = node.parent
        node.parent.children[node.tokens[0]] = child
        self.node_count -= 1


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Simulating the figure's steps (1)-(9) in simplified form
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
tree = RadixTree(max_nodes=8)

# Integer token ids with descriptive labels for clarity
SYS = (10, 11, 12)       # "You are a helpful assistant."
HELLO = (20, 21)          # "User: Hello! Assistant: Hi!"
WHAT = (30, 31, 32)       # "User: What can you do? Assistant: I can ..."
SOLVE = (40, 41)          # "User: Solve this problem ... Assistant: Sure! ..."
STORY = (50, 51)          # "User: Write a story ... Assistant: Sure! ..."

# --- Step (2): Insert first conversation [sys + hello] â†’ node a ---
seq_a = SYS + HELLO
tree.insert(seq_a)
cached = tree.match_prefix(seq_a)
print(f"Step 2: insert chat_a, cached prefix = {cached} tokens")

# --- Step (3): Insert second conversation [sys + solve] â†’ node b ---
seq_b = SYS + SOLVE
cached = tree.match_prefix(seq_b)
print(f"Step 3: insert chat_b, reused prefix = {cached} tokens (= system prompt)")
tree.insert(seq_b)

# --- Step (4): Two conversations share sys prefix, branch into hello / what ---
seq_c = SYS + WHAT
cached = tree.match_prefix(seq_c)
print(f"Step 4: insert chat_c, reused prefix = {cached} tokens")
tree.insert(seq_c)

# --- Step (6): fork creates parallel branches sharing the parent prefix ---
Q1 = (60, 61)  # "Question 1: ... Answer 1: ..."
Q2 = (62, 63)  # "Question 2: ... Answer 2: ..."
Q3 = (64, 65)  # "Question 3: ..."
fork_base = SYS + HELLO
for q in [Q1, Q2, Q3]:
    seq = fork_base + q
    cached = tree.match_prefix(seq)
    tree.insert(seq)
    print(f"Step 6 (fork): branch {q}, reused prefix = {cached} tokens")

print(f"\nTotal nodes in tree: {tree.node_count}")
```

Sample output showing the core advantage â€” **automatic prefix reuse**:

```
Step 2: insert chat_a, cached prefix = 5 tokens
Step 3: insert chat_b, reused prefix = 3 tokens (= system prompt)
Step 4: insert chat_c, reused prefix = 3 tokens
Step 6 (fork): branch (60, 61), reused prefix = 5 tokens
Step 6 (fork): branch (62, 63), reused prefix = 5 tokens
Step 6 (fork): branch (64, 65), reused prefix = 5 tokens
```

Each fork branch automatically reuses 5 tokens of KV Cache (system prompt + hello), corresponding to step (6) in the figure where node `e` shares the prefix of node `a`. In production SGLang, this means the prefill computation for those tokens can be entirely skipped.

Comparison with vLLM's PagedAttention:

| Aspect | PagedAttention | RadixAttention |
|--------|---------------|----------------|
| Focus | Memory management for a single request | KV Cache reuse across requests |
| Data structure | Block Table (page table) | Radix Tree |
| Sharing granularity | Physical block level | Prefix level |
| Discovery | Requires explicit marking | Automatic longest common prefix matching |

### 2. Compressed Finite State Machine (Compressed FSM)

In LLM programs, users often need to constrain model output to follow specific formats (e.g., JSON Schema). SGLang supports this through the `regex` parameter.

#### The Problem with Traditional Approaches

Existing systems convert the regular expression into a **Finite State Machine (FSM)**, then perform token-by-token constrained decoding:

1. Maintain the current FSM state
2. Retrieve allowed tokens from successor states by scanning the entire vocabulary
3. Set invalid token probabilities to zero (mask)
4. Sample one token, transition to the next FSM state

The problem: when a portion of the output is **deterministic** (e.g., the constant string `{"summary": "` in JSON), even though each position has only one valid next token, the traditional approach still requires **token-by-token decoding** â€” one full forward pass per token. This is clearly wasteful.

#### SGLang's Compression Optimization

SGLang analyzes the FSM's state transition graph and identifies states with a **single valid transition** (called singular-transition edges). Consecutive singular-transition edges can be **compressed into a single edge**, skipping multiple deterministic tokens in one forward pass.

Below is an ASCII illustration of this process (corresponding to Figure 4 in the paper):

```
(a) Original FSM (for a JSON schema regex):
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  regex: \{"summary": "[\w\d\s]+\.", "grade": "[ABCD][+-]?"\} â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    s0 â”€{â”€â†’ s1 â”€"â”€â†’ s2 â”€sâ”€â†’ s3 â”€uâ”€â†’ s4 â”€mâ”€â†’ s5 â”€mâ”€â†’ s6 â”€aâ”€â†’ s7
    â”€râ”€â†’ s8 â”€yâ”€â†’ s9 â”€"â”€â†’ s10 â”€:â”€â†’ s11 â”€ â”€â†’ s12 â”€"â”€â†’ s13
    â”€[free generate \w\d\s]+â”€â†’ s14 â”€.â”€â†’ s15 â”€"â”€â†’ s16 â”€,â”€â†’ s17 ...

    States s0â†’s13 each have exactly one successor (singular transition)
    Only s13 has multiple successors (can generate any \w\d\s character)

(b) Compressed FSM:
    s0 â”€â”€{"summary": "â”€â”€â†’ s13 â”€[\w\d\s]+â”€â†’ s14 â”€â”€.", "grade": "â”€â”€â†’ ...
         (one compressed edge)   (free gen)         (one compressed edge)

(c) Traditional token-by-token decoding (each arrow = one forward pass):
    { â†’ " â†’ s â†’ u â†’ m â†’ m â†’ a â†’ r â†’ y â†’ " â†’ : â†’ " â†’ [word1] â†’ [word2] â†’ ...
    â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘    â†‘         â†‘
    12 forward passes just to decode the deterministic prefix!

(d) Compressed decoding (skip deterministic tokens):
    {"summary": "  â†’  [word1]  â†’  [word2]  â†’  ...  â†’  .", "grade": "  â†’  A  â†’  "  â†’  }
    (1 forward)     (normal)     (normal)              (1 forward)     (decode)(decode)
```

The key insight: **multiple tokens on compressed transition edges can be decoded in a single forward pass**, since these tokens are fully deterministic and require no model "deliberation".

#### Code Demonstration

The following Python code demonstrates the core logic of Compressed FSM â€” building an FSM from a regex pattern, identifying compressible states, and comparing traditional vs. compressed decoding step counts:

```python
import re
from collections import defaultdict

class FSMState:
    """A single state in the Finite State Machine."""
    def __init__(self, state_id: int):
        self.id = state_id
        self.transitions: dict[str, int] = {}  # char â†’ next state id
        self.is_accept = False

class ConstrainedDecoder:
    """
    Demonstrates traditional FSM vs. Compressed FSM constrained decoding.
    For simplicity, we manually construct the FSM rather than compiling from regex.
    """
    def __init__(self):
        self.states: list[FSMState] = []
        self.start_state = 0

    def add_state(self, is_accept=False) -> int:
        sid = len(self.states)
        state = FSMState(sid)
        state.is_accept = is_accept
        self.states.append(state)
        return sid

    def add_transition(self, from_state: int, char: str, to_state: int):
        self.states[from_state].transitions[char] = to_state

    def is_singular(self, state_id: int) -> bool:
        """Check if a state has exactly one valid transition (singular)."""
        return len(self.states[state_id].transitions) == 1

    def compress(self) -> list[tuple[int, int, str]]:
        """
        Analyze the FSM and compress consecutive singular-transition edges.
        Returns: [(start_state, end_state, compressed_string), ...]
        """
        compressed_edges = []
        visited = set()
        for s in range(len(self.states)):
            if s in visited:
                continue
            if not self.is_singular(s):
                continue
            # Trace the chain of singular transitions
            chain_start = s
            chain_chars = ""
            current = s
            while self.is_singular(current):
                char = list(self.states[current].transitions.keys())[0]
                next_state = self.states[current].transitions[char]
                chain_chars += char
                visited.add(current)
                current = next_state
                if current == chain_start:
                    break
            if len(chain_chars) > 1:  # Only worth compressing if length > 1
                compressed_edges.append((chain_start, current, chain_chars))
        return compressed_edges


def build_json_fsm() -> ConstrainedDecoder:
    """
    Build an FSM for a simplified JSON schema:
    {"summary": "[free text].", "grade": "[A-D]"}

    Transitions:
      s0 -{'- s1 -"- s2 -s- s3 -u- s4 -m- s5 -m- s6 -a- s7
      -r- s8 -y- s9 -"- s10 -:- s11 - - s12 -"- s13
      s13 -[a-z ]- s13 (free generation loop)
      s13 -.- s14 -"- s15 -,- s16 - - s17 -"- s18
      -g- s19 -r- s20 -a- s21 -d- s22 -e- s23 -"- s24
      -:- s25 - - s26 -"- s27
      s27 -[A-D]- s28 -"- s29 -}- s30 (accept)
    """
    fsm = ConstrainedDecoder()

    # Create all states (s0 - s30)
    for i in range(31):
        fsm.add_state(is_accept=(i == 30))

    # Deterministic prefix: {"summary": " (s0 â†’ s13)
    prefix = '{"summary": "'
    for i, ch in enumerate(prefix):
        fsm.add_transition(i, ch, i + 1)

    # s13: Free generation zone (any [a-z ] character)
    for ch in "abcdefghijklmnopqrstuvwxyz ":
        fsm.add_transition(13, ch, 13)
    fsm.add_transition(13, ".", 14)  # Period ends free generation

    # Deterministic suffix: .", "grade": " (s14 â†’ s27)
    suffix = '", "grade": "'
    for i, ch in enumerate(suffix):
        fsm.add_transition(14 + i, ch, 14 + i + 1)

    # s27: Grade selection (A/B/C/D)
    for ch in "ABCD":
        fsm.add_transition(27, ch, 28)

    # Closing: "}
    fsm.add_transition(28, '"', 29)
    fsm.add_transition(29, '}', 30)

    return fsm


def simulate_decoding(fsm: ConstrainedDecoder, target: str):
    """Simulate traditional token-by-token decoding."""
    state = fsm.start_state
    steps = 0
    for ch in target:
        if ch in fsm.states[state].transitions:
            state = fsm.states[state].transitions[ch]
            steps += 1
    return steps


def simulate_compressed_decoding(fsm: ConstrainedDecoder, target: str):
    """Simulate compressed FSM decoding: deterministic segments in one step."""
    compressed = fsm.compress()
    jump_table = {start: (end, chars) for start, end, chars in compressed}

    state = fsm.start_state
    steps = 0
    pos = 0
    details = []

    while pos < len(target):
        if state in jump_table:
            end_state, chars = jump_table[state]
            if target[pos:pos+len(chars)] == chars:
                details.append(f"  â†’ skip '{chars}' ({len(chars)} tokens, 1 forward pass)")
                pos += len(chars)
                state = end_state
                steps += 1  # Only 1 step!
                continue
        ch = target[pos]
        if ch in fsm.states[state].transitions:
            state = fsm.states[state].transitions[ch]
            details.append(f"  â†’ decode '{ch}' (1 token, 1 forward pass)")
            pos += 1
            steps += 1

    return steps, details


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Run comparison
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
fsm = build_json_fsm()
target = '{"summary": "this is a good paper.", "grade": "A"}'

# Compression analysis
compressed = fsm.compress()
print("=== Compressed FSM Analysis ===")
for start, end, chars in compressed:
    print(f"  s{start} â†’ s{end}: compress '{chars}' ({len(chars)} deterministic tokens)")

# Traditional decoding
trad_steps = simulate_decoding(fsm, target)
print(f"\n=== Traditional Token-by-Token Decoding ===")
print(f"  Total steps (forward passes): {trad_steps}")

# Compressed decoding
comp_steps, details = simulate_compressed_decoding(fsm, target)
print(f"\n=== Compressed FSM Decoding ===")
for d in details:
    print(d)
print(f"  Total steps (forward passes): {comp_steps}")
print(f"\nSpeedup: {trad_steps / comp_steps:.1f}x ({trad_steps} steps â†’ {comp_steps} steps)")
```

Sample output:

```
=== Compressed FSM Analysis ===
  s0 â†’ s13: compress '{"summary": "' (13 deterministic tokens)
  s14 â†’ s27: compress '", "grade": "' (13 deterministic tokens)

=== Traditional Token-by-Token Decoding ===
  Total steps (forward passes): 48

=== Compressed FSM Decoding ===
  â†’ skip '{"summary": "' (13 tokens, 1 forward pass)
  â†’ decode 't' (1 token, 1 forward pass)
  ...
  â†’ decode '.' (1 token, 1 forward pass)
  â†’ skip '", "grade": "' (13 tokens, 1 forward pass)
  â†’ decode 'A' (1 token, 1 forward pass)
  â†’ decode '"' (1 token, 1 forward pass)
  â†’ decode '}' (1 token, 1 forward pass)
  Total steps (forward passes): 24

Speedup: 2.0x (48 steps â†’ 24 steps)
```

26 deterministic tokens (two JSON structural segments) are compressed to just 2 forward passes, reducing total decoding steps from 48 to 24 â€” a **2x** speedup. In production, the more complex the JSON Schema and the more structural characters it contains, the higher the compression ratio.

### 3. API Speculative Execution

Inspired by CPU branch prediction, when a program has **data dependencies** (e.g., the result of `select` determines subsequent branches):

1. **Speculate**: Without waiting for the `select` result, optimistically launch all possible branches
2. **Verify & rollback**: When the `select` result arrives, keep the correct branch and discard incorrect ones
3. **Benefit**: Transform sequential "request-wait-request" into pipelined execution, hiding network latency

This is especially effective when calling remote APIs (e.g., OpenAI API), where network round-trip latency far exceeds compute cost.

</div>

---

<div class="lang-zh">

## å®éªŒç»“æœ

### ååé‡æå‡

| ä»»åŠ¡ç±»å‹ | å¯¹æ¯”åŸºçº¿ | SGLang åŠ é€Ÿæ¯” |
|---------|---------|-------------|
| LLM è¯„å®¡ (å¤šç»´åº¦) | Guidance | **6.4x** |
| å¤šè½®å¯¹è¯ | vLLM | **3.1x** |
| Tree-of-Thought | LMQL | **2.2x** |
| JSON çº¦æŸè§£ç  | Outlines | **3.5x** |
| å°‘æ ·æœ¬å­¦ä¹  | åŸç”Ÿ API | **1.8x** |

### å…³é”®å‘ç°

- **RadixAttention** åœ¨å¤šè½®å¯¹è¯å’Œå…±äº«å‰ç¼€åœºæ™¯ä¸‹è´¡çŒ®æœ€å¤§çš„åŠ é€Ÿ
- **å‹ç¼© FSM** åœ¨ç»“æ„åŒ–è¾“å‡ºï¼ˆJSON/æ­£åˆ™ï¼‰åœºæ™¯ä¸‹åŠ é€Ÿæ˜¾è‘—
- **æ¨æµ‹æ‰§è¡Œ** åœ¨ä½¿ç”¨è¿œç¨‹ API æ—¶æ•ˆæœæœ€ä½³
- ä¸‰ç§ä¼˜åŒ–äº’ç›¸æ­£äº¤ï¼Œå¯ä»¥å åŠ ä½¿ç”¨

</div>

<div class="lang-en">

## Experimental Results

### Throughput Improvements

| Task Type | Baseline | SGLang Speedup |
|-----------|----------|---------------|
| LLM Judge (multi-dim) | Guidance | **6.4x** |
| Multi-turn Chat | vLLM | **3.1x** |
| Tree-of-Thought | LMQL | **2.2x** |
| JSON Constrained Decoding | Outlines | **3.5x** |
| Few-shot Learning | Raw API | **1.8x** |

### Key Findings

- **RadixAttention** contributes the largest speedup in multi-turn chat and shared-prefix scenarios
- **Compressed FSM** provides significant acceleration for structured output (JSON/regex) tasks
- **Speculative Execution** works best when using remote APIs
- The three optimizations are orthogonal and can be stacked together

</div>

---

<div class="lang-zh">

## ä¸ªäººæ€è€ƒ

1. **ç¼–ç¨‹èŒƒå¼åˆ›æ–°**ï¼šSGLang å°† LLM äº¤äº’ä»"è°ƒ API"æå‡ä¸º"å†™ç¨‹åº"ï¼Œ`gen()`ã€`select()`ã€`fork()` ç­‰åŸè¯­æå¤§åœ°æé«˜äº†è¡¨è¾¾åŠ›ï¼ŒåŒæ—¶è®©è¿è¡Œæ—¶æœ‰æ›´å¤šä¼˜åŒ–ç©ºé—´
2. **ä¸ vLLM çš„äº’è¡¥å…³ç³»**ï¼švLLM çš„ PagedAttention è§£å†³äº†å•è¯·æ±‚çš„å†…å­˜ç®¡ç†é—®é¢˜ï¼Œè€Œ SGLang çš„ RadixAttention è¿›ä¸€æ­¥è§£å†³äº†è·¨è¯·æ±‚çš„ KV Cache å¤ç”¨é—®é¢˜ã€‚ä¸¤è€…æ˜¯ä¸åŒå±‚æ¬¡çš„ä¼˜åŒ–
3. **çº¦æŸè§£ç çš„æœªæ¥**ï¼šå‹ç¼© FSM ä¸ºç»“æ„åŒ–è¾“å‡ºæä¾›äº†é«˜æ•ˆåŸºç¡€ï¼Œéšç€ Agent åº”ç”¨ï¼ˆå·¥å…·è°ƒç”¨ã€JSON è¾“å‡ºï¼‰çš„çˆ†å‘ï¼Œè¿™ä¸€æ–¹å‘çš„é‡è¦æ€§åªä¼šå¢åŠ 
4. **ç³»ç»Ÿ-è¯­è¨€ååŒè®¾è®¡**ï¼šSGLang çš„æ ¸å¿ƒå¯ç¤ºæ˜¯â€”â€”å¥½çš„è¯­è¨€æŠ½è±¡ä¸ä»…è®©ç¼–ç¨‹æ›´å®¹æ˜“ï¼Œè¿˜èƒ½æš´éœ²æ›´å¤šä¼˜åŒ–æœºä¼šç»™ç¼–è¯‘å™¨/è¿è¡Œæ—¶

## æ¨èé˜…è¯»

- [SGLang GitHub](https://github.com/sgl-project/sglang)
- [SGLang å®˜æ–¹æ–‡æ¡£](https://sgl-project.github.io/)
- [arXiv è®ºæ–‡](https://arxiv.org/abs/2312.07104)
- [Lianmin Zheng çš„åšå®¢](https://lmsys.org/blog/)

</div>

<div class="lang-en">

## Personal Thoughts

1. **Programming paradigm shift**: SGLang elevates LLM interaction from "calling APIs" to "writing programs". Primitives like `gen()`, `select()`, and `fork()` greatly increase expressiveness while giving the runtime more room for optimization
2. **Complementary to vLLM**: vLLM's PagedAttention addresses memory management for individual requests, while SGLang's RadixAttention solves cross-request KV Cache reuse â€” optimizations at different levels
3. **Future of constrained decoding**: Compressed FSM provides an efficient foundation for structured output. As Agent applications (tool calling, JSON output) explode, this direction will only grow in importance
4. **Co-design of system and language**: SGLang's key insight is that good language abstractions not only make programming easier, but also expose more optimization opportunities to the compiler/runtime

## Further Reading

- [SGLang GitHub](https://github.com/sgl-project/sglang)
- [SGLang Documentation](https://sgl-project.github.io/)
- [arXiv Paper](https://arxiv.org/abs/2312.07104)
- [Lianmin Zheng's Blog](https://lmsys.org/blog/)

</div>
