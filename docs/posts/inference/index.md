# ğŸš€ æ¨ç†å¼•æ“ä¸æœåŠ¡åŒ–

æ¢ç´¢ vLLMã€NVIDIA Triton Inference Serverã€TensorRT ç­‰æ¨ç†æ¡†æ¶ä¸éƒ¨ç½²æ–¹æ¡ˆã€‚

---

<div class="paper-grid">

<a class="paper-card" href="./vllm-paper">
  <div class="paper-icon">ğŸ“„</div>
  <div class="paper-body">
    <h3>vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention</h3>
    <p class="paper-meta">Woosuk Kwon et al. Â· UC Berkeley Â· 2023</p>
    <p class="paper-desc">æå‡º PagedAttention æœºåˆ¶ï¼Œé€šè¿‡è™šæ‹Ÿå†…å­˜åˆ†é¡µç®¡ç† KV Cacheï¼Œå¤§å¹…æå‡ LLM æ¨ç†ååé‡ï¼Œå‡å°‘å†…å­˜æµªè´¹ã€‚</p>
    <div class="paper-tags">
      <span class="tag">vLLM</span>
      <span class="tag">PagedAttention</span>
      <span class="tag">KV Cache</span>
      <span class="tag">LLM Serving</span>
    </div>
  </div>
</a>

<a class="paper-card" href="./tensorrt-llm">
  <div class="paper-icon">ğŸ“„</div>
  <div class="paper-body">
    <h3>TensorRT-LLM: A High-Performance Inference Framework for LLMs</h3>
    <p class="paper-meta">NVIDIA Â· 2024</p>
    <p class="paper-desc">NVIDIA æ¨å‡ºçš„é«˜æ€§èƒ½ LLM æ¨ç†æ¡†æ¶ï¼Œæ”¯æŒé‡åŒ–ã€Kernel èåˆã€In-flight Batching ç­‰æ ¸å¿ƒä¼˜åŒ–æŠ€æœ¯ã€‚</p>
    <div class="paper-tags">
      <span class="tag">TensorRT</span>
      <span class="tag">é‡åŒ–</span>
      <span class="tag">Kernel èåˆ</span>
      <span class="tag">NVIDIA</span>
    </div>
  </div>
</a>

<a class="paper-card" href="./triton-inference-server">
  <div class="paper-icon">ğŸ“„</div>
  <div class="paper-body">
    <h3>NVIDIA Triton Inference Server: æ¨¡å‹æœåŠ¡åŒ–éƒ¨ç½²å®è·µ</h3>
    <p class="paper-meta">NVIDIA Â· Triton Inference Server</p>
    <p class="paper-desc">å­¦ä¹  Triton Inference Server çš„æ¶æ„è®¾è®¡ã€æ¨¡å‹ä»“åº“ç®¡ç†ã€åŠ¨æ€æ‰¹å¤„ç†ä¸å¤šæ¨¡å‹ç¼–æ’ç­‰ç”Ÿäº§çº§éƒ¨ç½²æ–¹æ¡ˆã€‚</p>
    <div class="paper-tags">
      <span class="tag">Triton Server</span>
      <span class="tag">Model Serving</span>
      <span class="tag">Dynamic Batching</span>
    </div>
  </div>
</a>

<a class="paper-card" href="./sglang">
  <div class="paper-icon">ğŸ“„</div>
  <div class="paper-body">
    <h3>SGLang: Efficient Execution of Structured Language Model Programs</h3>
    <p class="paper-meta">Lianmin Zheng et al. Â· UC Berkeley & Stanford Â· 2024</p>
    <p class="paper-desc">æå‡ºç»“æ„åŒ–ç”Ÿæˆè¯­è¨€ SGLangï¼Œé€šè¿‡ RadixAttentionï¼ˆKV Cache è‡ªåŠ¨å¤ç”¨ï¼‰ã€å‹ç¼©æœ‰é™çŠ¶æ€æœºï¼ˆé«˜é€Ÿçº¦æŸè§£ç ï¼‰å’Œ API æ¨æµ‹æ‰§è¡Œä¸‰å¤§ä¼˜åŒ–ï¼Œå°†å¤æ‚ LLM ç¨‹åºåŠ é€Ÿæœ€é«˜ 6.4 å€ã€‚</p>
    <div class="paper-tags">
      <span class="tag">SGLang</span>
      <span class="tag">RadixAttention</span>
      <span class="tag">Constrained Decoding</span>
      <span class="tag">LLM Programming</span>
    </div>
  </div>
</a>

<a class="paper-card" href="./flash-attention">
  <div class="paper-icon">ğŸ“„</div>
  <div class="paper-body">
    <h3>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</h3>
    <p class="paper-meta">Tri Dao et al. Â· Stanford University Â· NeurIPS 2022</p>
    <p class="paper-desc">æå‡º IO æ„ŸçŸ¥çš„ç²¾ç¡®æ³¨æ„åŠ›ç®—æ³•ï¼Œé€šè¿‡åˆ†å—è®¡ç®—ï¼ˆTilingï¼‰å’Œåœ¨çº¿ Softmax é¿å…å®ä½“åŒ– NÂ² æ³¨æ„åŠ›çŸ©é˜µï¼Œå°†å†…å­˜å¤æ‚åº¦ä» O(NÂ²) é™è‡³ O(N)ï¼Œå¢™é’Ÿæ—¶é—´å¿« 2-4 å€ã€‚</p>
    <div class="paper-tags">
      <span class="tag">FlashAttention</span>
      <span class="tag">IO-Aware</span>
      <span class="tag">Tiling</span>
      <span class="tag">Kernel Fusion</span>
    </div>
  </div>
</a>

<a class="paper-card" href="./clipper">
  <div class="paper-icon">ğŸ“„</div>
  <div class="paper-body">
    <h3>Clipper: A Low-Latency Online Prediction Serving System</h3>
    <p class="paper-meta">Daniel Crankshaw et al. Â· UC Berkeley Â· NSDI 2017</p>
    <p class="paper-desc">æœ€æ—©ç³»ç»Ÿæ€§åœ°å°† ML æ¨¡å‹æ¨å‘åœ¨çº¿æ¨ç†æœåŠ¡çš„é€šç”¨ Serving ç³»ç»Ÿä¹‹ä¸€ï¼Œé€šè¿‡æ¨¡å‹æŠ½è±¡å±‚ï¼ˆå®¹å™¨åŒ– + è‡ªé€‚åº”æ‰¹å¤„ç†ï¼‰å’Œæ¨¡å‹é€‰æ‹©å±‚ï¼ˆBandit ç®—æ³• + é›†æˆå­¦ä¹ ï¼‰è§£å†³æ¡†æ¶ç¢ç‰‡åŒ–ä¸åœ¨çº¿æ¨¡å‹é€‰ä¼˜é—®é¢˜ã€‚</p>
    <div class="paper-tags">
      <span class="tag">Model Serving</span>
      <span class="tag">Adaptive Batching</span>
      <span class="tag">Bandit Algorithm</span>
      <span class="tag">Ensemble</span>
    </div>
  </div>
</a>

<a class="paper-card" href="./smoothquant">
  <div class="paper-icon">ğŸ“„</div>
  <div class="paper-body">
    <h3>SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models</h3>
    <p class="paper-meta">Guangxuan Xiao et al. Â· MIT & NVIDIA Â· ICML 2023</p>
    <p class="paper-desc">é€šè¿‡æ•°å­¦ç­‰ä»·çš„å¹³æ»‘å˜æ¢å°†æ¿€æ´»å€¼ä¸­çš„ç¦»ç¾¤å€¼è¿ç§»åˆ°æƒé‡ä¸Šï¼Œå®ç° W8A8 å…¨é‡åŒ– INT8 æ¨ç†ï¼Œåœ¨ OPT-175B ä¸Šç²¾åº¦æŸå¤±ä¸åˆ° 1%ï¼Œæ¨ç†åŠ é€Ÿ 1.56 å€ï¼Œæ˜¾å­˜èŠ‚çœè¿‘ 2 å€ã€‚</p>
    <div class="paper-tags">
      <span class="tag">Quantization</span>
      <span class="tag">INT8</span>
      <span class="tag">Post-Training</span>
      <span class="tag">Outlier Migration</span>
    </div>
  </div>
</a>

<a class="paper-card" href="./speculative-decoding">
  <div class="paper-icon">ğŸ“„</div>
  <div class="paper-body">
    <h3>Fast Inference from Transformers via Speculative Decoding</h3>
    <p class="paper-meta">Yaniv Leviathan et al. Â· Google Research Â· ICML 2023</p>
    <p class="paper-desc">ç”¨å°æ¨¡å‹çŒœæµ‹ã€å¤§æ¨¡å‹å¹¶è¡ŒéªŒè¯çš„æ–¹å¼æ— æŸåŠ é€Ÿè‡ªå›å½’è§£ç ï¼Œé€šè¿‡ç²¾å¿ƒè®¾è®¡çš„æ‹’ç»é‡‡æ ·ä¿è¯è¾“å‡ºåˆ†å¸ƒä¸åŸå§‹å¤§æ¨¡å‹å®Œå…¨ä¸€è‡´ï¼Œå®ç° 2-3 å€æ¨ç†åŠ é€Ÿã€‚</p>
    <div class="paper-tags">
      <span class="tag">Speculative Decoding</span>
      <span class="tag">Rejection Sampling</span>
      <span class="tag">Lossless Acceleration</span>
      <span class="tag">Autoregressive</span>
    </div>
  </div>
</a>

<a class="paper-card" href="./flash-decoding">
  <div class="paper-icon">ğŸ“</div>
  <div class="paper-body">
    <h3>Flash-Decoding for Long-Context Inference</h3>
    <p class="paper-meta">Tri Dao et al. Â· Stanford CRFM Â· 2023 Blog</p>
    <p class="paper-desc">åœ¨ FlashAttention åŸºç¡€ä¸Šå¢åŠ  KV åºåˆ—é•¿åº¦ç»´åº¦çš„å¹¶è¡Œæ‹†åˆ†ï¼Œé€šè¿‡ log-sum-exp å½’çº¦åˆå¹¶å„åˆ†å—ç»“æœï¼Œè®©è§£ç é˜¶æ®µæ³¨æ„åŠ›è®¡ç®—å……åˆ†åˆ©ç”¨ GPUï¼Œé•¿åºåˆ—ç«¯åˆ°ç«¯åŠ é€Ÿ 8 å€ã€‚</p>
    <div class="paper-tags">
      <span class="tag">Flash-Decoding</span>
      <span class="tag">KV Split</span>
      <span class="tag">GPU Utilization</span>
      <span class="tag">Long Context</span>
    </div>
  </div>
</a>

<a class="paper-card" href="./flash-decoding-pp">
  <div class="paper-icon">ğŸ“„</div>
  <div class="paper-body">
    <h3>FlashDecoding++: Faster Large Language Model Inference on GPUs</h3>
    <p class="paper-meta">Ke Hong et al. Â· æ¸…åå¤§å­¦ & ä¸Šäº¤ Â· arXiv 2023</p>
    <p class="paper-desc">é€šè¿‡ç»Ÿä¸€æœ€å¤§å€¼å®ç°å¼‚æ­¥ Softmax æ¶ˆé™¤ ~20% åŒæ­¥å¼€é”€ã€pad8+åŒç¼“å†²ä¼˜åŒ–æ‰å¹³ GEMMã€å¯å‘å¼æ•°æ®æµè‡ªé€‚åº”ç¡¬ä»¶ï¼Œåœ¨ Flash-Decoding åŸºç¡€ä¸Šç«¯åˆ°ç«¯å†åŠ é€Ÿ 1.37 å€ã€‚</p>
    <div class="paper-tags">
      <span class="tag">Async Softmax</span>
      <span class="tag">Flat GEMM</span>
      <span class="tag">Double Buffering</span>
      <span class="tag">Cross-Hardware</span>
    </div>
  </div>
</a>

</div>

::: tip ğŸ’¡ æŒç»­æ›´æ–°ä¸­
æ›´å¤šæ¨ç†å¼•æ“ä¸æœåŠ¡åŒ–ç›¸å…³çš„è®ºæ–‡è§£è¯»å’Œå­¦ä¹ ç¬”è®°å°†é™†ç»­æ›´æ–°ï¼Œæ•¬è¯·å…³æ³¨ï¼
:::

<style>
.paper-grid {
  display: grid;
  grid-template-columns: 1fr;
  gap: 16px;
  margin: 24px 0;
}

@media (min-width: 768px) {
  .paper-grid {
    grid-template-columns: repeat(2, 1fr);
  }
}

.paper-card {
  display: flex;
  gap: 16px;
  padding: 20px;
  border: 1px solid var(--vp-c-divider);
  border-radius: 12px;
  text-decoration: none !important;
  color: inherit !important;
  transition: all 0.3s ease;
  background: var(--vp-c-bg-soft);
}

.paper-card:hover {
  border-color: var(--vp-c-brand-1);
  box-shadow: 0 4px 16px rgba(0, 0, 0, 0.08);
  transform: translateY(-2px);
}

.paper-icon {
  font-size: 28px;
  flex-shrink: 0;
  margin-top: 2px;
}

.paper-body h3 {
  margin: 0 0 6px 0;
  font-size: 16px;
  font-weight: 600;
  line-height: 1.4;
  color: var(--vp-c-text-1);
}

.paper-meta {
  margin: 0 0 8px 0;
  font-size: 13px;
  color: var(--vp-c-text-3);
}

.paper-desc {
  margin: 0 0 12px 0;
  font-size: 14px;
  color: var(--vp-c-text-2);
  line-height: 1.6;
}

.paper-tags {
  display: flex;
  flex-wrap: wrap;
  gap: 6px;
}

.tag {
  padding: 2px 10px;
  font-size: 12px;
  border-radius: 999px;
  background: var(--vp-c-brand-soft);
  color: var(--vp-c-brand-1);
  font-weight: 500;
}
</style>
