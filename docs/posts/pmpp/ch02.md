---
title: "PMPP Ch02: Heterogeneous Data Parallel Computing"
date: 2026-02-19
---

# Ch02: Heterogeneous Data Parallel Computing

<p style="color: var(--vp-c-text-2); font-size: 14px;">
2026-02-19 &nbsp;·&nbsp; PMPP 专栏 &nbsp;·&nbsp; 第二章
</p>

> **本章信息**
> - **书名**: Programming Massively Parallel Processors: A Hands-on Approach (4th Edition)
> - **作者**: David B. Kirk, Wen-mei W. Hwu
> - **范围**: 第 2 章 Heterogeneous Data Parallel Computing
> - **配套代码**: [ch02_vecadd.cu](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch02_vecadd.cu)

## 一句话总结

本章通过 **向量加法** 这个最简单的例子，完整走通 CUDA 编程的全流程：理解数据并行的概念、编写第一个 Kernel 函数、掌握 GPU 内存管理与数据传输 API、学会配置线程网格——这是 CUDA 编程的 "Hello World"。

---

## 2.1 数据并行（Data Parallelism）

### 什么是数据并行？

**数据并行** 是指将同一个操作 **同时** 作用于数据集的不同元素。它是 GPU 编程的基础范式。

以向量加法为例——给定两个长度为 \(n\) 的向量 \(A\) 和 \(B\)，计算 \(C[i] = A[i] + B[i]\)：

```
串行执行 (CPU):                    数据并行 (GPU):

for i = 0 to n-1:                  所有 i 同时执行:
    C[i] = A[i] + B[i]              Thread i: C[i] = A[i] + B[i]

时间 ──────────────►               时间 ──►

 t0: C[0] = A[0] + B[0]            t0: C[0] = A[0] + B[0]
 t1: C[1] = A[1] + B[1]                C[1] = A[1] + B[1]
 t2: C[2] = A[2] + B[2]                C[2] = A[2] + B[2]
 ...                                    ...
 tn: C[n] = A[n] + B[n]                C[n] = A[n] + B[n]

总时间: O(n)                       总时间: O(1) (理想情况)
```

关键特征：每个输出元素 \(C[i]\) 的计算 **完全独立**——不依赖其他元素的结果。这种独立性使得它们可以被映射到不同的 GPU 线程上并行执行。

### 数据并行 vs 任务并行

| 对比维度 | 数据并行 | 任务并行 |
|---------|---------|---------|
| 核心思想 | 同一操作作用于不同数据 | 不同操作作用于不同（或相同）数据 |
| 典型场景 | 向量加法、矩阵乘法、图像滤波 | Web 服务器处理多个请求、流水线 |
| 可扩展性 | 随数据量线性扩展 | 受限于可分解的独立任务数 |
| GPU 适合度 | **极度适合** | 一般不适合 GPU |

GPU 最擅长的就是数据并行——因为它有成千上万的计算核心，每个核心执行相同的指令，只是处理不同的数据。

---

## 2.2 CUDA C 程序结构

一个 CUDA 程序由两部分代码组成：

- **Host 代码**：运行在 CPU 上，用标准 C/C++ 编写
- **Device 代码**：运行在 GPU 上，用 CUDA C 编写（C 的超集，带 GPU 专用扩展）

```
CUDA 程序结构:

    源文件 (.cu)
    ┌─────────────────────────────────────────┐
    │                                         │
    │   // Host 代码 (CPU 上执行)              │
    │   void host_function() { ... }          │
    │   int main() {                          │
    │       // 内存管理、数据传输、Kernel 启动   │
    │       kernel<<<grid, block>>>(...);      │
    │   }                                     │
    │                                         │
    │   // Device 代码 (GPU 上执行)            │
    │   __global__ void kernel(...) {         │
    │       // 每个线程执行这段代码             │
    │   }                                     │
    │                                         │
    └──────────────────┬──────────────────────┘
                       │ nvcc 编译
                       ▼
    ┌──────────────────────────────────────────┐
    │ nvcc 自动分离 Host/Device 代码:           │
    │                                          │
    │  Host 代码 ──→ GCC/Clang/MSVC ──→ CPU 目标 │
    │  Device 代码 ──→ NVIDIA 编译器 ──→ PTX/SASS │
    │                                          │
    │  最终链接为一个可执行文件                   │
    └──────────────────────────────────────────┘
```

CUDA 引入了三个函数类型限定符来区分代码的执行位置：

| 限定符 | 执行位置 | 调用方 | 说明 |
|--------|---------|--------|------|
| `__global__` | Device (GPU) | Host 或 Device | **Kernel 函数**，入口点 |
| `__device__` | Device (GPU) | Device | GPU 上的辅助函数 |
| `__host__` | Host (CPU) | Host | 普通 CPU 函数（默认） |

::: tip `__host__` 和 `__device__` 可以组合使用
`__host__ __device__ float add(float a, float b)` 表示这个函数会被同时编译为 CPU 版本和 GPU 版本——在 Host 调用时跑 CPU，在 Device 调用时跑 GPU。这在编写需要在两端复用的工具函数时很方便。
:::

---

## 2.3 第一个 Kernel：向量加法

### Kernel 函数的写法

先看 CPU 版本：

```c
void vecadd_cpu(const float* A, const float* B, float* C, int n) {
    for (int i = 0; i < n; i++) {
        C[i] = A[i] + B[i];
    }
}
```

CUDA 版本——把循环"拆开"，每个线程负责一个元素：

```c
__global__ void vecadd_kernel(const float* A, const float* B,
                               float* C, int n) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) {
        C[i] = A[i] + B[i];
    }
}
```

核心差异只有三处：

```
从 CPU 到 GPU 的代码转换:

  CPU                              GPU
┌──────────────────────┐    ┌──────────────────────────────┐
│ void vecadd(...)     │    │ __global__ void vecadd(...)   │ ← ① 加 __global__
│ {                    │    │ {                             │
│   for (i=0; i<n; i++)│    │   int i = blockDim.x *        │ ← ② 循环变量 → 线程 ID
│   {                  │    │           blockIdx.x +         │
│                      │    │           threadIdx.x;         │
│                      │    │   if (i < n)                  │ ← ③ 边界检查
│     C[i]=A[i]+B[i];  │    │     C[i]=A[i]+B[i];          │    (线程数可能 > n)
│   }                  │    │                               │
│ }                    │    │ }                             │
└──────────────────────┘    └──────────────────────────────┘

串行 for 循环           →   并行: 每个线程执行循环体一次
循环变量 i              →   由 线程ID 替代
无需边界检查             →   需要 if (i < n) 防越界
```

### 线程 ID 计算

在 1D 场景下，全局线程 ID 的计算公式是：

$$
i = \texttt{blockIdx.x} \times \texttt{blockDim.x} + \texttt{threadIdx.x}
$$

直觉：把 Block 想象成"班级"，Thread 想象成"学生"。`blockIdx.x` 是班级编号，`blockDim.x` 是每班人数，`threadIdx.x` 是班内座位号。全局学号 = 班级编号 × 每班人数 + 班内座位号。

```
线程 ID 映射 (blockDim.x = 4, gridDim.x = 3):

Block 0           Block 1           Block 2
┌──┬──┬──┬──┐    ┌──┬──┬──┬──┐    ┌──┬──┬──┬──┐
│t0│t1│t2│t3│    │t0│t1│t2│t3│    │t0│t1│t2│t3│
└──┴──┴──┴──┘    └──┴──┴──┴──┘    └──┴──┴──┴──┘

全局 ID:
i = 0*4+0  0*4+1  0*4+2  0*4+3   1*4+0  1*4+1  1*4+2  1*4+3   2*4+0  2*4+1  2*4+2  2*4+3
  =  0      1      2      3       4      5      6      7       8      9      10     11

        ↓      ↓      ↓      ↓       ↓      ↓      ↓      ↓       ↓      ↓      ↓      ↓
数据:  A[0]   A[1]   A[2]   A[3]   A[4]   A[5]   A[6]   A[7]   A[8]   A[9]   A[10]  A[11]
      +B[0]  +B[1]  +B[2]  +B[3]  +B[4]  +B[5]  +B[6]  +B[7]  +B[8]  +B[9]  +B[10] +B[11]
      =C[0]  =C[1]  =C[2]  =C[3]  =C[4]  =C[5]  =C[6]  =C[7]  =C[8]  =C[9]  =C[10] =C[11]
```

### 为什么需要边界检查 `if (i < n)`？

因为线程总数（`gridDim.x × blockDim.x`）不一定恰好等于 \(n\)。当 \(n\) 不是 `blockDim.x` 的整数倍时，最后一个 Block 会有多余的线程：

```
n = 10, blockDim.x = 4 → gridDim.x = ceil(10/4) = 3

Block 0         Block 1         Block 2
[t0 t1 t2 t3]  [t4 t5 t6 t7]  [t8 t9 t10 t11]
 ↓  ↓  ↓  ↓    ↓  ↓  ↓  ↓    ↓  ↓   ✗   ✗
C[0..3]         C[4..7]         C[8] C[9]  越界!

→ Thread 10 和 11 没有对应的数据
→ 如果不加 if (i < n), 会越界访问 A[10], B[10]
→ 导致读取随机内存, 可能 crash 或产生错误结果
```

这是 CUDA 编程最常见的 bug 来源之一。**永远不要忘记边界检查**。

---

## 2.4 Device 全局内存与数据传输

GPU 有自己的独立内存（Device Memory / Global Memory），与 CPU 内存（Host Memory）物理隔离。数据必须通过 **PCIe 总线** 或 **NVLink** 在两者之间显式传输。

### 内存管理 API

CUDA 的内存管理 API 与标准 C 的 `malloc/free` 一一对应：

| 功能 | C 标准库 (Host) | CUDA (Device) |
|------|----------------|---------------|
| 分配内存 | `malloc(size)` | `cudaMalloc(&ptr, size)` |
| 释放内存 | `free(ptr)` | `cudaFree(ptr)` |
| 内存拷贝 | `memcpy(dst, src, size)` | `cudaMemcpy(dst, src, size, direction)` |
| 内存初始化 | `memset(ptr, val, size)` | `cudaMemset(ptr, val, size)` |

关键区别：`cudaMemcpy` 多了一个方向参数，用于指定数据流向：

| 方向 | 枚举值 | 含义 |
|------|--------|------|
| Host → Device | `cudaMemcpyHostToDevice` | 上传数据到 GPU |
| Device → Host | `cudaMemcpyDeviceToHost` | 从 GPU 取回结果 |
| Device → Device | `cudaMemcpyDeviceToDevice` | GPU 内部拷贝 |
| Host → Host | `cudaMemcpyHostToHost` | CPU 内部拷贝（少用） |

### 完整流程示意

```
Host-Device 数据传输流程:

  Host (CPU) Memory              Device (GPU) Memory
  ┌───────────────────┐          ┌───────────────────┐
  │                   │          │                   │
  │  h_A [1,2,3,4,5]  │          │  d_A              │
  │  h_B [5,4,3,2,1]  │          │  d_B              │
  │  h_C [_,_,_,_,_]  │          │  d_C              │
  │                   │          │                   │
  └───────────────────┘          └───────────────────┘

Step 1: cudaMalloc — 在 GPU 上分配空间
  ┌───────────────────┐          ┌───────────────────┐
  │  h_A [1,2,3,4,5]  │          │  d_A [?,?,?,?,?]  │ ← 已分配
  │  h_B [5,4,3,2,1]  │          │  d_B [?,?,?,?,?]  │ ← 已分配
  │  h_C [_,_,_,_,_]  │          │  d_C [?,?,?,?,?]  │ ← 已分配
  └───────────────────┘          └───────────────────┘

Step 2: cudaMemcpy(H→D) — 上传输入数据
  ┌───────────────────┐          ┌───────────────────┐
  │  h_A [1,2,3,4,5]  │ ══H2D═► │  d_A [1,2,3,4,5]  │ ← 数据就绪
  │  h_B [5,4,3,2,1]  │ ══H2D═► │  d_B [5,4,3,2,1]  │ ← 数据就绪
  │  h_C [_,_,_,_,_]  │          │  d_C [?,?,?,?,?]  │
  └───────────────────┘          └───────────────────┘

Step 3: kernel<<<>>> — GPU 并行计算
  ┌───────────────────┐          ┌───────────────────┐
  │                   │          │  d_A [1,2,3,4,5]  │
  │  CPU 等待或做其他事│          │  d_B [5,4,3,2,1]  │
  │                   │          │  d_C [6,6,6,6,6]  │ ← GPU 计算完成
  └───────────────────┘          └───────────────────┘

Step 4: cudaMemcpy(D→H) — 取回结果
  ┌───────────────────┐          ┌───────────────────┐
  │  h_C [6,6,6,6,6]  │ ◄═D2H══ │  d_C [6,6,6,6,6]  │
  └───────────────────┘          └───────────────────┘

Step 5: cudaFree — 释放 GPU 内存
```

### 代码实现

下面是完整的 Host 端函数，对应上述五个步骤：

```c
void vecadd_gpu(const float* h_A, const float* h_B, float* h_C, int n) {
    size_t size = n * sizeof(float);
    float *d_A, *d_B, *d_C;

    // Step 1: 分配 Device 内存
    cudaMalloc(&d_A, size);
    cudaMalloc(&d_B, size);
    cudaMalloc(&d_C, size);

    // Step 2: Host → Device
    cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice);

    // Step 3: 启动 Kernel
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;
    vecadd_kernel<<<gridSize, blockSize>>>(d_A, d_B, d_C, n);

    // Step 4: Device → Host
    cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost);

    // Step 5: 释放 Device 内存
    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);
}
```

::: warning 指针不能混用
`d_A` 是 Device 指针，只能在 GPU 代码中解引用。如果在 Host 代码中写 `d_A[0]`，会导致 **段错误（Segfault）**，因为这个地址在 CPU 的地址空间中无意义。反之亦然——Host 指针不能传入 Kernel。

这是 CUDA 初学者最常踩的坑之一。
:::

---

## 2.5 Kernel 启动：`<<<grid, block>>>` 语法

### 语法详解

CUDA 的 Kernel 启动使用独特的三尖括号语法：

```c
kernel_name<<<gridDim, blockDim>>>(arg1, arg2, ...);
```

- `gridDim`：Grid 中有多少个 Block
- `blockDim`：每个 Block 中有多少个 Thread

对于向量加法：

```c
int blockSize = 256;                            // 每个 Block 256 个线程
int gridSize = (n + blockSize - 1) / blockSize; // Block 数量 (向上取整)
vecadd_kernel<<<gridSize, blockSize>>>(d_A, d_B, d_C, n);
```

### Grid 大小的计算

为什么是 `(n + blockSize - 1) / blockSize` 而不是 `n / blockSize`？

```
向上取整除法: ceil(n / blockSize)

方法 1 (浮点, 不推荐):  (int)ceil((float)n / blockSize)
方法 2 (整数, 推荐):    (n + blockSize - 1) / blockSize

示例: n = 1000, blockSize = 256
  n / blockSize = 1000 / 256 = 3  (截断) → 3 × 256 = 768 线程 < 1000 ✗
  ceil(n / blockSize) = 4          → 4 × 256 = 1024 线程 ≥ 1000 ✓

  (1000 + 256 - 1) / 256 = 1255 / 256 = 4 ✓
```

**永远用向上取整**，确保线程总数 \(\geq n\)。多余的线程由 `if (i < n)` 边界检查跳过。

### Block 大小的选择

Block 大小（`blockDim`）该选多大？这是一个影响性能的重要参数：

| 约束条件 | 说明 |
|---------|------|
| **必须是 32 的倍数** | 因为硬件以 Warp（32 线程）为单位调度。非 32 的倍数会浪费执行槽位 |
| **上限 1024** | 每个 Block 最多 1024 个线程（硬件限制） |
| **影响 Occupancy** | Block 太大→每个 SM 放不下多个 Block→调度灵活性差；Block 太小→Warp 太少→延迟隐藏不足 |
| **常用值** | 128、256、512。**256 是最常见的默认选择** |

::: tip 为什么 256 是好的默认值？
- \(256 = 8 \times 32\)，即 8 个 Warp，是 Warp 的整数倍
- 大多数 GPU 每个 SM 最多能驻留 1536-2048 个线程，256 意味着可以同时驻留 6-8 个 Block，提供良好的 Occupancy
- 不太大（不会占用过多 Shared Memory/Register），不太小（有足够的 Warp 做延迟隐藏）
:::

---

## 2.6 错误检查：CUDA 调试的第一步

CUDA API 调用默认是 **静默失败** 的——出错时不会 crash，而是返回一个错误码。如果不检查，bug 会在后续代码中以莫名其妙的方式表现出来。

### 错误检查宏

生产级 CUDA 代码必须检查每个 API 调用的返回值：

```c
#define CUDA_CHECK(call)                                                    \
    do {                                                                    \
        cudaError_t err = (call);                                           \
        if (err != cudaSuccess) {                                           \
            fprintf(stderr, "CUDA Error at %s:%d - %s\n",                  \
                    __FILE__, __LINE__, cudaGetErrorString(err));           \
            exit(EXIT_FAILURE);                                             \
        }                                                                   \
    } while (0)

// 使用方式
CUDA_CHECK(cudaMalloc(&d_A, size));
CUDA_CHECK(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice));
```

### Kernel 启动的错误检查

Kernel 启动语法 `<<<>>>` 不返回值，错误检查需要两步：

```c
myKernel<<<grid, block>>>(args);

// 检查启动时是否有配置错误 (如 blockDim 超过上限)
CUDA_CHECK(cudaGetLastError());

// 等待 Kernel 完成, 检查执行时是否有错误 (如越界访问)
CUDA_CHECK(cudaDeviceSynchronize());
```

::: warning 不要忽略错误检查
CUDA 初学者最常犯的错误就是省略错误检查。一个典型场景：`cudaMalloc` 因为 GPU 显存不足返回 `cudaErrorMemoryAllocation`，但你没检查，继续用返回的 `nullptr` 做 `cudaMemcpy`，再把它传入 Kernel——最终你看到的可能是"结果全是 0"而不是"内存分配失败"。

**调试 CUDA 的第一条铁律：检查每一个 API 调用的返回值。**
:::

---

## 2.7 完整示例：向量加法 (全部代码)

将上述所有概念整合到一起。完整代码见 [`src/pmpp/cuda/ch02_vecadd.cu`](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch02_vecadd.cu)，下面是核心部分的逐行讲解。

### Kernel 函数

```c
__global__ void vecadd_kernel(const float* A, const float* B,
                               float* C, int n) {
    int i = blockDim.x * blockIdx.x + threadIdx.x;
    if (i < n) {
        C[i] = A[i] + B[i];
    }
}
```

**逐行分析**：

| 行 | 代码 | 说明 |
|---|------|------|
| 1 | `__global__` | 这是一个 Kernel 函数，由 Host 调用、在 Device 执行 |
| 1 | `void` | Kernel 函数 **必须** 返回 `void` |
| 1 | `const float* A` | Device 指针，指向 GPU 全局内存 |
| 2 | `int i = ...` | 计算当前线程的全局 ID |
| 3 | `if (i < n)` | 边界检查，防止越界访问 |
| 4 | `C[i] = A[i] + B[i]` | 核心计算——每个线程做一次加法 |

### Host 端完整流程

```c
void vecadd_gpu(const float* h_A, const float* h_B, float* h_C, int n) {
    size_t size = n * sizeof(float);
    float *d_A, *d_B, *d_C;

    // 1. 分配 Device 内存
    CUDA_CHECK(cudaMalloc(&d_A, size));
    CUDA_CHECK(cudaMalloc(&d_B, size));
    CUDA_CHECK(cudaMalloc(&d_C, size));

    // 2. Host → Device 数据传输
    CUDA_CHECK(cudaMemcpy(d_A, h_A, size, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(d_B, h_B, size, cudaMemcpyHostToDevice));

    // 3. 配置并启动 Kernel
    int blockSize = 256;
    int gridSize = (n + blockSize - 1) / blockSize;
    vecadd_kernel<<<gridSize, blockSize>>>(d_A, d_B, d_C, n);
    CUDA_CHECK(cudaGetLastError());
    CUDA_CHECK(cudaDeviceSynchronize());

    // 4. Device → Host 数据传输
    CUDA_CHECK(cudaMemcpy(h_C, d_C, size, cudaMemcpyDeviceToHost));

    // 5. 释放 Device 内存
    CUDA_CHECK(cudaFree(d_A));
    CUDA_CHECK(cudaFree(d_B));
    CUDA_CHECK(cudaFree(d_C));
}
```

### 编译与运行

```bash
# 编译
nvcc -o ch02_vecadd ch02_vecadd.cu

# 运行 (默认 16M 元素)
./ch02_vecadd

# 指定向量大小
./ch02_vecadd 1000000
```

---

## 2.8 性能初探：向量加法的瓶颈在哪里？

向量加法虽然简单，但它揭示了一个 GPU 编程中反复出现的关键问题：**数据传输 vs 计算的比例**。

### 计算密度分析

对于向量加法，每个元素需要：
- **读取**：2 次（A[i] 和 B[i]）
- **写入**：1 次（C[i]）
- **计算**：1 次加法

总的内存访问量：\(3 \times n \times 4\) 字节（float = 4 字节）\
总的计算量：\(n\) 次浮点加法

**计算强度（Arithmetic Intensity）**：

$$
\text{AI} = \frac{\text{FLOPs}}{\text{Bytes}} = \frac{n}{3 \times n \times 4} = \frac{1}{12} \approx 0.083 \text{ FLOP/Byte}
$$

这意味着每搬运 12 字节的数据才做 1 次浮点运算——**严重的内存带宽受限（Memory-bound）**。

```
向量加法的 Roofline 分析:

性能
(GFLOPS)
  │
  │         ╱ 算力上限 (如 A100: ~19,500 GFLOPS FP32)
  │        ╱
  │       ╱
  │      ╱
  │     ╱
  │    ╱
  │   ╱   ← 带宽上限 (如 A100: 2039 GB/s)
  │  ╱     斜率 = 内存带宽
  │ ╱
  │╱       ● 向量加法 (AI = 1/12)
  ┼────────┬──────────────────── 计算强度 (FLOP/Byte)
           1/12

→ 向量加法远在 Roofline 的带宽受限区域
→ 性能完全由内存带宽决定, 与 GPU 有多少 CUDA Core 无关
```

### PCIe 传输开销

更致命的是 **Host-Device 数据传输**。PCIe 4.0 x16 的理论带宽约 32 GB/s，而 A100 的 HBM 带宽高达 2 TB/s——差了 **60 倍**。

对于向量加法，需要传输的数据量为 \(3n \times 4\) 字节（上传 A、B，下载 C），而 Kernel 本身的计算几乎是瞬时的。这意味着：

> 对于向量加法这种简单操作，**90%+ 的时间花在 PCIe 传输上，而非 GPU 计算**。

```cpp-run title="向量加法传输开销分析"
#include <iostream>
#include <iomanip>

int main() {
    std::cout << "=== 向量加法: 传输 vs 计算时间估算 ===\n\n";

    double pcie_bw = 32.0;      // PCIe 4.0 x16: 32 GB/s
    double hbm_bw  = 2039.0;    // A100 HBM2e: 2039 GB/s
    double gpu_flops = 19500.0;  // A100 FP32: 19.5 TFLOPS = 19500 GFLOPS

    std::cout << std::setw(14) << "向量大小"
              << std::setw(14) << "数据量"
              << std::setw(14) << "PCIe传输"
              << std::setw(14) << "Kernel计算"
              << std::setw(14) << "传输占比" << "\n";
    std::cout << std::string(70, '-') << "\n";

    for (long long n : {1000000LL, 10000000LL, 100000000LL, 1000000000LL}) {
        double data_gb = 3.0 * n * 4.0 / 1e9;
        double pcie_ms = data_gb / pcie_bw * 1000.0;

        double kernel_data_gb = data_gb;
        double kernel_ms_bw = kernel_data_gb / hbm_bw * 1000.0;

        double total_ms = pcie_ms + kernel_ms_bw;
        double pcie_pct = pcie_ms / total_ms * 100.0;

        std::string size_str;
        if (n >= 1e9) size_str = std::to_string(n / 1000000000) + "B";
        else if (n >= 1e6) size_str = std::to_string(n / 1000000) + "M";
        else size_str = std::to_string(n / 1000) + "K";

        std::cout << std::setw(14) << size_str
                  << std::setw(11) << std::fixed << std::setprecision(1)
                  << data_gb * 1000 << " MB"
                  << std::setw(11) << std::setprecision(2) << pcie_ms << " ms"
                  << std::setw(11) << std::setprecision(3) << kernel_ms_bw << " ms"
                  << std::setw(11) << std::setprecision(1) << pcie_pct << "%"
                  << "\n";
    }

    std::cout << "\n→ 对于内存密集型操作, PCIe 传输是压倒性瓶颈\n";
    std::cout << "→ 这就是为什么实际 GPU 程序要尽量减少 Host↔Device 传输\n";
    std::cout << "→ 策略: 数据常驻 GPU, 多个 Kernel 串联计算, 避免中间结果回传\n";

    return 0;
}
```

::: tip 关键启示
向量加法看似是"GPU 不划算"的反面教材——传输开销远大于计算收益。但实际的 GPU 程序（如深度学习训练）是 **成百上千个 Kernel 串联执行**，数据始终驻留在 GPU 上，Host-Device 传输只在最初上传数据和最终取回结果时发生。在这种场景下，传输开销被 **摊薄** 到可以忽略。

向量加法的价值在于教学——它是理解 CUDA 编程流程最简单的载体。
:::

---

## 2.9 GPU 设备查询

在运行 CUDA 程序之前，了解 GPU 的硬件参数（SM 数量、内存大小、最大线程数等）非常有用。CUDA 提供了 `cudaGetDeviceProperties` API：

```c
cudaDeviceProp prop;
cudaGetDeviceProperties(&prop, 0);  // 查询第 0 号 GPU

printf("设备名称: %s\n", prop.name);
printf("SM 数量: %d\n", prop.multiProcessorCount);
printf("每个 Block 最大线程数: %d\n", prop.maxThreadsPerBlock);
printf("每个 SM 最大线程数: %d\n", prop.maxThreadsPerMultiProcessor);
printf("全局内存: %.1f GB\n", prop.totalGlobalMem / 1e9);
printf("Shared Memory / Block: %lu KB\n", prop.sharedMemPerBlock / 1024);
printf("Warp 大小: %d\n", prop.warpSize);
printf("Compute Capability: %d.%d\n", prop.major, prop.minor);
```

`cudaDeviceProp` 结构体中重要字段一览：

| 字段 | 含义 | 典型值 (A100) |
|------|------|--------------|
| `name` | GPU 名称 | "A100-SXM4-80GB" |
| `multiProcessorCount` | SM 数量 | 108 |
| `maxThreadsPerBlock` | 每 Block 最大线程 | 1024 |
| `maxThreadsPerMultiProcessor` | 每 SM 最大线程 | 2048 |
| `warpSize` | Warp 大小 | 32 |
| `totalGlobalMem` | 全局内存总量 | 80 GB |
| `sharedMemPerBlock` | 每 Block Shared Memory | 48 KB (可配到 164 KB) |
| `major`, `minor` | Compute Capability | 8, 0 |
| `clockRate` | SM 核心频率 | 1410 MHz |
| `memoryClockRate` | 显存频率 | 1215 MHz |
| `memoryBusWidth` | 显存位宽 | 5120 bit |

::: tip Compute Capability
Compute Capability（计算能力）是 NVIDIA GPU 的代际标识，决定了 GPU 支持哪些 CUDA 特性。编译时通过 `-arch=sm_XX` 指定：

```bash
nvcc -arch=sm_80 -o program program.cu   # A100
nvcc -arch=sm_90 -o program program.cu   # H100
```

常见的 Compute Capability：
- 7.0 = V100
- 8.0 = A100
- 8.9 = RTX 4090
- 9.0 = H100
:::

---

## 2.10 CUDA 编译流程：nvcc

CUDA 程序的源文件通常以 `.cu` 为后缀。NVIDIA 编译器 `nvcc` 负责编译整个程序，流程如下：

```
nvcc 编译流程:

  source.cu
      │
      ▼
  ┌───────────┐
  │   nvcc    │ (NVIDIA CUDA Compiler)
  │ 预处理分离 │
  └─────┬─────┘
        │
   ┌────┴────┐
   ▼         ▼
Host 代码   Device 代码
   │         │
   ▼         ▼
┌──────┐  ┌──────────┐
│GCC/  │  │ NVIDIA   │
│Clang │  │ PTX 编译器│
└──┬───┘  └────┬─────┘
   │           │
   ▼           ▼
 CPU 目标    PTX (中间表示)
 (.o)          │
               ▼
            ┌──────────┐
            │ ptxas    │ (PTX → SASS 汇编)
            │ 目标架构  │
            └────┬─────┘
                 │
                 ▼
              SASS (GPU 机器码)
                 │
   ┌─────────────┘
   ▼
┌──────────┐
│  链接器   │ → 可执行文件
└──────────┘
```

### 常用编译选项

```bash
# 基础编译
nvcc -o program program.cu

# 指定 GPU 架构
nvcc -arch=sm_80 -o program program.cu

# 开启优化
nvcc -O3 -o program program.cu

# 生成调试信息 (用于 cuda-gdb)
nvcc -g -G -o program program.cu

# 同时生成多个架构的代码 (兼容性)
nvcc -gencode arch=compute_70,code=sm_70 \
     -gencode arch=compute_80,code=sm_80 \
     -o program program.cu

# 查看寄存器和 Shared Memory 使用情况
nvcc --ptxas-options=-v -o program program.cu
```

### PTX：GPU 的"汇编语言"

PTX（Parallel Thread Execution）是 NVIDIA 的虚拟指令集，类似于 LLVM IR 的角色。它是一个稳定的中间表示——用 PTX 编写的程序可以在不同代际的 GPU 上运行（由 Driver 在运行时翻译为特定架构的 SASS 机器码）。

可以通过 `nvcc -ptx` 查看生成的 PTX 代码，这在性能分析时很有用。

---

## 2.11 Grid-Stride Loop：一种更灵活的 Kernel 模式

书中介绍的基础 Kernel 模式是"一线程一元素"。在实际开发中，还有一种更灵活的模式—— **Grid-Stride Loop**：

```c
__global__ void vecadd_stride(const float* A, const float* B,
                               float* C, int n) {
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    int stride = blockDim.x * gridDim.x;  // Grid 中总线程数
    for (int i = idx; i < n; i += stride) {
        C[i] = A[i] + B[i];
    }
}
```

对比两种模式：

```
模式 1: 一线程一元素 (n=12, 总线程=12)

Thread:  0  1  2  3  4  5  6  7  8  9  10  11
         ↓  ↓  ↓  ↓  ↓  ↓  ↓  ↓  ↓  ↓   ↓   ↓
Data:   [0][1][2][3][4][5][6][7][8][9][10][11]

每个线程处理 1 个元素. gridSize 必须 >= ceil(n / blockSize).


模式 2: Grid-Stride Loop (n=12, 总线程=4, stride=4)

迭代 1:  Thread 0→[0]  Thread 1→[1]  Thread 2→[2]  Thread 3→[3]
迭代 2:  Thread 0→[4]  Thread 1→[5]  Thread 2→[6]  Thread 3→[7]
迭代 3:  Thread 0→[8]  Thread 1→[9]  Thread 2→[10] Thread 3→[11]

每个线程处理多个元素. gridSize 可以自由选择.
```

Grid-Stride Loop 的优势：

| 特性 | 一线程一元素 | Grid-Stride Loop |
|------|------------|-----------------|
| Grid 大小 | 必须 \(\geq \lceil n / \text{blockSize} \rceil\) | 任意 |
| n 非常大时 | Grid 可能超大（性能影响） | Grid 大小固定，更可控 |
| 可复用于串行调试 | 不便 | `<<<1, 1>>>` 即可退化为串行 |
| 代码通用性 | 仅适合 1D | 容易推广 |

---

## 2.12 本章总结

### 核心概念速查

| 概念 | 要点 |
|------|------|
| **数据并行** | 同一操作作用于不同数据元素，GPU 的根基范式 |
| **Host / Device** | CPU = Host, GPU = Device, 各有独立内存 |
| **Kernel** | `__global__` 标记的函数，由 Host 启动、在 GPU 上并行执行 |
| **线程 ID** | `blockIdx.x * blockDim.x + threadIdx.x` |
| **内存管理** | `cudaMalloc` → `cudaMemcpy` → Kernel → `cudaMemcpy` → `cudaFree` |
| **Grid/Block** | `<<<gridDim, blockDim>>>` 配置线程组织方式 |
| **边界检查** | `if (i < n)` 防止最后一个 Block 的多余线程越界 |
| **错误检查** | 用宏包裹每个 CUDA API 调用，检查返回的 `cudaError_t` |

### CUDA API 速查

```c
// 内存管理
cudaMalloc(&ptr, size);                              // 分配 Device 内存
cudaFree(ptr);                                       // 释放 Device 内存
cudaMemcpy(dst, src, size, cudaMemcpyHostToDevice);  // H→D
cudaMemcpy(dst, src, size, cudaMemcpyDeviceToHost);  // D→H
cudaMemset(ptr, value, size);                        // Device 内存初始化

// Kernel 启动
kernel<<<gridDim, blockDim>>>(args...);              // 启动 Kernel
cudaGetLastError();                                  // 检查启动错误
cudaDeviceSynchronize();                             // 等待 GPU 完成

// 设备查询
cudaGetDeviceCount(&count);                          // GPU 数量
cudaGetDeviceProperties(&prop, device_id);           // GPU 属性
cudaSetDevice(device_id);                            // 选择 GPU
```

### 常见 Bug 清单

| Bug | 症状 | 解决 |
|-----|------|------|
| 忘记边界检查 | 结果随机错误或 crash | 加 `if (i < n)` |
| 指针混用 | Segfault 或 `cudaErrorInvalidValue` | Host/Device 指针严格区分 |
| 忘记 `cudaMemcpy` | GPU 计算的是未初始化数据 | 确保先传输再计算 |
| Grid 大小不够 | 只有部分元素被计算 | 用向上取整公式 |
| 忘记错误检查 | Bug 在很远的地方才暴露 | 用 `CUDA_CHECK` 宏 |
| 忘记 `cudaDeviceSynchronize` | Host 过早读取未完成的结果 | 在 `cudaMemcpy` 前同步（`cudaMemcpy` 本身自带同步） |

---

::: tip 下一章预告
[Ch03: Multidimensional Grids and Data](./ch03) 将把向量加法推广到 **多维** 场景——矩阵运算和图像处理。你会学到 2D/3D 的 Grid 和 Block 配置、行优先内存布局、以及如何将多维坐标映射到线性内存地址。
:::

---

## 扩展思考

::: details 思考题 1：如果 n = 1（只有一个元素），使用 GPU 做向量加法值得吗？
绝对不值得。原因：

1. **Kernel 启动延迟**：一次 Kernel 启动本身就需要 ~5-10 μs 的开销（Host-Device 指令传递、SM 调度等）
2. **cudaMemcpy 延迟**：PCIe 传输有 ~10 μs 的固定延迟（即使只传 4 字节）
3. **一次加法**：CPU 只需 ~1 ns

总计：GPU 路径花 ~20+ μs 做一次 CPU 1 ns 就能完成的加法——慢了 **20,000 倍**。

**GPU 的价值在于摊薄固定开销**。当 \(n\) 足够大（通常 \(n > 10^4 \sim 10^5\)），GPU Kernel 的计算时间超过固定开销，才开始体现并行优势。
:::

::: details 思考题 2：cudaMemcpy 是同步还是异步的？这有什么影响？
`cudaMemcpy` 是 **同步（Synchronous）** 的——它会阻塞 Host 线程，直到数据传输完成。这意味着 CPU 在传输期间完全空闲。

如果想让 CPU 在传输期间做其他事（比如准备下一批数据），可以使用异步版本 `cudaMemcpyAsync` + CUDA Stream：

```c
cudaStream_t stream;
cudaStreamCreate(&stream);

cudaMemcpyAsync(d_A, h_A, size, cudaMemcpyHostToDevice, stream);
// CPU 可以在这里做其他工作, 不用等待传输完成
prepare_next_batch();

cudaStreamSynchronize(stream);  // 需要时再同步
```

异步传输还可以实现 **传输与计算的重叠**——当一批数据正在从 Host 传向 Device 时，GPU 可以同时计算已经传完的上一批数据。这是高性能 CUDA 程序的标准优化手段。
:::

::: details 思考题 3：为什么 Kernel 函数必须返回 void？
两个原因：

1. **执行模型**：一个 Kernel 会被成千上万个线程同时执行。如果 Kernel 有返回值，哪个线程的返回值传给 Host？这在语义上是模糊的。
2. **异步启动**：Kernel 是异步启动的——`<<<>>>` 语句返回时 Kernel 可能还没开始执行。如果有返回值，Host 拿到的会是什么？

因此，Kernel 的"输出"只能通过写入 Device 内存（即传入的指针参数），再由 Host 通过 `cudaMemcpy` 读取。这种设计虽然多了一步，但语义清晰、没有歧义。
:::

::: details 思考题 4：blockDim 选 32 和选 256 有什么区别？
假设 n = 10000：

**blockDim = 32**：
- gridDim = ceil(10000/32) = 313 个 Block
- 每个 Block 只有 1 个 Warp
- SM 可以同时驻留更多 Block（因为每个 Block 占资源少）
- 但如果每个 Block 的 Shared Memory / Register 需求有固定开销，Block 太多会浪费

**blockDim = 256**：
- gridDim = ceil(10000/256) = 40 个 Block
- 每个 Block 有 8 个 Warp
- SM 驻留的 Block 数量少，但每个 Block 内有更多 Warp 做延迟隐藏
- Block 内线程可以通过 `__syncthreads()` 和 Shared Memory 协作（虽然向量加法用不到）

对于向量加法这种简单 Kernel，两者的性能差异不大。但对于需要 Block 内协作（如 Tiled 矩阵乘法、归约）的复杂 Kernel，256 通常是更好的选择。
:::
