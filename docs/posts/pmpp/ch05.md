---
title: "PMPP Ch05: Memory Architecture and Data Locality"
date: 2026-02-19
---

# Ch05: Memory Architecture and Data Locality

<p style="color: var(--vp-c-text-2); font-size: 14px;">
2026-02-19 &nbsp;·&nbsp; PMPP 专栏 &nbsp;·&nbsp; 第五章
</p>

> **本章信息**
> - **书名**: Programming Massively Parallel Processors: A Hands-on Approach (4th Edition)
> - **作者**: David B. Kirk, Wen-mei W. Hwu
> - **范围**: 第 5 章 Memory Architecture and Data Locality
> - **配套代码**: [ch05_tiled_matmul.cu](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch05_tiled_matmul.cu) · [ch05_mem_bandwidth.cu](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch05_mem_bandwidth.cu)

## 一句话总结

本章是全书最核心的一章——GPU 性能优化的本质是 **内存优化**。深入理解 GPU 内存层次结构（寄存器 → Shared Memory → L2 → Global Memory），掌握 **Tiling** 技术将数据从慢内存搬到快内存并复用，用 Shared Memory Tiled 矩阵乘法将性能从峰值的 1-5% 提升到 30-60%。

---

## 5.1 GPU 内存层次结构全景

### 为什么内存比计算更重要？

现代 GPU 的计算能力增长远快于内存带宽。以 A100 为例：

| 指标 | 数值 |
|------|------|
| FP32 峰值算力 | 19.5 TFLOPS |
| HBM 带宽 | 2.0 TB/s |
| **计算-带宽比** | **9.75 FLOP/Byte** |

这意味着：每从全局内存读取 1 个 float（4 字节），GPU 可以做约 **39 次** 浮点运算。如果你的 Kernel 每读一个数据只做 1 次运算（如向量加法，AI = 1/12），那么 **97% 的计算能力被浪费了**——GPU 大部分时间在等内存。

$$
\text{实际性能} = \min\!\left(\text{峰值算力},\ \text{AI} \times \text{内存带宽}\right)
$$

提高性能有两条路：

1. **提高计算强度（AI）**：让每字节数据被复用更多次 → **Tiling 技术**
2. **提高有效带宽**：优化内存访问模式 → **合并访问、Shared Memory**

### 完整内存层次

```
GPU 内存层次结构 (从快到慢):

速度 ←──────────────────────────────────────── 容量
快/小                                          慢/大

 寄存器 (Registers)
 ┌──────────────────────┐
 │ 每线程私有            │  容量: ~255 个/线程 (约 1 KB)
 │ 延迟: 0 cycles       │  带宽: ~数十 TB/s (估算)
 │ 生命周期: 线程        │  作用域: 单个线程
 └──────────┬───────────┘
            ▼
 Shared Memory
 ┌──────────────────────┐
 │ Block 内共享          │  容量: 48-228 KB / SM
 │ 延迟: ~5 cycles      │  带宽: ~10-19 TB/s (估算)
 │ 生命周期: Block       │  作用域: 同一 Block 的所有线程
 │ 程序员显式管理        │  ← 本章核心!
 └──────────┬───────────┘
            ▼
 L1 Cache (每 SM)
 ┌──────────────────────┐
 │ 硬件自动管理          │  容量: 与 Shared Mem 共享片上 SRAM
 │ 延迟: ~30 cycles     │  作用域: 单个 SM
 └──────────┬───────────┘
            ▼
 L2 Cache
 ┌──────────────────────┐
 │ 全局共享              │  容量: 数 MB (A100: 40 MB)
 │ 延迟: ~200 cycles    │  作用域: 所有 SM
 └──────────┬───────────┘
            ▼
 Global Memory (HBM)
 ┌──────────────────────┐
 │ 所有线程可访问        │  容量: 数十 GB (A100: 80 GB)
 │ 延迟: 200-800 cycles │  带宽: ~2 TB/s
 │ 生命周期: 应用程序    │  作用域: 所有 SM 所有线程
 └──────────┬───────────┘
            ▼
 Host Memory (CPU DRAM)
 ┌──────────────────────┐
 │ 通过 PCIe/NVLink     │  容量: 数百 GB - TB
 │ 延迟: ~数万 cycles   │  带宽: 32-900 GB/s (PCIe/NVLink)
 └──────────────────────┘
```

### 各内存类型的 CUDA 对应关系

| 内存类型 | CUDA 声明 | 作用域 | 生命周期 | 在哪里 |
|---------|----------|--------|---------|--------|
| **寄存器** | 局部变量 `float x;` | 线程 | 线程 | 片上 (SM) |
| **Local Memory** | 寄存器溢出 / 大数组 | 线程 | 线程 | 片外 (HBM)! |
| **Shared Memory** | `__shared__ float s[N];` | Block | Block | 片上 (SM) |
| **Global Memory** | `cudaMalloc` / `__device__` | 全局 | 应用 | 片外 (HBM) |
| **Constant Memory** | `__constant__ float c[N];` | 全局 | 应用 | 片外 (有缓存) |
| **Texture Memory** | `texture<>` / `cudaArray` | 全局 | 应用 | 片外 (有缓存) |

::: warning Local Memory 的陷阱
名字叫 "Local" 但实际存储在 **全局内存（HBM）** 中，速度和全局内存一样慢！当一个线程使用的寄存器超过硬件上限，编译器会自动将部分变量 **spill** 到 Local Memory——这是一个常见的性能杀手。

触发 spill 的典型场景：
- Kernel 中定义了过多局部变量
- 使用了大的静态数组（如 `float arr[1000]`）
- 编译器无法确定数组索引（动态索引的数组只能放在 Local Memory）
:::

---

## 5.2 Shared Memory 详解

### Shared Memory 是什么？

Shared Memory 是位于 **SM 片上（On-Chip）** 的高速缓存，由程序员 **显式管理**。它的关键特性：

- **速度极快**：延迟约 5 个时钟周期，带宽约为全局内存的 **10 倍以上**
- **Block 内共享**：同一 Block 中的所有线程可以读写同一块 Shared Memory
- **生命周期 = Block 生命周期**：Block 执行结束时自动释放
- **容量有限**：每个 SM 48-228 KB（具体取决于架构和配置）

### 声明与使用

```c
// 方式 1: 静态分配 (编译时确定大小)
__shared__ float tile[16][16];

// 方式 2: 动态分配 (运行时确定大小)
extern __shared__ float dynamic_smem[];

// 启动时指定动态 Shared Memory 大小
kernel<<<grid, block, sharedMemBytes>>>(args...);
```

### Shared Memory 的 Bank 结构

Shared Memory 被组织为 **32 个 Bank**，连续的 4 字节（一个 float/int）分布在连续的 Bank 上：

```
Shared Memory Bank 结构:

地址:   0    4    8   12   16   ...  124   128  132  ...
Bank: [ 0 ][ 1 ][ 2 ][ 3 ][ 4 ] ... [ 31][ 0 ][ 1 ] ...

float 数组 s[256] 的 Bank 分布:
  s[0]  → Bank 0     s[32] → Bank 0
  s[1]  → Bank 1     s[33] → Bank 1
  s[2]  → Bank 2     s[34] → Bank 2
  ...                 ...
  s[31] → Bank 31    s[63] → Bank 31

关键: 相邻 float 元素在相邻 Bank 中
```

**Bank Conflict**：当同一个 Warp 中的多个线程在同一时钟周期访问 **同一个 Bank 中的不同地址** 时，这些访问必须串行化——这就是 **Bank Conflict**。

```
无 Bank Conflict (理想情况):
  Thread 0 → Bank 0    (s[0])
  Thread 1 → Bank 1    (s[1])
  ...
  Thread 31 → Bank 31  (s[31])
  → 32 个线程同时访问 32 个不同 Bank → 1 次完成 ✓

2-way Bank Conflict:
  Thread 0 → Bank 0    (s[0])
  Thread 1 → Bank 1    (s[1])
  ...
  Thread 16 → Bank 0   (s[32])  ← 和 Thread 0 冲突!
  Thread 17 → Bank 1   (s[33])  ← 和 Thread 1 冲突!
  ...
  → 需要 2 轮才能完成 → 慢 2 倍 ✗

广播 (特殊情况, 无冲突):
  Thread 0..31 全部读 Bank 0 的 同一地址 (s[0])
  → 硬件广播, 1 次完成 ✓
```

::: tip Bank Conflict 的避免策略
最常见的避免方法是 **padding**：在 Shared Memory 数组的行末添加一个多余的元素，打破列访问时的 Bank 冲突模式。

```c
// 有 Bank Conflict: 列访问时 s[0][0], s[1][0], s[2][0]... 都在 Bank 0
__shared__ float s[32][32];

// 无 Bank Conflict: padding 使每行偏移 1 个 Bank
__shared__ float s[32][33];  // 多加 1 列
// s[0][0] → Bank 0, s[1][0] → Bank 1, s[2][0] → Bank 2... ✓
```
:::

---

## 5.3 Tiling 技术：核心思想

### 为什么需要 Tiling？

回顾第 3 章的基础矩阵乘法——每个 C 的元素需要读取 A 的整行和 B 的整列：

```
基础矩阵乘法的全局内存访问:

C[row][col] = Σ A[row][k] * B[k][col]

问题: A 的第 row 行被同一 Block 行中的所有线程读取
      B 的第 col 列被同一 Block 列中的所有线程读取
      → 巨大的冗余全局内存访问

具体量化 (M=N=K=1024, TILE=16):
  基础版: 每个 C 元素读 2×1024 = 2048 个 float 从全局内存
  总读取: 1024² × 2048 × 4B = 8 GB

  如果能复用:
  A 只有 1024×1024 = 1M 个 float → 4 MB
  B 只有 1024×1024 = 1M 个 float → 4 MB
  理论最少只需读 8 MB, 基础版读了 8 GB → 冗余 1000 倍!
```

### Tiling 的核心思想

**Tiling（分块）** 的思想是：将大矩阵分成小块（Tile），每次把一小块从全局内存加载到 Shared Memory，Block 内的所有线程共享这块数据，做完计算后再加载下一块。

```
Tiling 的核心循环:

for 每一对 Tile (A的列块, B的行块):
    1. Block 内所有线程 协作 将 A 的一个 Tile 加载到 Shared Memory
    2. Block 内所有线程 协作 将 B 的一个 Tile 加载到 Shared Memory
    3. __syncthreads()  // 确保所有数据加载完毕
    4. 每个线程从 Shared Memory 计算部分点积
    5. __syncthreads()  // 确保所有线程用完数据后再加载下一个 Tile
```

这样，一个 float 被从全局内存加载 **1 次** 到 Shared Memory，然后被 Block 内 **TILE_SIZE 个线程** 复用——全局内存访问量减少了 **TILE_SIZE 倍**。

---

## 5.4 Tiled 矩阵乘法：逐步推导

### Step 1：确定 Tile 尺寸和数据映射

选择 `TILE_SIZE = 16`。一个 Block 有 16×16 = 256 个线程，负责计算 C 中一个 16×16 的子块。

```
Tiled 矩阵乘法数据映射:

C (M×N)                    A (M×K)                    B (K×N)
┌───┬───┬───┬───┐         ┌───┬───┬───┬───┐         ┌───┬───┬───┬───┐
│C00│C01│C02│C03│         │   │   │   │   │         │   │   │   │   │
├───┼───┼───┼───┤         ├───┼───┼───┼───┤         ├───┼───┼───┼───┤
│C10│C11│C12│C13│         │   │   │   │   │         │   │   │   │   │
├───┼───┼───┼───┤         ├───┼───┼───┼───┤         ├───┼───┼───┼───┤
│C20│C21│C22│C23│         │   │   │   │   │         │   │   │   │   │
├───┼───┼───┼───┤         ├───┼───┼───┼───┤         ├───┼───┼───┼───┤
│C30│C31│C32│C33│         │   │   │   │   │         │   │   │   │   │
└───┴───┴───┴───┘         └───┴───┴───┴───┘         └───┴───┴───┴───┘

每个 Cij 是 16×16 的子块
Block(i,j) 负责计算 Cij

Cij = Σ_p Aip × Bpj   (p 遍历 K/16 个 Tile 对)
```

### Step 2：理解分阶段（Phase）计算

计算 C 中一个 16×16 子块需要遍历 A 的一个 Tile 行和 B 的一个 Tile 列。将 K 维度分成 `K/TILE_SIZE` 个阶段（Phase）：

```
Phase 0: 加载 A[:,0:15] 和 B[0:15,:] 的对应 Tile
Phase 1: 加载 A[:,16:31] 和 B[16:31,:] 的对应 Tile
...
Phase p: 加载 A[:,16p:16p+15] 和 B[16p:16p+15,:] 的对应 Tile

详细展开 (Block 计算 C11, K=64, TILE=16):

Phase 0:
   A 的 Tile: A[16:31, 0:15]    B 的 Tile: B[0:15, 16:31]
   ┌────────────────┐            ┌────────────────┐
   │ ████████████████│ ← 16×16  │ ████████████████│ ← 16×16
   └────────────────┘            └────────────────┘
   加载到 As[16][16]             加载到 Bs[16][16]
   __syncthreads()
   每线程: sum += Σ_{k=0}^{15} As[ty][k] * Bs[k][tx]
   __syncthreads()

Phase 1:
   A 的 Tile: A[16:31, 16:31]   B 的 Tile: B[16:31, 16:31]
   加载 → __syncthreads → 计算 → __syncthreads

Phase 2:
   A 的 Tile: A[16:31, 32:47]   B 的 Tile: B[32:47, 16:31]
   加载 → __syncthreads → 计算 → __syncthreads

Phase 3:
   A 的 Tile: A[16:31, 48:63]   B 的 Tile: B[48:63, 16:31]
   加载 → __syncthreads → 计算 → __syncthreads

最终: C[16:31, 16:31] = sum (累积 4 个 Phase 的结果)
```

### Step 3：每个线程的具体工作

```
Block(bx, by) 中 Thread(tx, ty) 在 Phase ph 中的操作:

1. 加载:
   As[ty][tx] = A[ (by*TILE+ty) * K + (ph*TILE+tx) ]
                     ↑ 全局行号              ↑ 全局列号

   Bs[ty][tx] = B[ (ph*TILE+ty) * N + (bx*TILE+tx) ]
                     ↑ 全局行号              ↑ 全局列号

   → 每个线程加载 A 和 B 各一个元素
   → 一个 Block 的 256 个线程协作加载 256+256=512 个元素
   → 这两个 Tile 接下来被 256 个线程共享使用

2. 同步: __syncthreads()

3. 计算:
   for k = 0 to TILE_SIZE-1:
       sum += As[ty][k] * Bs[k][tx]

   → 每个线程做 16 次 FMA (乘加)
   → 每次访问的是 Shared Memory, 不是全局内存!

4. 同步: __syncthreads()
```

### Step 4：完整 Kernel 代码

```c
#define TILE_SIZE 16

__global__ void matmul_tiled(const float* A, const float* B, float* C,
                              int M, int K, int N) {
    // 在 Shared Memory 中为 A 和 B 各分配一个 Tile
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    int tx = threadIdx.x;
    int ty = threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + tx;  // C 中的全局列号
    int row = blockIdx.y * TILE_SIZE + ty;  // C 中的全局行号

    float sum = 0.0f;  // 累积器, 存在寄存器中

    // 遍历所有 Phase
    for (int ph = 0; ph < K / TILE_SIZE; ph++) {

        // Phase 1: 协作加载
        // 每个线程加载 A 和 B 的各一个元素
        As[ty][tx] = A[row * K + (ph * TILE_SIZE + tx)];
        Bs[ty][tx] = B[(ph * TILE_SIZE + ty) * N + col];

        // 确保整个 Tile 加载完毕
        __syncthreads();

        // Phase 2: 计算部分点积
        for (int k = 0; k < TILE_SIZE; k++) {
            sum += As[ty][k] * Bs[k][tx];
        }

        // 确保所有线程用完当前 Tile 后再加载下一个
        __syncthreads();
    }

    // 写回结果
    C[row * N + col] = sum;
}
```

---

## 5.5 Tiling 的内存访问量化分析

### 全局内存访问量对比

| 版本 | 每线程全局内存读取 | 总全局内存读取 (M=N=K=1024) | Shared Memory 用量/Block |
|------|------------------|----------------------------|------------------------|
| 基础版 | \(2K\) 个 float | \(M \times N \times 2K = 2G\) 个 float (8 GB) | 0 |
| Tiled (T=16) | \(\frac{2K}{T} = \frac{2K}{16}\) 个 float | \(\frac{M \times N \times 2K}{T} = 128M\) 个 float (512 MB) | \(2 \times 16^2 \times 4 = 2\) KB |
| Tiled (T=32) | \(\frac{2K}{32}\) 个 float | \(\frac{MN \cdot 2K}{32} = 64M\) 个 float (256 MB) | \(2 \times 32^2 \times 4 = 8\) KB |

**全局内存减少倍数 = TILE_SIZE**。TILE_SIZE=16 → 减少 16 倍；TILE_SIZE=32 → 减少 32 倍。

### 计算强度提升

$$
\text{AI}_{\text{basic}} = \frac{2K}{(2K+1) \times 4} \approx \frac{1}{4} \text{ FLOP/Byte}
$$

$$
\text{AI}_{\text{tiled}} = \frac{2K}{\frac{2K}{T} \times 4} = \frac{T}{4} \text{ FLOP/Byte}
$$

```cpp-run title="Tiling 全局内存节省与计算强度提升"
#include <iostream>
#include <iomanip>

int main() {
    std::cout << "=== Tiling 效果量化分析 ===\n\n";

    int M = 1024, N = 1024, K = 1024;
    double base_reads = (double)M * N * 2 * K;

    std::cout << "矩阵: " << M << "×" << K << " × " << K << "×" << N << "\n";
    std::cout << "FLOPs: " << std::fixed << std::setprecision(1)
              << 2.0*M*N*K/1e9 << " GFLOP\n\n";

    std::cout << std::setw(12) << "TILE_SIZE"
              << std::setw(14) << "全局读(MB)"
              << std::setw(14) << "减少倍数"
              << std::setw(14) << "AI(FLOP/B)"
              << std::setw(14) << "Smem/Block"
              << std::setw(16) << "A100利用率" << "\n";
    std::cout << std::string(84, '-') << "\n";

    double a100_peak = 19500.0;   // GFLOPS
    double a100_bw   = 2039.0;    // GB/s
    double ridge_ai  = a100_peak / a100_bw;

    for (int T : {1, 4, 8, 16, 32, 64}) {
        double tiled_reads = (double)M * N * 2.0 * K / T;
        double reads_mb = tiled_reads * 4.0 / 1e6;
        double reduction = base_reads / tiled_reads;
        double ai = (double)T / 4.0;
        double smem_kb = 2.0 * T * T * 4.0 / 1024.0;

        double achievable_gflops = std::min(a100_peak, ai * a100_bw);
        double utilization = achievable_gflops / a100_peak * 100.0;

        std::cout << std::setw(12) << T
                  << std::setw(11) << std::setprecision(0) << reads_mb << " MB"
                  << std::setw(12) << std::setprecision(0) << reduction << "x"
                  << std::setw(14) << std::setprecision(2) << ai
                  << std::setw(11) << std::setprecision(1) << smem_kb << " KB"
                  << std::setw(13) << std::setprecision(1) << utilization << "%"
                  << "\n";
    }

    std::cout << "\nA100 Roofline 拐点: AI = " << std::setprecision(1) << ridge_ai
              << " FLOP/Byte\n";
    std::cout << "  T=1  (基础): AI=0.25 → 严重内存受限\n";
    std::cout << "  T=16 (Tiled): AI=4.0 → 仍然内存受限, 但好很多\n";
    std::cout << "  T=32 (Tiled): AI=8.0 → 接近拐点\n";
    std::cout << "  T=64 理论最优, 但 Shared Memory 太大 (32 KB/Block)\n";

    return 0;
}
```

---

## 5.6 `__syncthreads()` 的作用与陷阱

### 为什么需要两次 `__syncthreads()`？

Tiled Kernel 中有 **两个** `__syncthreads()`，缺一不可：

```c
for (int ph = 0; ph < numPhases; ph++) {
    // 加载 Tile
    As[ty][tx] = A[...];
    Bs[ty][tx] = B[...];

    __syncthreads();   // ← 同步点 1: 确保 Tile 完全加载

    // 计算
    for (int k = 0; k < TILE_SIZE; k++)
        sum += As[ty][k] * Bs[k][tx];

    __syncthreads();   // ← 同步点 2: 确保所有线程用完 Tile
}
```

**同步点 1 的作用**：

```
如果没有同步点 1:

时间 →→→→→→→→→→→→→→→
Thread 0: [加载 As[0][0]] [计算 sum += As[0][0]*Bs[0][0]] ...
Thread 1: [加载 As[0][1]] [加载中...还没完]
Thread 2: [加载 As[0][2]] [还在加载...]

Thread 0 要用 As[0][1] (由 Thread 1 加载), 但 Thread 1 还没加载完!
→ Thread 0 读到了 上一个 Phase 的旧数据 或 未初始化的数据
→ 结果错误!
```

**同步点 2 的作用**：

```
如果没有同步点 2:

Phase 0:
  所有线程加载 Tile → __syncthreads → 计算

  Thread 0: 计算完毕, 开始 Phase 1 的加载
            As[0][0] = A[...]  ← 写入了 Phase 1 的新数据!

  Thread 31: 还在用 As[0][0] 做 Phase 0 的计算!
             → 读到了被 Thread 0 覆盖的 Phase 1 数据!
             → 结果错误!
```

::: warning `__syncthreads()` 必须被所有线程执行
`__syncthreads()` 是 Block 内的 **栅栏同步**——Block 中所有线程都到达这一行后才能继续。如果把它放在 `if` 分支中，且只有部分线程进入该分支，就会 **死锁**（部分线程永远等不到另一部分线程）。

```c
// ✗ 危险: 可能死锁!
if (threadIdx.x < 16) {
    // 只有一半线程执行
    __syncthreads();  // 另一半线程永远不会到达 → 死锁
}

// ✓ 正确: 所有线程都执行
__syncthreads();
if (threadIdx.x < 16) {
    // 只有一半线程执行计算
}
```
:::

---

## 5.7 边界条件处理

### 问题：M、K、N 不是 TILE_SIZE 的整数倍

在实际中，矩阵维度很少恰好是 Tile 大小的整数倍。最后一个 Tile 可能 **越界**：

```
M=70, K=70, TILE_SIZE=16:

K 方向有 ceil(70/16) = 5 个 Phase
最后一个 Phase (ph=4) 要访问列 64..79
但 K 只有 70 列 → 列 70..79 越界!

          ph=0    ph=1    ph=2    ph=3    ph=4
A 的列:  [0:15] [16:31] [32:47] [48:63] [64:79]
                                          ↑
                                     70..79 越界!
```

### 解决方案：越界位置填 0

```c
__global__ void matmul_tiled_boundary(const float* A, const float* B, float* C,
                                       int M, int K, int N) {
    __shared__ float As[TILE_SIZE][TILE_SIZE];
    __shared__ float Bs[TILE_SIZE][TILE_SIZE];

    int tx = threadIdx.x, ty = threadIdx.y;
    int col = blockIdx.x * TILE_SIZE + tx;
    int row = blockIdx.y * TILE_SIZE + ty;
    float sum = 0.0f;

    int numPhases = (K + TILE_SIZE - 1) / TILE_SIZE;

    for (int ph = 0; ph < numPhases; ph++) {
        int a_col = ph * TILE_SIZE + tx;
        int b_row = ph * TILE_SIZE + ty;

        // 边界检查: 越界填 0 (0 不影响加法累积)
        As[ty][tx] = (row < M && a_col < K) ? A[row * K + a_col] : 0.0f;
        Bs[ty][tx] = (b_row < K && col < N) ? B[b_row * N + col] : 0.0f;

        __syncthreads();

        for (int k = 0; k < TILE_SIZE; k++)
            sum += As[ty][k] * Bs[k][tx];

        __syncthreads();
    }

    // 写回时也要检查边界
    if (row < M && col < N)
        C[row * N + col] = sum;
}
```

```
边界处理图示 (M=5, K=5, TILE=4):

Phase 0: 加载 A[:,0:3] 和 B[0:3,:]   → 正常

Phase 1: 加载 A[:,4:7] 和 B[4:7,:]
  A 只有 5 列 → 列 5,6,7 越界 → 填 0
  B 只有 5 行 → 行 5,6,7 越界 → 填 0

  As (加载后):             Bs (加载后):
  ┌───┬───┬───┬───┐       ┌───┬───┬───┬───┐
  │a40│ 0 │ 0 │ 0 │       │b40│b41│b42│b43│
  │a41│ 0 │ 0 │ 0 │       │ 0 │ 0 │ 0 │ 0 │ ← 越界, 填 0
  │a42│ 0 │ 0 │ 0 │       │ 0 │ 0 │ 0 │ 0 │
  │a43│ 0 │ 0 │ 0 │       │ 0 │ 0 │ 0 │ 0 │
  └───┴───┴───┴───┘       └───┴───┴───┴───┘
            ↑ 越界列填 0

  计算时: 0 × 任何数 = 0 → 不影响正确性!
```

---

## 5.8 Tile 大小的选择

### TILE_SIZE 越大越好？

增大 Tile 可以提高数据复用率（减少全局内存访问），但受限于 Shared Memory 容量和 Occupancy：

| TILE_SIZE | Shared Mem / Block | Block 线程数 | A100 最大 Block/SM | Occupancy |
|-----------|-------------------|-------------|-------------------|-----------|
| 8 | 0.5 KB | 64 | 32 (thread limit) | 100% |
| 16 | 2 KB | 256 | 8 | 100% |
| 32 | 8 KB | 1024 | 2 | 100% |
| 48 | 18 KB | 2304 > 1024! | **不合法** | — |

TILE_SIZE=48 需要 48×48=2304 个线程/Block，超过了 1024 的上限——**不合法**。

### 实际选择指导

```
TILE_SIZE 选择决策树:

                TILE_SIZE 候选: 16 vs 32

                    TILE_SIZE = 16
                    ┌──────────────────┐
                    │ Block = 256 线程  │
                    │ Smem = 2 KB      │
                    │ AI = 4 FLOP/B    │
                    │ Occupancy: 高     │
                    │ 适合: 中小矩阵    │
                    └──────────────────┘

                    TILE_SIZE = 32
                    ┌──────────────────┐
                    │ Block = 1024 线程 │
                    │ Smem = 8 KB      │
                    │ AI = 8 FLOP/B    │
                    │ Occupancy: 中     │
                    │ 适合: 大矩阵      │
                    └──────────────────┘

经验法则:
  - 矩阵较小 (< 512): TILE_SIZE=16 (更多 Block → 更好的 SM 利用)
  - 矩阵较大 (≥ 1024): TILE_SIZE=32 (更高计算强度 → 更接近峰值)
  - 生产代码: 用 cuBLAS (NVIDIA 已调优到极致)
```

---

## 5.9 Constant Memory

### 特性

Constant Memory 是一种特殊的只读内存，有自己的专用缓存：

- **容量**：64 KB（整个 GPU 共享）
- **缓存**：每个 SM 有专用的 Constant Cache（~8 KB）
- **广播**：当一个 Warp 中所有线程读取 **同一个地址** 时，只需一次内存访问，结果广播给所有线程
- **适用场景**：卷积核权重、物理常数、查找表等 **所有线程读同一数据** 的场景

```c
// 声明 (在全局作用域)
__constant__ float filter[256];

// Host 端写入
float h_filter[256] = { ... };
cudaMemcpyToSymbol(filter, h_filter, 256 * sizeof(float));

// Device 端直接使用 (像全局变量一样)
__global__ void conv_kernel(...) {
    float val = filter[k];  // 从 Constant Cache 读取, 极快
}
```

::: warning Constant Memory 的陷阱
如果一个 Warp 中的 32 个线程访问 Constant Memory 的 **不同地址**，这 32 次访问会被 **串行化**——比全局内存还慢！

```c
// ✓ 好: 所有线程读同一地址 → 广播
float w = filter[k];  // k 对所有线程相同

// ✗ 坏: 每个线程读不同地址 → 串行
float w = filter[threadIdx.x];  // 32 次串行访问!
```

Constant Memory 只在 **广播** 模式下有优势。如果不同线程需要不同数据，用全局内存（可以合并访问）或 Shared Memory 更好。
:::

---

## 5.10 寄存器优化

### 寄存器是最快的存储

寄存器的访问延迟是 **0 个时钟周期**（执行单元直接读取）。在 Tiled 矩阵乘法中，累积变量 `sum` 就存储在寄存器中——每个 Phase 的部分点积结果被累积到 `sum` 中，整个过程没有任何内存访问。

### 寄存器 Spill

当 Kernel 使用的寄存器超过 SM 可分配的数量时，编译器会将部分变量 **spill** 到 Local Memory（物理上在 HBM 中），速度骤降。

```bash
# 查看寄存器使用情况
nvcc --ptxas-options=-v -o program program.cu

# 输出示例:
# ptxas info: Used 32 registers, 2048 bytes smem, ...
#                 ↑ 每线程 32 个寄存器

# 如果看到:
# ptxas info: Used 64 registers, 2048 bytes smem, 16 bytes spill stores, 16 bytes spill loads
#                                                  ↑ 发生了 spill!
```

### 减少寄存器使用的策略

1. **减少局部变量**：合并临时变量，减少同时活跃的变量数量
2. **`__launch_bounds__`**：告诉编译器 Block 大小和最小 Occupancy，让编译器优化寄存器分配
3. **`-maxrregcount`**：强制限制每线程最大寄存器数（多余的 spill 到 Local Memory）
4. **循环展开控制**：`#pragma unroll` 可以增加或减少展开程度，影响寄存器压力

---

## 5.11 综合对比：四种矩阵乘法版本

让我们把所有版本放在一起对比。完整代码见 [`ch05_tiled_matmul.cu`](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch05_tiled_matmul.cu)。

### 版本一览

| 版本 | 特性 | 全局内存读取 | 计算强度 |
|------|------|------------|---------|
| V1: Basic | 无优化 | \(2MNK\) | \(\frac{1}{4}\) |
| V2: Tiled 16 | Shared Mem, 假设整除 | \(\frac{2MNK}{16}\) | 4 |
| V3: Tiled 16 + Boundary | + 边界处理 | \(\frac{2MNK}{16}\) | 4 |
| V4: Tiled 32 | 更大 Tile + `#pragma unroll` | \(\frac{2MNK}{32}\) | 8 |

### 代码对比：关键差异

```c
// V1: Basic — 全局内存 K 次读取
for (int k = 0; k < K; k++)
    sum += A[row*K+k] * B[k*N+col];  // 每次都从全局内存读!

// V2: Tiled — Shared Memory 分阶段复用
for (int ph = 0; ph < K/TILE; ph++) {
    As[ty][tx] = A[row*K + ph*TILE+tx];   // 全局→Shared (1次)
    Bs[ty][tx] = B[(ph*TILE+ty)*N + col];
    __syncthreads();
    for (int k = 0; k < TILE; k++)
        sum += As[ty][k] * Bs[k][tx];     // Shared Memory (快!)
    __syncthreads();
}

// V3: + Boundary — 条件加载, 越界填 0
As[ty][tx] = (row<M && a_col<K) ? A[row*K+a_col] : 0.0f;

// V4: + 大 Tile + unroll — 更高计算密度
#pragma unroll
for (int k = 0; k < TILE_32; k++)
    sum += As[ty][k] * Bs[k][tx];
```

### 性能预期

```cpp-run title="矩阵乘法各版本性能预估 (A100)"
#include <iostream>
#include <iomanip>
#include <algorithm>

int main() {
    std::cout << "=== 矩阵乘法各版本性能预估 (A100) ===\n\n";

    double peak_gflops = 19500.0;  // FP32
    double hbm_bw = 2039.0;       // GB/s

    struct Version {
        const char* name;
        double ai;
        double overhead_factor;
    };

    Version versions[] = {
        {"V1: Basic (no tile)", 0.25, 1.0},
        {"V2: Tiled 16×16",    4.0,  0.85},
        {"V3: Tiled 16 + boundary", 4.0, 0.80},
        {"V4: Tiled 32×32",    8.0,  0.85},
        {"cuBLAS (参考)",      100.0, 0.90},
    };

    int sizes[] = {256, 512, 1024, 2048, 4096};

    for (int n : sizes) {
        double gflop = 2.0 * n * n * n / 1e9;

        std::cout << "矩阵大小: " << n << "×" << n
                  << " (" << std::fixed << std::setprecision(1) << gflop << " GFLOP)\n";
        std::cout << std::setw(28) << "版本"
                  << std::setw(12) << "GFLOPS"
                  << std::setw(12) << "耗时(ms)"
                  << std::setw(14) << "峰值利用率" << "\n";
        std::cout << std::string(66, '-') << "\n";

        for (auto& v : versions) {
            double bw_limit = v.ai * hbm_bw;
            double achievable = std::min(peak_gflops, bw_limit) * v.overhead_factor;
            double time_ms = gflop / achievable * 1000.0;
            double util = achievable / peak_gflops * 100.0;

            std::cout << std::setw(28) << v.name
                      << std::setw(10) << std::setprecision(0) << achievable
                      << std::setw(10) << std::setprecision(2) << time_ms << " ms"
                      << std::setw(11) << std::setprecision(1) << util << "%"
                      << "\n";
        }
        std::cout << "\n";
    }

    return 0;
}
```

---

## 5.12 进阶：为什么 cuBLAS 能做到 80%+ 峰值？

我们的 Tiled Kernel 最高约 30-50% 峰值利用率，而 NVIDIA 的 cuBLAS 可以达到 **80-95%**。差距在哪里？

### cuBLAS 使用的高级优化

```
我们的 Tiled Kernel vs cuBLAS 的差距:

┌──────────────────────────────────────────────────────────────────┐
│ 优化技术                  │ 我们的 Kernel │ cuBLAS         │
├──────────────────────────────────────────────────────────────────┤
│ Shared Memory Tiling      │ ✓ (16/32)    │ ✓ (128+)       │
│ 寄存器 Tiling             │ ✗            │ ✓ 每线程计算    │
│                           │              │   多个 C 元素   │
│ 双缓冲 (Prefetch)        │ ✗            │ ✓ 加载下一Tile  │
│                           │              │   同时计算当前   │
│ 向量化加载 (float4)       │ ✗            │ ✓ 4个float一次  │
│ Tensor Core               │ ✗            │ ✓ (wmma API)   │
│ Bank Conflict 消除        │ ✗            │ ✓ padding/swizzle│
│ 自动调优 (Auto-tuning)    │ ✗            │ ✓ 根据矩阵大小  │
│                           │              │   选最佳配置    │
│ 汇编级优化               │ ✗            │ ✓ 手写 SASS     │
└──────────────────────────────────────────────────────────────────┘
```

### 关键技术 1：寄存器 Tiling（Thread Tiling）

cuBLAS 中每个线程不止计算 C 的 **一个** 元素，而是计算一个 **小矩阵**（如 8×8）。这大幅增加了每线程的计算量，提高了寄存器复用率：

```
我们的 Kernel: 每线程计算 C 的 1 个元素

Thread(tx,ty) → C[row][col] (1 个元素)
每 Phase: 从 Shared Mem 读 2×TILE 个 float, 做 TILE 次 FMA
计算:访存 比 = TILE : 2×TILE = 1 : 2 (FMA 太少)


cuBLAS 风格: 每线程计算 C 的 8×8 = 64 个元素

Thread → C[row:row+8][col:col+8] (64 个元素)
每 Phase: 从 Shared Mem 读 2×8 = 16 个 float
         → 做 8×8 = 64 次 FMA
计算:访存 比 = 64 : 16 = 4 : 1 (FMA 远多于访存!)
→ 寄存器中的 8×8 累积器被反复使用 → 极高的计算密度
```

### 关键技术 2：双缓冲 Prefetch

在当前 Tile 计算的同时，提前发起下一个 Tile 的全局内存加载请求——**计算和访存完全重叠**：

```
无 Prefetch:
  [Load Tile 0] [Compute Tile 0] [Load Tile 1] [Compute Tile 1] ...
                                 ↑ GPU 空闲等待加载

有 Prefetch (双缓冲):
  [Load Tile 0]                  [Load Tile 2]
  ─────────────[Compute Tile 0]  ─────────────[Compute Tile 2]
               [Load Tile 1]                  [Load Tile 3]
               ─────────────[Compute Tile 1]  ─────────────...

  → 加载和计算完全重叠, GPU 永不空闲!
```

### 关键技术 3：向量化加载

使用 `float4` 一次加载 4 个 float（16 字节），减少内存指令数量：

```c
// 标量加载: 4 条 load 指令
float a = A[idx];
float b = A[idx+1];
float c = A[idx+2];
float d = A[idx+3];

// 向量加载: 1 条 load 指令
float4 vec = *reinterpret_cast<const float4*>(&A[idx]);
// vec.x = A[idx], vec.y = A[idx+1], vec.z = A[idx+2], vec.w = A[idx+3]
```

---

## 5.13 内存带宽测量实验

配套代码 [`ch05_mem_bandwidth.cu`](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch05_mem_bandwidth.cu) 测量了不同内存层级的有效带宽，包括：

| 测试项 | 测量内容 |
|--------|---------|
| Global (Coalesced) | 合并访问的全局内存带宽 |
| Global (Strided) | 非合并访问（stride=32）的带宽衰减 |
| Shared Memory | 数据在 Shared Mem 中被复用 32 次的等效带宽 |
| Constant Memory | 广播模式下的 Constant Cache 带宽 |
| Register-only | 纯寄存器计算基线 |

```bash
# 编译运行
nvcc -O3 -arch=sm_80 -o ch05_mem_bandwidth ch05_mem_bandwidth.cu
./ch05_mem_bandwidth
```

预期观察：

- **Coalesced vs Strided**：Strided 访问带宽可能只有 Coalesced 的 **1/10 到 1/32**
- **Shared Memory 复用**：等效带宽远超全局内存（因为数据被复用了多次）
- **Constant 广播**：所有线程读同一地址时极快

---

## 5.14 本章总结

### 核心知识速查

| 概念 | 要点 |
|------|------|
| **内存层次** | Register (0 cyc) > Shared Mem (~5 cyc) > L1 (~30 cyc) > L2 (~200 cyc) > HBM (200-800 cyc) |
| **Shared Memory** | 片上、Block 内共享、程序员管理、32 Bank |
| **Tiling** | 将数据分块加载到 Shared Memory 复用，减少全局内存访问 TILE_SIZE 倍 |
| **计算强度** | Basic: \(\frac{1}{4}\) → Tiled(T=16): 4 → Tiled(T=32): 8 FLOP/Byte |
| **`__syncthreads()`** | 加载后同步 + 使用后同步，缺一不可 |
| **边界处理** | 越界填 0，写回时做边界检查 |
| **Bank Conflict** | 同一 Warp 访问同一 Bank 不同地址 → 串行化 → padding 解决 |
| **Constant Memory** | 64 KB，广播模式极快，非广播模式极慢 |
| **Local Memory** | 名为 Local 实为 HBM，寄存器 spill 的去处 |

### Tiled 矩阵乘法代码模板

```c
#define TILE 16

__global__ void matmul_tiled(const float* A, const float* B, float* C,
                              int M, int K, int N) {
    __shared__ float As[TILE][TILE];
    __shared__ float Bs[TILE][TILE];

    int tx = threadIdx.x, ty = threadIdx.y;
    int col = blockIdx.x * TILE + tx;
    int row = blockIdx.y * TILE + ty;
    float sum = 0.0f;

    for (int ph = 0; ph < (K + TILE - 1) / TILE; ph++) {
        int a_col = ph * TILE + tx;
        int b_row = ph * TILE + ty;
        As[ty][tx] = (row < M && a_col < K) ? A[row*K + a_col] : 0.0f;
        Bs[ty][tx] = (b_row < K && col < N) ? B[b_row*N + col] : 0.0f;
        __syncthreads();

        #pragma unroll
        for (int k = 0; k < TILE; k++)
            sum += As[ty][k] * Bs[k][tx];
        __syncthreads();
    }
    if (row < M && col < N) C[row*N + col] = sum;
}
```

### 性能优化路线图

```
矩阵乘法优化路线 (性能递增):

  V1: 基础版                              ~1-5% 峰值
      每线程从全局内存读取 2K 个 float
      │
      │ ← Shared Memory Tiling
      ▼
  V2: Tiled 16×16                         ~15-30% 峰值
      全局内存访问减少 16 倍
      │
      │ ← 增大 Tile + #pragma unroll
      ▼
  V3: Tiled 32×32                         ~25-45% 峰值
      全局内存访问减少 32 倍
      │
      │ ← 寄存器 Tiling (每线程算 8×8)
      │ ← 双缓冲 Prefetch
      │ ← float4 向量化加载
      │ ← Bank Conflict 消除
      ▼
  V4: 高度优化                            ~60-80% 峰值
      (CUTLASS 库的实现水平)
      │
      │ ← Tensor Core (wmma / mma.sync)
      │ ← 手写 PTX/SASS
      │ ← Auto-tuning
      ▼
  V5: cuBLAS                              ~80-95% 峰值
      NVIDIA 极致优化
```

---

::: tip 下一章预告
[Ch06: Performance Considerations](./ch06) 将聚焦 GPU 性能的三大杀手——**内存合并访问（Coalescing）**、**Warp Divergence** 的详细量化分析、以及 **资源分配与 Occupancy 的高级权衡**。如果说第 5 章教你"把数据放对地方"，第 6 章教你"把数据用对方式"。
:::

---

## 扩展思考

::: details 思考题 1：为什么 Shared Memory 用 `__shared__` 声明而不是让编译器自动管理（像 L1 Cache 一样）？
这是一个深刻的设计选择。Shared Memory 之所以要程序员显式管理，有几个原因：

1. **确定性**：硬件管理的 Cache 有 eviction 策略（LRU 等），程序员无法保证数据在何时被淘汰。Shared Memory 的数据在整个 Block 生命周期内始终存在——没有意外的 cache miss。

2. **协作加载**：Tiling 的核心是 Block 内所有线程 **协作** 加载数据。硬件 Cache 无法做到"Thread 0 加载的数据让 Thread 1 也能用"——Cache 是按请求加载的，不是按 Block 协作的。

3. **精确控制**：程序员知道哪些数据需要复用、复用多少次、何时可以丢弃。硬件 Cache 是盲目的——它不知道下一步会访问什么。

4. **同步语义**：`__syncthreads()` 配合 Shared Memory 实现的生产者-消费者模式（Thread A 写、Thread B 读）无法用 Cache 实现——Cache 的一致性模型太弱。

不过，现代 GPU（Volta+）允许 L1 Cache 和 Shared Memory **共享同一块物理 SRAM**，并且可以动态配置比例。某些场景下（如不需要线程间协作的随机访问），让硬件自动管理 L1 比手动管理 Shared Memory 更方便。
:::

::: details 思考题 2：如果 K 很小（比如 K=4），Tiling 还有意义吗？
当 K 很小时，Tiling 的收益大幅下降：

- K=4, TILE=16：只有 1 个 Phase（K < TILE），Tiling 相当于全量加载
- 总全局内存读取和基础版一样——因为"分块"后只有一块
- Shared Memory 和 `__syncthreads()` 反而带来了额外开销

此时更好的策略：
- 如果 M 和 N 很大：直接用基础 Kernel，K=4 意味着每线程只做 4 次 FMA，瓶颈在全局内存带宽
- 使用向量化加载（`float4`）一次读取 A 的整行
- 考虑让每个线程计算 C 的多行/多列（Thread Coarsening）
:::

::: details 思考题 3：FlashAttention 的 Tiling 和矩阵乘法的 Tiling 有什么异同？
**相同点**：
- 核心思想一致：把数据分块加载到 SRAM（Shared Memory / Register File），避免反复读写 HBM
- 都是为了提高计算强度（Arithmetic Intensity）

**关键差异**：
- **矩阵乘法的 Tiling**：Q × K × V 三个矩阵的分块，Tile 之间结果可以直接累加（加法可交换）
- **FlashAttention 的 Tiling**：难点在于 Softmax 不可分块——标准 Softmax 需要先看完所有值再归一化。FlashAttention 的核心创新是 **Online Softmax**（维护 running max 和 running sum），使得可以分块处理序列维度

```
矩阵乘法 Tiling:
  C[i][j] = Σ_k A[i][k]*B[k][j]
  分块: sum += (A tile) * (B tile)  ← 直接累加, 无序列依赖

FlashAttention Tiling:
  O = softmax(QK^T / √d) × V
  分块: 需要维护 running max m, running sum l
        每个新 Tile 可能更新全局 max → 需要 rescale 之前的结果
  → 更复杂, 但核心 Tiling 思想相同
```

可以说 FlashAttention 是 Tiling 思想在 Attention 计算中的创造性应用。如果你深入理解了本章的 Tiling，阅读 FlashAttention 论文会容易很多。
:::

::: details 思考题 4：Shared Memory 的 Bank Conflict 在 Tiled 矩阵乘法中会发生吗？
会的，而且很容易发生。分析 `As[ty][k]` 和 `Bs[k][tx]` 的访问模式：

```
内层循环: sum += As[ty][k] * Bs[k][tx]

As[ty][k] 的访问 (固定 ty, 遍历 k):
  同一 Warp 中的线程有不同的 tx 但相同的 ty
  它们在同一个 k 时刻访问 As[ty][0], As[ty][1], ...
  → 但等等, ty 对同一 Warp 中的线程不一定相同
  → 取决于 Warp 如何映射到 (tx, ty)

对于 TILE=16 的 16×16 Block:
  Warp 0: ty=0,tx=0..15 和 ty=1,tx=0..15 (32 个线程)
  访问 As[0][k] 和 As[1][k] → 两个不同地址 → 无冲突 ✓

Bs[k][tx] 的访问:
  Warp 0 中 tx=0..15,ty=0 和 tx=0..15,ty=1
  访问 Bs[k][0], Bs[k][1], ..., Bs[k][15] → 连续, 无冲突 ✓
```

对于 TILE=32 的 32×32 Block 列访问可能出现 Bank Conflict，解决方案就是 padding：`__shared__ float As[32][33]`。
:::

::: details 思考题 5：如何用 Nsight Compute 验证 Tiling 的效果？
Nsight Compute 是 NVIDIA 的 GPU Kernel profiling 工具。关键指标：

```bash
ncu --metrics \
  l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum,\
  l1tex__t_sectors_pipe_lsu_mem_global_op_st.sum,\
  sm__sass_thread_inst_executed_op_ffma_pred_on.sum \
  ./ch05_tiled_matmul 1024 1024 1024
```

对比指标：

| 指标 | 基础版 | Tiled 版 | 说明 |
|------|--------|---------|------|
| Global Load Sectors | 很大 | 减少 ~16x | 全局内存加载次数 |
| Shared Memory Load | 0 | 很大 | Shared Memory 使用量 |
| FFMA Instructions | 相同 | 相同 | 计算量不变 |
| Achieved Occupancy | 可能相同 | 可能略低 | Shared Mem 占用 |
| Duration | 很长 | 短很多 | **最终指标** |

最值得看的是 **Roofline 图**——它会直观显示你的 Kernel 是内存受限还是计算受限，以及 Tiling 后从内存受限区域向计算受限区域的移动。
:::
