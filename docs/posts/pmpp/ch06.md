---
title: "PMPP Ch06: Performance Considerations"
date: 2026-02-19
---

# Ch06: Performance Considerations

<p style="color: var(--vp-c-text-2); font-size: 14px;">
2026-02-19 &nbsp;·&nbsp; PMPP 专栏 &nbsp;·&nbsp; 第六章
</p>

> **本章信息**
> - **书名**: Programming Massively Parallel Processors: A Hands-on Approach (4th Edition)
> - **作者**: David B. Kirk, Wen-mei W. Hwu
> - **范围**: 第 6 章 Performance Considerations
> - **配套代码**: [ch06_divergence.cu](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch06_divergence.cu)

## 一句话总结

本章是 GPU 性能优化的实战指南——深入分析三大性能杀手：**内存合并访问（Coalescing）**、**Warp Divergence**、**资源分配与 Occupancy**。通过量化分析、代码对比和实验验证，系统性地回答"为什么我的 Kernel 性能不好"以及"如何优化"。

---

## 6.1 性能模型：Roofline 与三类瓶颈

### Roofline 模型回顾

Roofline 模型是理解 GPU 性能的根本框架。它指出实际性能由两个上限决定：

$$
\text{实际性能} = \min\!\left(\text{峰值算力},\ \text{AI} \times \text{内存带宽}\right)
$$

其中 AI（Arithmetic Intensity，计算强度）= FLOPs / 实际访存字节数。

```
Roofline 模型图示:

性能 (GFLOPS)
│
│         ╱ 算力上限 (Peak Compute)
│        ╱
│       ╱
│      ╱
│     ╱
│    ╱
│   ╱   ← 带宽上限 (Peak Bandwidth)
│  ╱     斜率 = 内存带宽
│ ╱
│╱
┼────────┬──────────────────── 计算强度 (FLOP/Byte)
         拐点

拐点 = 峰值算力 / 峰值带宽

A100: 拐点 = 19.5 TFLOPS / 2.039 TB/s ≈ 9.6 FLOP/Byte

Kernel 在拐点左侧 → 内存受限 (Memory-bound)
Kernel 在拐点右侧 → 计算受限 (Compute-bound)
```

### 三类性能瓶颈

根据 Roofline 模型和实际观察，GPU Kernel 的性能瓶颈可以分为三类：

| 瓶颈类型 | 特征 | 典型场景 | 优化方向 |
|---------|------|---------|---------|
| **内存带宽受限** | AI < 拐点，有效带宽 < 峰值带宽 | 向量加法、稀疏矩阵、数据移动 | Coalescing、Tiling、减少冗余访问 |
| **计算受限** | AI > 拐点，有效算力 < 峰值算力 | 矩阵乘法（已优化）、FFT | ILP、循环展开、减少 Divergence |
| **延迟受限** | Occupancy 低，Warp 不足 | 小问题、寄存器溢出、Shared Mem 过大 | 减少资源占用、调整 Block 大小 |

```
三类瓶颈的 Roofline 位置:

性能
│
│         ╱ 算力上限
│        ╱
│       ╱
│      ╱
│     ╱
│    ╱
│   ╱   ← 带宽上限
│  ╱
│ ╱
│╱
┼───┬───┬───┬───┬───┬───┬─── AI (FLOP/Byte)
   0   1   2   5  10  20  50

   延迟受限    内存受限        计算受限
   (Warp少)   (带宽低)        (算力低)
```

### 性能优化的系统化方法

```
性能优化决策树:

Kernel 性能差
│
├─ 是内存受限吗？
│  ├─ 是 → 检查 Coalescing
│  │      ├─ 访问模式连续？ → 优化数据布局 (AoS→SoA)
│  │      └─ 访问模式跳跃？ → 重新组织数据或算法
│  │
│  └─ 否 → 检查计算强度
│         ├─ AI 低？ → Tiling (Ch05)
│         └─ AI 高但性能仍差？ → 检查计算受限
│
├─ 是计算受限吗？
│  ├─ 是 → 检查 ILP
│  │      ├─ 指令依赖链长？ → 循环展开、多累积器
│  │      └─ Warp Divergence？ → 对齐分支到 Warp
│  │
│  └─ 否 → 检查 Occupancy
│         ├─ 寄存器溢出？ → 减少局部变量、__launch_bounds__
│         ├─ Shared Mem 过大？ → 减小 Tile 大小
│         └─ Block 太大？ → 减小 Block 大小
│
└─ 是延迟受限吗？
   ├─ 是 → 检查 Warp 数量
   │      ├─ Block 太少？ → 增加 Grid 大小
   │      └─ Occupancy 低？ → 减少资源占用
   │
   └─ 否 → 检查其他因素
          ├─ PCIe 传输开销？ → 数据常驻 GPU、异步传输
          └─ Kernel 启动开销？ → 合并多个 Kernel、CUDA Graphs
```

---

## 6.2 内存合并访问（Memory Coalescing）详解

### 什么是 Coalescing？

**Coalescing（合并访问）** 是 GPU 硬件自动将同一 Warp 中多个线程的全局内存访问合并为少量（理想情况下 1 次）内存事务（Memory Transaction）的机制。

```
Coalescing 的硬件机制:

Warp 0 的 32 个线程同时执行:
  Thread 0:  load A[0]
  Thread 1:  load A[1]
  Thread 2:  load A[2]
  ...
  Thread 31: load A[31]

硬件检测: 这 32 个访问地址连续 (0, 1, 2, ..., 31)
→ 合并为 1 次内存事务 (128 字节对齐)
→ 读取 A[0..31] (128 字节 = 32 × 4 字节)
→ 分发给各线程

如果没有 Coalescing:
→ 需要 32 次独立的内存事务
→ 带宽利用率只有 1/32
```

### Coalescing 的条件

Coalescing 需要满足以下条件（以 A100 为例）：

1. **地址对齐**：访问的起始地址必须是 128 字节对齐（32 个 float）
2. **连续访问**：Warp 中线程访问的地址必须连续（或接近连续）
3. **访问大小**：每次访问的大小必须是 4、8、16 字节（float、float2、float4）

```
Coalescing 条件详解:

条件 1: 地址对齐
  Warp 0 访问 A[0..31] → 起始地址 A[0] 对齐到 128B → ✓ 可以合并
  Warp 0 访问 A[1..32] → 起始地址 A[1] 不对齐 → ✗ 可能需要 2 次事务

条件 2: 连续访问
  Warp 0: A[0], A[1], A[2], ..., A[31] → 连续 → ✓ 完美合并
  Warp 0: A[0], A[32], A[64], ..., A[992] → stride=32 → ✗ 无法合并

条件 3: 访问大小
  float (4B) → ✓ 支持
  float2 (8B) → ✓ 支持
  float4 (16B) → ✓ 支持
  char (1B) → ✗ 效率低 (需要更多事务)
```

### Coalescing 的量化分析

```cpp-run title="Coalescing 带宽利用率计算"
#include <iostream>
#include <iomanip>
#include <cmath>

int main() {
    std::cout << "=== Memory Coalescing 带宽利用率分析 ===\n\n";

    const int WARP_SIZE = 32;
    const int TRANSACTION_SIZE = 128;  // bytes
    const int FLOAT_SIZE = 4;

    std::cout << "假设: Warp 中 32 个线程各访问 1 个 float (4B)\n";
    std::cout << "理想情况: 1 次内存事务读取 128B (32 floats)\n\n";

    std::cout << std::setw(20) << "访问模式"
              << std::setw(15) << "事务数"
              << std::setw(15) << "带宽利用率"
              << std::setw(20) << "说明" << "\n";
    std::cout << std::string(70, '-') << "\n";

    struct Pattern {
        const char* name;
        int transactions;
        const char* note;
    };

    Pattern patterns[] = {
        {"连续 (stride=1)", 1, "完美 Coalescing"},
        {"跳跃 (stride=2)", 2, "部分合并"},
        {"跳跃 (stride=4)", 4, "部分合并"},
        {"跳跃 (stride=8)", 8, "部分合并"},
        {"跳跃 (stride=16)", 16, "部分合并"},
        {"跳跃 (stride=32)", 32, "无法合并"},
        {"随机访问", 32, "无法合并"},
    };

    for (auto& p : patterns) {
        double utilization = (double)WARP_SIZE * FLOAT_SIZE / (p.transactions * TRANSACTION_SIZE) * 100.0;
        std::cout << std::setw(20) << p.name
                  << std::setw(15) << p.transactions
                  << std::setw(13) << std::fixed << std::setprecision(1) << utilization << "%"
                  << std::setw(20) << p.note << "\n";
    }

    std::cout << "\n关键洞察:\n";
    std::cout << "  - stride=1: 1 次事务, 100% 带宽利用率\n";
    std::cout << "  - stride=32: 32 次事务, 3.125% 带宽利用率 (慢 32 倍!)\n";
    std::cout << "  - 随机访问: 最坏情况, 每线程独立事务\n";

    return 0;
}
```

### 非合并访问的代价

当访问模式无法合并时，每个线程的访问都需要独立的内存事务：

```
非合并访问 (stride=32):

Warp 0:
  Thread 0:  load A[0]   → 事务 1: 读取 A[0..31] (但只用 A[0])
  Thread 1:  load A[32]  → 事务 2: 读取 A[32..63] (但只用 A[32])
  Thread 2:  load A[64]  → 事务 3: 读取 A[64..95] (但只用 A[64])
  ...
  Thread 31: load A[992] → 事务 32: 读取 A[992..1023] (但只用 A[992])

总事务数: 32 次
实际读取: 32 × 4 = 128 字节
但硬件读取: 32 × 128 = 4096 字节 (32 倍冗余!)
带宽利用率: 128 / 4096 = 3.125%
```

这就是为什么非合并访问会慢 **10-32 倍**。

---

## 6.3 Warp Divergence 的深入量化分析

### Divergence 的硬件执行机制

第 4 章已经介绍了 Warp Divergence 的基本概念。本章深入量化分析其性能影响。

当一个 Warp 遇到分支时，硬件会：

1. **检测活跃掩码（Active Mask）**：确定哪些线程走 if，哪些走 else
2. **串行执行各路径**：先执行 if 路径（禁用 else 路径的线程），再执行 else 路径（禁用 if 路径的线程）
3. **在汇合点重新统一**：所有线程重新同步

```
Divergence 执行时间线 (50/50 分裂):

时间 ────────────────────────────────────────────────►

Warp 0 (32 线程):
  t0: 检测分支条件
      Thread 0-15: 条件为真 → 活跃
      Thread 16-31: 条件为假 → 禁用

  t1-t_if: 执行 if 分支
      Thread 0-15: ████████████████ (执行)
      Thread 16-31: ░░░░░░░░░░░░░░░░ (空等)

  t_if+1: 汇合点 (重新统一)

  t_if+2-t_else: 执行 else 分支
      Thread 0-15: ░░░░░░░░░░░░░░░░ (空等)
      Thread 16-31: ████████████████ (执行)

  t_else+1: 最终汇合点

总执行时间 = time(if) + time(else)
理想时间 = max(time(if), time(else))

性能损失 = (time(if) + time(else)) / max(time(if), time(else))
如果 if 和 else 耗时相同 → 损失 = 2x
```

### Divergence 代价的数学模型

假设：
- Warp 中有 \(p\) 比例的线程走 if 分支
- if 分支耗时 \(T_{\text{if}}\)，else 分支耗时 \(T_{\text{else}}\)
- 无 Divergence 的理想耗时 = \(\max(T_{\text{if}}, T_{\text{else}})\)

实际耗时：

$$
T_{\text{actual}} = T_{\text{if}} + T_{\text{else}}
$$

性能损失：

$$
\text{损失倍数} = \frac{T_{\text{if}} + T_{\text{else}}}{\max(T_{\text{if}}, T_{\text{else}})}
$$

当 \(T_{\text{if}} = T_{\text{else}}\) 时，损失 = 2 倍（**与 p 无关！**）

```cpp-run title="Warp Divergence 性能损失量化"
#include <iostream>
#include <iomanip>
#include <algorithm>

int main() {
    std::cout << "=== Warp Divergence 性能损失分析 ===\n\n";

    std::cout << "假设: Warp 中 p 比例的线程走 if 分支\n";
    std::cout << "     if 分支耗时 T_if, else 分支耗时 T_else\n\n";

    std::cout << std::setw(12) << "T_if/T_else"
              << std::setw(15) << "实际耗时"
              << std::setw(15) << "理想耗时"
              << std::setw(15) << "性能损失"
              << std::setw(20) << "说明" << "\n";
    std::cout << std::string(77, '-') << "\n";

    struct Case {
        double t_if;
        double t_else;
        const char* note;
    };

    Case cases[] = {
        {1.0, 1.0, "两分支耗时相同"},
        {1.0, 0.1, "if 分支重, else 轻"},
        {0.1, 1.0, "if 分支轻, else 重"},
        {1.0, 5.0, "else 分支重 5 倍"},
        {5.0, 1.0, "if 分支重 5 倍"},
    };

    for (auto& c : cases) {
        double actual = c.t_if + c.t_else;
        double ideal = std::max(c.t_if, c.t_else);
        double loss = actual / ideal;

        std::cout << std::setw(8) << std::fixed << std::setprecision(1)
                  << c.t_if << "/" << std::setprecision(1) << c.t_else
                  << std::setw(13) << std::setprecision(2) << actual
                  << std::setw(13) << std::setprecision(2) << ideal
                  << std::setw(13) << std::setprecision(2) << loss << "x"
                  << std::setw(20) << c.note << "\n";
    }

    std::cout << "\n关键洞察:\n";
    std::cout << "  - 即使只有 1 个线程走不同分支, 整个 Warp 也要执行两遍\n";
    std::cout << "  - 性能损失与 '分裂比例' 无关, 只取决于两分支的耗时比\n";
    std::cout << "  - 如果两分支耗时相同, 损失固定为 2x\n";
    std::cout << "  - 如果一分支很轻 (如空操作), 损失接近 1x (但仍需执行两遍)\n";

    return 0;
}
```

### 实际代码中的 Divergence 案例

配套代码 [`ch06_divergence.cu`](https://github.com/Blueboylee/AI-INFRA-ALL-IN-ONE/blob/main/src/pmpp/cuda/ch06_divergence.cu) 包含了三个对比版本：

**版本 1：Divergent（有 Divergence）**

```c
__global__ void branch_divergent(const float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        if (threadIdx.x % 2 == 0) {
            output[idx] = input[idx] * 2.0f;  // 偶数线程
        } else {
            output[idx] = input[idx] * 3.0f;  // 奇数线程
        }
    }
}
```

问题：Warp 0 中线程 0,2,4,...,30 走 if，线程 1,3,5,...,31 走 else → **100% Divergence**

**版本 2：Uniform（无 Divergence）**

```c
__global__ void branch_uniform(const float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        if ((threadIdx.x / 32) % 2 == 0) {
            output[idx] = input[idx] * 2.0f;  // Warp 0,2,4,... 走 if
        } else {
            output[idx] = input[idx] * 3.0f;  // Warp 1,3,5,... 走 else
        }
    }
}
```

优化：同一 Warp 内所有线程走同一分支 → **0% Divergence**

**版本 3：Predicated（算术替代）**

```c
__global__ void branch_predicated(const float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float multiplier = (threadIdx.x % 2 == 0) ? 2.0f : 3.0f;
        output[idx] = input[idx] * multiplier;
    }
}
```

优化：用三元运算符替代 if-else，编译器可能优化为 **predicated 指令**（条件执行，无分支）

### Divergence 的检测与测量

使用 Nsight Compute 可以精确测量 Divergence：

```bash
ncu --metrics \
  sm__warps_active.avg.pct_of_peak_sustained_active,\
  smsp__warps_launched.sum,\
  smsp__sass_thread_inst_executed_op_integer_pred_on.sum \
  ./ch06_divergence
```

关键指标：

| 指标 | 含义 | 理想值 |
|------|------|--------|
| `smsp__warps_launched.sum` | 启动的 Warp 总数 | — |
| `smsp__sass_thread_inst_executed_op_integer_pred_on.sum` | Predicated 指令数 | 越少越好（说明 Divergence 少） |
| Branch Efficiency | 分支效率 = 无 Divergence 的分支数 / 总分支数 | 100% |

---

## 6.4 内存访问模式优化：AoS vs SoA

### 问题：结构体数组的访问模式

在处理 3D 点云、RGB 图像等数据时，有两种常见的内存布局：

**AoS (Array of Structures)**：结构体数组

```c
struct Point3D {
    float x, y, z;
};
Point3D points[N];  // [x0,y0,z0, x1,y1,z1, x2,y2,z2, ...]
```

**SoA (Structure of Arrays)**：数组的结构体

```c
float x_arr[N], y_arr[N], z_arr[N];  // [x0,x1,x2,...], [y0,y1,y2,...], [z0,z1,z2,...]
```

### AoS 的 Coalescing 问题

当多个线程并行访问 AoS 布局时：

```c
__global__ void aos_kernel(Point3D* points, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        output[idx] = points[idx].x + points[idx].y + points[idx].z;
    }
}
```

```
Warp 0 访问 points[0..31].x:

内存布局:
  points[0].x  points[0].y  points[0].z
  points[1].x  points[1].y  points[1].z
  points[2].x  points[2].y  points[2].z
  ...

Warp 0 的线程访问:
  Thread 0:  points[0].x  → 地址 offset = 0
  Thread 1:  points[1].x  → 地址 offset = 12 (跳过 y, z)
  Thread 2:  points[2].x  → 地址 offset = 24
  ...
  Thread 31: points[31].x → 地址 offset = 372

地址间隔: 12 字节 (stride=3)
→ 无法合并! 需要 32 次独立事务
```

### SoA 的 Coalescing 优势

```c
__global__ void soa_kernel(const float* x_arr, const float* y_arr, const float* z_arr,
                           float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        output[idx] = x_arr[idx] + y_arr[idx] + z_arr[idx];
    }
}
```

```
Warp 0 访问 x_arr[0..31]:

内存布局:
  x_arr[0]  x_arr[1]  x_arr[2]  ...  x_arr[31]
  y_arr[0]  y_arr[1]  y_arr[2]  ...  y_arr[31]
  z_arr[0]  z_arr[1]  z_arr[2]  ...  z_arr[31]

Warp 0 的线程访问:
  Thread 0:  x_arr[0]  → 地址 offset = 0
  Thread 1:  x_arr[1]  → 地址 offset = 4
  Thread 2:  x_arr[2]  → 地址 offset = 8
  ...
  Thread 31: x_arr[31] → 地址 offset = 124

地址连续: stride=1
→ 完美合并! 只需 1 次事务
```

### 性能对比量化

配套代码中的实验会显示：

| 布局 | 访问 x 坐标的事务数 | 带宽利用率 | 相对性能 |
|------|-------------------|-----------|---------|
| AoS | 32 次 | ~3% | 1x (基线) |
| SoA | 1 次 | ~100% | **10-30x** |

```
AoS vs SoA 性能对比 (理论):

访问 N 个点的 x 坐标:

AoS:
  每 Warp: 32 次事务 × 128B = 4096B
  总事务: (N/32) × 32 = N 次
  实际读取: N × 4B = 4N 字节
  硬件读取: N × 128B = 128N 字节
  带宽利用率: 4N / 128N = 3.125%

SoA:
  每 Warp: 1 次事务 × 128B = 128B
  总事务: (N/32) × 1 = N/32 次
  实际读取: N × 4B = 4N 字节
  硬件读取: (N/32) × 128B = 4N 字节
  带宽利用率: 4N / 4N = 100%

性能提升: 128N / 4N = 32x (理论最大值)
实际提升: 10-30x (受其他因素影响)
```

---

## 6.5 指令级并行（ILP）优化

### 什么是 ILP？

**ILP（Instruction-Level Parallelism）** 是指单个线程内多条独立指令可以并行执行的程度。GPU 的 SM 有多个执行单元（FP32、INT32、LD/ST、SFU），如果指令之间没有数据依赖，它们可以同时执行。

### 低 ILP：连续依赖链

```c
__global__ void ilp_low(const float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        float val = input[idx];
        val = val * 1.1f + 0.1f;  // 指令 1: 依赖 val
        val = val * 1.1f + 0.1f;  // 指令 2: 依赖指令 1 的结果
        val = val * 1.1f + 0.1f;  // 指令 3: 依赖指令 2 的结果
        val = val * 1.1f + 0.1f;  // 指令 4: 依赖指令 3 的结果
        output[idx] = val;
    }
}
```

```
低 ILP 执行时间线:

时间 ────────────────────────────────────────────────►

FP32 单元:
  t0: [指令 1: val * 1.1f + 0.1f]  ← 等待 input[idx] 加载
  t1: [等待指令 1 结果...]
  t2: [指令 2: val * 1.1f + 0.1f]  ← 依赖指令 1
  t3: [等待指令 2 结果...]
  t4: [指令 3: val * 1.1f + 0.1f]  ← 依赖指令 2
  t5: [等待指令 3 结果...]
  t6: [指令 4: val * 1.1f + 0.1f]  ← 依赖指令 3

总时间: 7 个周期 (假设每指令 1 周期 + 1 周期延迟)

问题: FP32 单元大部分时间在等待前一条指令的结果
→ 执行单元利用率低
```

### 高 ILP：独立操作并行

```c
__global__ void ilp_high(const float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < n) {
        // 四个独立的累积器, 无数据依赖
        float val0 = input[idx];
        float val1 = input[idx] * 0.5f;
        float val2 = input[idx] * 0.25f;
        float val3 = input[idx] * 0.125f;
        
        // 这四个操作可以并行执行!
        val0 = val0 * 1.1f + 0.1f;
        val1 = val1 * 1.1f + 0.1f;
        val2 = val2 * 1.1f + 0.1f;
        val3 = val3 * 1.1f + 0.1f;
        
        output[idx] = val0 + val1 + val2 + val3;
    }
}
```

```
高 ILP 执行时间线:

时间 ────────────────────────────────────────────────►

FP32 单元 0: [val0 * 1.1f + 0.1f]
FP32 单元 1: [val1 * 1.1f + 0.1f]  ← 同时执行!
FP32 单元 2: [val2 * 1.1f + 0.1f]  ← 同时执行!
FP32 单元 3: [val3 * 1.1f + 0.1f]  ← 同时执行!

总时间: 2 个周期 (1 周期计算 + 1 周期延迟)

优势: 4 个 FP32 单元同时工作
→ 执行单元利用率高
→ 性能提升约 2-4 倍 (取决于可用执行单元数)
```

### ILP 的量化分析

```cpp-run title="ILP 性能提升分析"
#include <iostream>
#include <iomanip>

int main() {
    std::cout << "=== ILP 性能提升分析 ===\n\n";

    std::cout << "假设: 每个 FP32 操作需要 1 个周期执行 + 1 个周期延迟\n";
    std::cout << "     SM 有 64 个 FP32 单元\n\n";

    std::cout << std::setw(20) << "策略"
              << std::setw(15) << "指令数"
              << std::setw(15) << "并行度"
              << std::setw(15) << "执行周期"
              << std::setw(15) << "相对性能" << "\n";
    std::cout << std::string(80, '-') << "\n";

    struct Strategy {
        const char* name;
        int instructions;
        int parallelism;
    };

    Strategy strategies[] = {
        {"低 ILP (串行依赖)", 4, 1},
        {"中 ILP (2 组并行)", 4, 2},
        {"高 ILP (4 组并行)", 4, 4},
        {"极高 ILP (8 组并行)", 8, 8},
    };

    for (auto& s : strategies) {
        int cycles = (s.instructions / s.parallelism) * 2;  // 假设每指令 2 周期
        double speedup = (double)strategies[0].instructions * 2 / cycles;

        std::cout << std::setw(20) << s.name
                  << std::setw(15) << s.instructions
                  << std::setw(15) << s.parallelism
                  << std::setw(13) << cycles << " cycles"
                  << std::setw(13) << std::fixed << std::setprecision(2) << speedup << "x"
                  << "\n";
    }

    std::cout << "\n关键洞察:\n";
    std::cout << "  - ILP 提升受限于可用执行单元数 (A100: 64 FP32/SM)\n";
    std::cout << "  - 过度展开 (如 100 个独立操作) 不会带来额外收益\n";
    std::cout << "  - 最佳 ILP 通常在 4-8 之间 (平衡寄存器压力和并行度)\n";

    return 0;
}
```

### 循环展开与 ILP

`#pragma unroll` 指令可以提示编译器展开循环，增加 ILP：

```c
// 未展开: 编译器可能保留循环结构
for (int k = 0; k < TILE_SIZE; k++)
    sum += As[ty][k] * Bs[k][tx];

// 展开: 编译器生成 TILE_SIZE 条独立指令
#pragma unroll
for (int k = 0; k < TILE_SIZE; k++)
    sum += As[ty][k] * Bs[k][tx];

// 等价于:
sum += As[ty][0] * Bs[0][tx];
sum += As[ty][1] * Bs[1][tx];
// ...
sum += As[ty][TILE_SIZE-1] * Bs[TILE_SIZE-1][tx];
```

展开的好处：
- 消除循环控制开销（计数器、分支）
- 增加 ILP（多条独立的乘加指令可以并行）
- 编译器可以更好地调度指令

但要注意：
- 过度展开会增加寄存器压力（可能导致 spill）
- 代码体积增大（可能影响指令缓存）

---

## 6.6 Launch Configuration 优化

### Block 大小的选择策略

Block 大小直接影响 Occupancy 和性能。选择时需要权衡：

| Block 大小 | 优点 | 缺点 | 适用场景 |
|-----------|------|------|---------|
| **小 (64-128)** | 高 Occupancy、灵活调度 | Warp 少、延迟隐藏能力弱 | 内存密集型、简单 Kernel |
| **中 (256)** | 平衡点、常用默认值 | — | 大多数场景 |
| **大 (512-1024)** | Warp 多、延迟隐藏强 | Occupancy 可能受限 | 计算密集型、复杂 Kernel |

```
Block 大小对 Occupancy 的影响 (A100, 假设每线程 32 寄存器):

blockDim = 64:
  每 Block: 64 线程 = 2 Warp
  每 Block 寄存器: 64 × 32 = 2048
  SM 最多 Block: min(2048/64, 65536/2048, 32) = min(32, 32, 32) = 32
  Occupancy: 32 × 64 / 2048 = 100%

blockDim = 256:
  每 Block: 256 线程 = 8 Warp
  每 Block 寄存器: 256 × 32 = 8192
  SM 最多 Block: min(2048/256, 65536/8192, 32) = min(8, 8, 32) = 8
  Occupancy: 8 × 256 / 2048 = 100%

blockDim = 1024:
  每 Block: 1024 线程 = 32 Warp
  每 Block 寄存器: 1024 × 32 = 32768
  SM 最多 Block: min(2048/1024, 65536/32768, 32) = min(2, 2, 32) = 2
  Occupancy: 2 × 1024 / 2048 = 100%

→ 三种大小都能达到 100% Occupancy (如果寄存器不是瓶颈)
→ 但 Block 数量不同: 32 vs 8 vs 2
→ 调度灵活性: 小 Block > 中 Block > 大 Block
```

### Grid 大小的选择

Grid 大小应该足够大，确保所有 SM 都有工作：

```
Grid 大小选择原则:

最小 Grid 大小:
  gridSize ≥ SM 数量
  例如 A100 (108 SMs) → gridSize ≥ 108

理想 Grid 大小:
  gridSize = SM 数量 × 每个 SM 的 Block 数 × 2-4 (余量)
  例如 A100, blockDim=256, 每 SM 8 Block
  → gridSize = 108 × 8 × 2 = 1728 Block

为什么需要余量?
  - Block 执行时间不同 (负载不均衡)
  - 某些 Block 可能提前完成
  - 余量确保 SM 不会空闲等待
```

### 动态调整策略

实际开发中，可以使用 CUDA Occupancy API 自动选择最优配置：

```c
int minGridSize, optimalBlockSize;
cudaOccupancyMaxPotentialBlockSize(
    &minGridSize,
    &optimalBlockSize,
    myKernel,
    0,    // 动态 Shared Memory
    0     // Block 大小上限
);

// 使用返回的最优值
dim3 block(optimalBlockSize);
dim3 grid(minGridSize);
myKernel<<<grid, block>>>(...);
```

---

## 6.7 综合案例：从 Naive 到优化的完整路径

让我们通过一个完整的例子，展示如何系统性地优化一个 Kernel。

### 问题：1D Stencil（邻域平均）

计算一维数组的移动平均：`output[i] = (input[i-1] + input[i] + input[i+1]) / 3`

### 版本 1：Naive（完全未优化）

```c
__global__ void stencil_naive(const float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx > 0 && idx < n - 1) {
        output[idx] = (input[idx-1] + input[idx] + input[idx+1]) / 3.0f;
    }
}
```

问题分析：
- **Coalescing**：✓ 访问连续（stride=1）
- **Divergence**：✗ 边界检查导致部分线程提前退出
- **ILP**：低（只有 3 次加法）
- **Occupancy**：可能 OK（简单 Kernel）

### 版本 2：修复 Divergence

```c
__global__ void stencil_no_divergence(const float* input, float* output, int n) {
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    
    // 边界检查移到计算后, 避免 Divergence
    float sum = 0.0f;
    int count = 0;
    
    if (idx > 0) { sum += input[idx-1]; count++; }
    if (idx < n) { sum += input[idx]; count++; }
    if (idx < n - 1) { sum += input[idx+1]; count++; }
    
    if (idx > 0 && idx < n - 1) {
        output[idx] = sum / (float)count;
    }
}
```

优化：用 predicated 指令替代分支，减少 Divergence

### 版本 3：Shared Memory Tiling

```c
__global__ void stencil_tiled(const float* input, float* output, int n) {
    __shared__ float tile[BLOCK_SIZE + 2];  // +2 用于 halo
    
    int tx = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tx;
    
    // 加载 Tile (包含 halo)
    if (idx - 1 >= 0 && idx - 1 < n) tile[tx] = input[idx - 1];
    if (idx >= 0 && idx < n) tile[tx + 1] = input[idx];
    if (idx + 1 >= 0 && idx + 1 < n) tile[tx + 2] = input[idx + 1];
    
    __syncthreads();
    
    // 从 Shared Memory 计算 (避免重复读取全局内存)
    if (tx > 0 && tx < BLOCK_SIZE - 1 && idx > 0 && idx < n - 1) {
        output[idx] = (tile[tx] + tile[tx+1] + tile[tx+2]) / 3.0f;
    }
}
```

优化：使用 Shared Memory 缓存数据，减少全局内存访问

### 版本 4：向量化加载

```c
__global__ void stencil_vectorized(const float* input, float* output, int n) {
    __shared__ float tile[BLOCK_SIZE + 2];
    
    int tx = threadIdx.x;
    int idx = blockIdx.x * blockDim.x + tx;
    
    // 使用 float4 向量化加载 (一次加载 4 个 float)
    if (idx * 4 + 3 < n) {
        float4 vec = *reinterpret_cast<const float4*>(&input[idx * 4]);
        tile[tx * 4] = vec.x;
        tile[tx * 4 + 1] = vec.y;
        tile[tx * 4 + 2] = vec.z;
        tile[tx * 4 + 3] = vec.w;
    }
    
    __syncthreads();
    
    // 计算 (每线程处理 4 个元素)
    if (tx > 0 && tx < BLOCK_SIZE - 1) {
        for (int i = 0; i < 4; i++) {
            int pos = idx * 4 + i;
            if (pos > 0 && pos < n - 1) {
                output[pos] = (tile[tx*4+i-1] + tile[tx*4+i] + tile[tx*4+i+1]) / 3.0f;
            }
        }
    }
}
```

优化：向量化加载减少内存指令数，提高 ILP

### 性能对比总结

| 版本 | 优化技术 | 全局内存访问 | 预期加速比 |
|------|---------|------------|-----------|
| V1: Naive | 无 | 3N 次读取 | 1x |
| V2: No Divergence | 消除 Divergence | 3N 次读取 | 1.5-2x |
| V3: Tiled | Shared Memory | N 次读取（减少 3x） | 2-3x |
| V4: Vectorized | + 向量化加载 | N/4 次事务（减少 12x） | 3-5x |

---

## 6.8 Profiling 实战：Nsight Compute 指标解读

### 关键性能指标分类

Nsight Compute 提供了数百个性能指标。对于性能优化，重点关注以下几类：

**1. 吞吐量指标（Throughput）**

| 指标 | 含义 | 理想值 |
|------|------|--------|
| `sm__throughput.avg.pct_of_peak_sustained_elapsed` | SM 算力利用率 | > 80% |
| `dram__throughput.avg.pct_of_peak_sustained_elapsed` | 内存带宽利用率 | > 80% |
| `l1tex__throughput.avg.pct_of_peak_sustained_elapsed` | L1/L2 带宽利用率 | > 80% |

**2. 内存访问指标**

| 指标 | 含义 | 说明 |
|------|------|------|
| `dram__bytes_read.sum` | 全局内存读取字节数 | 越小越好（说明 Tiling 有效） |
| `l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum` | 全局内存加载事务数 | 反映 Coalescing 效果 |
| `l1tex__t_bytes_pipe_lsu_mem_global_op_ld.sum` | 实际传输字节数 | 与 bytes_read 对比看效率 |

**3. Warp 效率指标**

| 指标 | 含义 | 理想值 |
|------|------|--------|
| `smsp__warps_active.avg.pct_of_peak_sustained_active` | 活跃 Warp 占比 | > 50% |
| `smsp__sass_thread_inst_executed_op_integer_pred_on.sum` | Predicated 指令数 | 越少越好 |
| Branch Efficiency | 分支效率 | 100% |

**4. Occupancy 指标**

| 指标 | 含义 | 说明 |
|------|------|------|
| `sm__warps_active.avg.pct_of_peak_sustained_active` | 实际 Occupancy | 与理论 Occupancy 对比 |
| `launch__registers_per_thread` | 每线程寄存器数 | 检查是否过多 |
| `launch__shared_mem_config_bytes` | Shared Memory 使用量 | 检查是否过大 |

### 典型性能问题诊断

```
性能问题诊断流程:

Kernel 性能差
│
├─ 检查 sm__throughput < 50%?
│  ├─ 是 → 检查 dram__throughput
│  │      ├─ dram__throughput < 50%? → 内存受限
│  │      │   ├─ l1tex__t_sectors 很大? → Coalescing 问题
│  │      │   └─ dram__bytes_read 很大? → Tiling 不足
│  │      │
│  │      └─ dram__throughput > 80%? → 计算受限
│  │         ├─ Branch Efficiency < 90%? → Divergence
│  │         └─ ILP 不足? → 循环展开、多累积器
│  │
│  └─ 否 → 检查 Occupancy
│         ├─ sm__warps_active < 30%? → 资源限制
│         │  ├─ registers_per_thread 很大? → 寄存器溢出
│         │  └─ shared_mem_config_bytes 很大? → Shared Mem 过大
│         │
│         └─ sm__warps_active > 50%? → 其他问题
│            ├─ Kernel 启动开销? → 合并 Kernel、CUDA Graphs
│            └─ 数据传输开销? → 异步传输、数据常驻 GPU
```

### Nsight Compute 使用示例

```bash
# 基础 profiling
ncu --set full ./my_program

# 只关注关键指标
ncu --metrics \
  sm__throughput.avg.pct_of_peak_sustained_elapsed,\
  dram__throughput.avg.pct_of_peak_sustained_elapsed,\
  smsp__warps_active.avg.pct_of_peak_sustained_active,\
  l1tex__t_sectors_pipe_lsu_mem_global_op_ld.sum \
  ./my_program

# 对比两个 Kernel
ncu --export my_kernel1.ncu-rep ./program --kernel myKernel1
ncu --export my_kernel2.ncu-rep ./program --kernel myKernel2
ncu --compare my_kernel1.ncu-rep my_kernel2.ncu-rep
```

---

## 6.9 资源分配的高级权衡

### 寄存器 vs Occupancy

这是 GPU 优化中最经典的权衡：

```
寄存器使用量对 Occupancy 的影响:

假设 A100 SM:
  总寄存器: 65536 个
  最大线程: 2048 个

每线程寄存器数 → 每 Block 寄存器 → 最大 Block 数 → Occupancy
─────────────────────────────────────────────────────────────
16 regs → 16×256 = 4096 → 65536/4096 = 16 Block → 100%
32 regs → 32×256 = 8192 → 65536/8192 = 8 Block  → 100%
64 regs → 64×256 = 16384 → 65536/16384 = 4 Block → 50%
128 regs → 128×256 = 32768 → 65536/32768 = 2 Block → 25%

→ 寄存器翻倍, Occupancy 减半
```

但更多寄存器可能带来：
- 避免 Local Memory spill（spill 到全局内存很慢）
- 更高的 ILP（更多中间变量可以并行计算）
- 更好的编译器优化空间

**最佳实践**：先用 `--ptxas-options=-v` 检查寄存器使用，如果发生 spill，尝试：
1. 减少局部变量
2. 使用 `__launch_bounds__` 限制寄存器
3. 如果 Occupancy 仍然 > 50%，spill 的开销可能可以接受

### Shared Memory vs Occupancy

类似地，Shared Memory 使用量也影响 Occupancy：

```
Shared Memory 对 Occupancy 的影响:

假设 A100 SM (164 KB Shared Memory):

每 Block Shared Memory → 最大 Block 数 → Occupancy
─────────────────────────────────────────────────
2 KB → 164/2 = 82 Block → 受限于 Thread Slot (32) → 100%
8 KB → 164/8 = 20 Block → 受限于 Thread Slot → 100%
32 KB → 164/32 = 5 Block → 5×256 = 1280 < 2048 → 62.5%
64 KB → 164/64 = 2 Block → 2×256 = 512 < 2048 → 25%

→ Shared Memory 翻倍, Occupancy 可能减半
```

但更多 Shared Memory 可能带来：
- 更大的 Tile（更高的计算强度）
- 更好的数据复用（减少全局内存访问）

**权衡原则**：
- 如果 Kernel 是内存受限：优先保证高 Occupancy（延迟隐藏更重要）
- 如果 Kernel 是计算受限：可以牺牲 Occupancy 换取更大的 Tile（计算强度更重要）

---

## 6.10 实际 AI Kernel 中的优化技术映射

### FlashAttention 中的优化技术

FlashAttention 是 GPU 性能优化的集大成者，让我们看看它如何应用本章的技术：

| 优化技术 | FlashAttention 中的应用 |
|---------|----------------------|
| **Tiling** | Q、K、V 分块加载到 SRAM，避免实体化 N² 注意力矩阵 |
| **Coalescing** | 确保每个 Warp 访问连续的 Q/K/V 子块 |
| **避免 Divergence** | Online Softmax 的 rescale 操作无分支 |
| **高 ILP** | 每个线程处理多个元素，多个累积器并行 |
| **低 Occupancy** | 每线程使用大量寄存器（保存 running max/sum），但计算强度极高 |

### vLLM PagedAttention 中的优化

| 优化技术 | PagedAttention 中的应用 |
|---------|----------------------|
| **内存布局优化** | KV Cache 分页存储（类似 SoA），避免大块连续分配 |
| **Coalescing** | 同一请求的 KV 在内存中连续，Warp 可以合并访问 |
| **减少冗余访问** | 通过分页复用，避免重复计算相同前缀的注意力 |

---

## 6.11 本章总结

### 核心优化技术速查

| 优化技术 | 目标 | 关键指标 | 典型提升 |
|---------|------|---------|---------|
| **Coalescing** | 提高内存带宽利用率 | `l1tex__t_sectors` | 10-32x |
| **消除 Divergence** | 提高 Warp 利用率 | Branch Efficiency | 1.5-2x |
| **Tiling** | 提高计算强度 | `dram__bytes_read` | 10-30x |
| **ILP 优化** | 提高执行单元利用率 | 指令并行度 | 2-4x |
| **Occupancy 优化** | 提高延迟隐藏能力 | `sm__warps_active` | 1.2-2x |

### 性能优化检查清单

```
GPU Kernel 性能优化检查清单:

□ 内存访问
  □ 访问模式连续？(Coalescing)
  □ 数据布局优化？(AoS → SoA)
  □ 冗余访问消除？(Tiling)

□ 分支控制
  □ 分支对齐到 Warp？(避免 Divergence)
  □ 可以用算术替代分支？(Predicated 指令)

□ 指令级并行
  □ 循环可以展开？(#pragma unroll)
  □ 有独立的操作可以并行？(多累积器)

□ 资源分配
  □ Occupancy 够高？(> 50%)
  □ 寄存器没有溢出？(检查 --ptxas-options=-v)
  □ Shared Memory 使用合理？(不会过度限制 Occupancy)

□ Launch 配置
  □ Block 大小合适？(常用 256)
  □ Grid 大小足够？(≥ SM 数量)
```

### 从理论到实践

本章的所有优化技术最终都要通过 **实际测量** 来验证。记住：

1. **先测量，再优化**：用 Nsight Compute 找出真正的瓶颈
2. **一次优化一个方面**：不要同时改多个地方，难以定位效果
3. **量化对比**：每次优化都要记录性能提升（时间、带宽、利用率）
4. **理解原理**：知道"为什么"比知道"怎么做"更重要

---

::: tip 下一章预告
[Ch07: Convolution](./ch07) 将把前面学到的所有优化技术应用到 **卷积** 这个经典问题上。你会看到如何用 Shared Memory Tiling、常量内存、Halo 元素处理等技术，将卷积 Kernel 优化到接近 cuDNN 的性能水平。
:::

---

## 扩展思考

::: details 思考题 1：为什么 Coalescing 要求 128 字节对齐？
这是 GPU 内存系统的硬件特性。全局内存访问以 **128 字节** 为单位（一个 cache line / memory transaction）。如果 Warp 的 32 个线程要访问连续的 32 个 float（128 字节），且起始地址对齐到 128 字节边界，硬件可以一次性读取整个 cache line，然后分发给各线程。

如果起始地址不对齐（如从 A[1] 开始），可能需要 2 次事务：
- 事务 1：读取包含 A[1] 的 128 字节块
- 事务 2：读取包含 A[32] 的 128 字节块

这就是为什么 `cudaMalloc` 返回的指针总是对齐的，以及为什么使用 `float4` 向量化加载时要注意对齐。
:::

::: details 思考题 2：AoS vs SoA 的选择是否总是 SoA 更好？
不一定。选择取决于 **访问模式**：

**SoA 适合的场景**：
- 并行访问同一字段（如所有线程读 x 坐标）
- 字段间无依赖（如 x、y、z 独立计算）

**AoS 适合的场景**：
- 顺序访问所有字段（如先读 x，再读 y，再读 z，且顺序处理）
- 字段间有强局部性（如 x、y、z 经常一起使用）
- CPU 代码需要 AoS 布局（避免转换开销）

实际中，**混合布局（Hybrid）** 也很常见：将数据组织为"结构体的数组的数组"，在中间层做 SoA，在顶层做 AoS。
:::

::: details 思考题 3：ILP 的提升是否有上限？
是的。ILP 的上限受限于：

1. **执行单元数量**：A100 每个 SM 有 64 个 FP32 单元。即使有 100 个独立操作，也只能同时执行 64 个
2. **寄存器数量**：更多独立操作需要更多寄存器保存中间结果，可能导致 spill
3. **指令发射带宽**：Warp Scheduler 每个周期只能发射有限条指令

经验法则：ILP 在 4-8 之间通常是最佳平衡点。超过这个值，收益递减，甚至可能因为寄存器压力导致性能下降。
:::

::: details 思考题 4：如何判断一个 Kernel 是内存受限还是计算受限？
使用 Roofline 模型：

1. **计算实际 AI**：
   $$
   \text{AI} = \frac{\text{FLOPs}}{\text{实际访存字节数}}
   $$

2. **计算拐点**：
   $$
   \text{拐点} = \frac{\text{峰值算力}}{\text{峰值带宽}}
   $$

3. **判断**：
   - AI < 拐点 → 内存受限（提升带宽利用率更重要）
   - AI > 拐点 → 计算受限（提升算力利用率更重要）

实际中，可以用 Nsight Compute 的 Roofline 图直观看到 Kernel 的位置。
:::

::: details 思考题 5：Occupancy 100% 一定比 50% 好吗？
不一定。这取决于 Kernel 的类型：

**高 Occupancy 更重要的场景**：
- 内存密集型 Kernel（需要大量 Warp 隐藏内存延迟）
- 简单 Kernel（每个 Warp 的计算量小，需要更多 Warp 保持忙碌）

**低 Occupancy 可接受的场景**：
- 计算密集型 Kernel（每个 Warp 做大量计算，少量 Warp 也能隐藏延迟）
- 复杂 Kernel（每线程需要大量寄存器/Shared Memory，高 Occupancy 不可行）

FlashAttention 是典型例子：Occupancy 只有 25-50%，但性能极好，因为计算强度极高，少量 Warp 就足以隐藏延迟。
:::

